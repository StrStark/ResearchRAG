[
    {
        "id": "cbff9bea-a277-4471-a396-19a49a477ea5",
        "title": "",
        "chunk_text": "High Probability Complexity Bounds of Trust-Region Stochastic Sequential Quadratic Programming with Heavy-Tailed Noise Yuchen Fang1, Javad Lavaei2, Katya Scheinberg3, and Sen Na3 1Department of Mathematics, University of California, Berkeley 2Department of Industrial Engineering and Operations Research, University of California, Berkeley 3H.",
        "metadata": {
            "author": "",
            "keywords": [
                "Berkeley",
                "University",
                "California",
                "Javad",
                "Katya",
                "Yuchen",
                "Sen",
                "Mathematics",
                "Research",
                "Probability"
            ]
        }
    },
    {
        "id": "eec773ff-474b-46f5-931e-6a377633b989",
        "title": "",
        "chunk_text": "Milton School of Industrial and Systems Engineering, Georgia Institute of Technology Abstract In this paper, we consider nonlinear optimization problems with a stochastic objective and determin- istic equality constraints. We propose a Trust-Region Stochastic Sequential Quadratic Programming (TR-SSQP) method and establish its high-probability iteration complexity bounds for identifying first- and second-order ϵ-stationary points.",
        "metadata": {
            "author": "",
            "keywords": [
                "Engineering",
                "Georgia",
                "School",
                "Industrial",
                "Systems",
                "Institute",
                "Technology",
                "Abstract",
                "stochastic",
                "Milton"
            ]
        }
    },
    {
        "id": "47b2f524-fa83-4e08-9ce2-b87b20d89edf",
        "title": "",
        "chunk_text": "In our algorithm, we assume that exact objective values, gradients, and Hessians are not directly accessible but can be estimated via zeroth-, first-, and second- order probabilistic oracles.",
        "metadata": {
            "author": "",
            "keywords": [
                "gradients",
                "Hessians",
                "algorithm",
                "zeroth",
                "order",
                "oracles",
                "assume",
                "exact",
                "objective",
                "directly"
            ]
        }
    },
    {
        "id": "2b9da6a8-5524-4490-aa1f-b1e3ad66d509",
        "title": "",
        "chunk_text": "Compared to existing complexity studies of SSQP methods that rely on a zeroth-order oracle with sub-exponential tail noise (i.e., light-tailed) and focus mostly on first-order stationarity, our analysis accommodates irreducible and heavy-tailed noise in the zeroth-order oracle and significantly extends the analysis to second-order stationarity.",
        "metadata": {
            "author": "",
            "keywords": [
                "stationarity",
                "light-tailed",
                "zeroth-order",
                "oracle",
                "SSQP",
                "noise",
                "analysis",
                "Compared",
                "existing",
                "complexity"
            ]
        }
    },
    {
        "id": "583be7b3-a223-4b8a-9b12-0389adc8158d",
        "title": "",
        "chunk_text": "We show that under weaker noise conditions, our method achieves the same high-probability first-order iteration complexity bounds, while also exhibiting promising second-order iteration complexity bounds. Specifically, the method identifies a first-order ϵ-stationary point in O(ϵ−2) iterations and a second-order ϵ-stationary point in O(ϵ−3) iterations with high probability, provided that ϵ is lower bounded by a constant deter- mined by the irreducible noise level in estimation.",
        "metadata": {
            "author": "",
            "keywords": [
                "bounds",
                "complexity",
                "conditions",
                "noise",
                "method",
                "first-order",
                "second-order",
                "point",
                "show",
                "weaker"
            ]
        }
    },
    {
        "id": "d0886f37-0a1a-4ecb-9d5c-0302e8609b8c",
        "title": "",
        "chunk_text": "We validate our theoretical findings and evaluate the practical performance of our method on CUTEst benchmark test set. 1 Introduction In this paper, we consider stochastic optimization problems with deterministic equality constraints of the following form: min x∈Rd f(x), s.t. c(x) = 0, (1) where f : Rd →R is the stochastic objective and c : Rd →Rm are deterministic equality constraints.",
        "metadata": {
            "author": "",
            "keywords": [
                "set",
                "equality",
                "validate",
                "theoretical",
                "findings",
                "evaluate",
                "practical",
                "performance",
                "method",
                "CUTEst"
            ]
        }
    },
    {
        "id": "58a6e5d6-6496-483c-b331-65de866921dc",
        "title": "",
        "chunk_text": "For Problem (1), we assume that the exact objective value f(x), together with its gradient ∇f(x) and Hessian ∇2f(x), cannot be evaluated accurately but can be estimated via stochastic probabilistic oracles. For the sake of generality, we do not specify the underlying true sampling distribution of f, since the oracles may have different sampling distributions, leading to intrinsically biased estimates.",
        "metadata": {
            "author": "",
            "keywords": [
                "Problem",
                "Hessian",
                "gradient",
                "oracles",
                "assume",
                "exact",
                "objective",
                "evaluated",
                "accurately",
                "estimated"
            ]
        }
    },
    {
        "id": "de6722ce-56c5-42fa-9f8a-3869cfe19d60",
        "title": "",
        "chunk_text": "Constrained stochastic optimization problems of form (1) appear ubiquitously in scientific and engi- neering areas, such as optimal control (Betts, 2010), portfolio optimization (C¸akmak and ¨Ozekici, 1 arXiv:2503.19091v1 [math.OC] 24 Mar 2025 2005), multi-stage optimization (Shapiro et al., 2021), supply chain network design (Santoso et al., 2005), and statistical machine learning (Cuomo et al., 2022). Deterministic constrained optimization has been extensively studied for decades.",
        "metadata": {
            "author": "",
            "keywords": [
                "Betts",
                "Ozekici",
                "Mar",
                "Shapiro",
                "Santoso",
                "Cuomo",
                "arXiv",
                "math.OC",
                "optimization",
                "form"
            ]
        }
    },
    {
        "id": "1991bd53-e470-4eef-8c66-bfdb4441f14b",
        "title": "",
        "chunk_text": "While numerous methods have been developed under favorable problem settings, Sequential Quadratic Programming (SQP) has proven to be robust and effective, especially for highly nonlinear problems (Bertsekas, 1982; Boggs and Tolle, 1995; Nocedal and Wright, 2006).",
        "metadata": {
            "author": "",
            "keywords": [
                "SQP",
                "Bertsekas",
                "Sequential",
                "Programming",
                "Boggs",
                "Tolle",
                "Nocedal",
                "Wright",
                "Quadratic",
                "settings"
            ]
        }
    },
    {
        "id": "2d61aa33-a1f3-4fd6-aa56-ec9bbff5b74c",
        "title": "",
        "chunk_text": "Recent studies on optimization problems where the objective function and constraints are evaluated subject to bounded noise further highlight the advantages of SQP methods (Sun and Nocedal, 2023; Lou et al., 2024; Oztoprak et al., 2023; Sun and Nocedal, 2024). Inspired by the success of SQP, a series of Stochastic SQP (SSQP) methods has been proposed to solve Problem (1), where the exact computations of objective quantities are replaced by the corresponding estimates obtained through sampling.",
        "metadata": {
            "author": "",
            "keywords": [
                "Sun",
                "Nocedal",
                "SQP",
                "Lou",
                "Oztoprak",
                "Recent",
                "objective",
                "methods",
                "SSQP",
                "studies"
            ]
        }
    },
    {
        "id": "4d613d20-e553-44de-b374-b038d6d02dad",
        "title": "",
        "chunk_text": "The quality of the estimates is assumed to satisfy specific conditions that depend on different sampling frameworks. The first widely used framework is the fully stochastic setup, where at each iteration a single sam- ple or a fixed batch of samples is drawn to estimate the objective gradient, and a sequence βk assists in stepsize selection. Gradient estimates are typically assumed to be unbiased, with variance either bounded or subject to a growth condition. Under this setup, Berahas et al.",
        "metadata": {
            "author": "",
            "keywords": [
                "assumed",
                "setup",
                "quality",
                "satisfy",
                "specific",
                "depend",
                "sampling",
                "estimates",
                "gradient",
                "Berahas"
            ]
        }
    },
    {
        "id": "abef2a5f-f390-477e-9433-766cee218ed4",
        "title": "",
        "chunk_text": "(2021) introduced the first line-search SSQP method, while Fang et al. (2024a) introduced the first trust-region SSQP method. Building on these designs, subsequent extensions in algorithm design and analysis have been reported (see, e.g., Berahas et al., 2024, 2023; Curtis et al., 2023a, 2024a,b; Beiser et al., 2023; Bollapragada et al., 2023; Berahas et al., 2025; Curtis et al., 2025; Kuang et al., 2025). The non-asymptotic itera- tion complexity has also been investigated under this setup.",
        "metadata": {
            "author": "",
            "keywords": [
                "SSQP",
                "Fang",
                "introduced",
                "method",
                "Berahas",
                "Curtis",
                "line-search",
                "Beiser",
                "Bollapragada",
                "Kuang"
            ]
        }
    },
    {
        "id": "10176a83-7b0d-4ecd-9adc-850d3d139ec3",
        "title": "",
        "chunk_text": "Based on the algorithm in Berahas et al. (2021), Curtis et al. (2023b) established a worst-case iteration complexity of O(ϵ−4) for first-order ϵ- stationarity with high probability, assuming that the trial step is solved exactly in each iteration and βk is constant.",
        "metadata": {
            "author": "",
            "keywords": [
                "Berahas",
                "Based",
                "Curtis",
                "algorithm",
                "iteration",
                "established",
                "stationarity",
                "probability",
                "assuming",
                "constant"
            ]
        }
    },
    {
        "id": "ed315c9b-e092-4d69-a11d-d59ae3aa6797",
        "title": "",
        "chunk_text": "Meanwhile, Na and Mahoney (2025) established the local convergence guarantee of the SSQP method with decaying βk, demonstrating the same O(ϵ−4) iteration complexity for first-order ϵ-stationarity while allowing the trial step to be solved inexactly in each iteration. In contrast to the fully stochastic setup, methods in the random model setup do not require un- biased gradient estimates, and the stepsizes are also selected adaptively.",
        "metadata": {
            "author": "",
            "keywords": [
                "Mahoney",
                "iteration",
                "SSQP",
                "established",
                "demonstrating",
                "local",
                "convergence",
                "guarantee",
                "decaying",
                "complexity"
            ]
        }
    },
    {
        "id": "3583ff2b-6d03-4ea3-97b3-4410f26fb3b5",
        "title": "",
        "chunk_text": "This framework leverages zeroth-, first-, and second-order probabilistic oracles to estimate objective values, gradients, and Hessians, respectively, ensuring that the estimates satisfy proper, adaptive accuracy conditions with a high but fixed probability. Under this setup, Na et al. (2022a) introduced the first line-search SSQP method, while Fang et al. (2024b) introduced the first trust-region SSQP method with a second-order convergence guarantee.",
        "metadata": {
            "author": "",
            "keywords": [
                "Hessians",
                "gradients",
                "zeroth",
                "ensuring",
                "proper",
                "adaptive",
                "probability",
                "SSQP",
                "introduced",
                "framework"
            ]
        }
    },
    {
        "id": "0ab35306-69df-43d5-9d07-ba6bcfdd5231",
        "title": "",
        "chunk_text": "The studies have been further generalized in Na et al. (2023); Berahas et al. (2022); Qiu and Kungurtsev (2023) to accommodate inequality constraints, adaptive sampling mech- anisms, and robust designs. However, the above studies in the random model setup share two key limitations: (1) they assume the estimation noise vanishes with fixed probability, and (2) they lack a non-asymptotic, high-probability iteration complexity analysis. Recently, Berahas et al.",
        "metadata": {
            "author": "",
            "keywords": [
                "Berahas",
                "generalized",
                "studies",
                "Qiu",
                "Kungurtsev",
                "anisms",
                "Recently",
                "constraints",
                "adaptive",
                "mech"
            ]
        }
    },
    {
        "id": "261a7669-bf8e-4cc3-a059-2415f3d930fd",
        "title": "",
        "chunk_text": "(2025) addressed these two limitations in constrained optimization by analyzing a line-search (also referred to as step-search) method that allows for an irreducible noise in the estimation. The authors conducted a non-asymptotic complexity analysis and established an iter- ation complexity of O(ϵ−2) for first-order ϵ-stationarity with high probability, provided that ϵ exceeds a threshold determined by the irreducible noise level.",
        "metadata": {
            "author": "",
            "keywords": [
                "addressed",
                "line-search",
                "step-search",
                "method",
                "estimation",
                "irreducible",
                "noise",
                "limitations",
                "constrained",
                "optimization"
            ]
        }
    },
    {
        "id": "f791b501-53c0-4d2d-8597-91fcff5962f9",
        "title": "",
        "chunk_text": "However, their zeroth-order oracle assumes the noise follows a sub-exponential distribution, thereby excluding all heavy-tailed noise; and their method does not exhibit second-order convergence guarantees. Identifying only first-order stationary points can be sometimes unsatisfactory in nonlinear optimization, as such points may correspond to 2 saddle points or even local maxima, leading to highly sensitive solutions (Dauphin et al., 2014; Choro- manska et al., 2015; Jain et al., 2017).",
        "metadata": {
            "author": "",
            "keywords": [
                "noise",
                "distribution",
                "guarantees",
                "Choro",
                "points",
                "zeroth-order",
                "oracle",
                "assumes",
                "sub-exponential",
                "excluding"
            ]
        }
    },
    {
        "id": "9e0c0efe-87b5-4636-990c-2c6967400e3a",
        "title": "",
        "chunk_text": "To the best of our knowledge, in the random model setup, high-probability iteration complexity for second-order stationarity has only been explored in the context of unconstrained optimization. In this setting, Cao et al. (2023) performed a non-asymptotic analysis for a trust-region method, demonstrating an iteration complexity of O(ϵ−2) for first-order ϵ-stationarity and O(ϵ−3) for second-order ϵ-stationarity.",
        "metadata": {
            "author": "",
            "keywords": [
                "knowledge",
                "setup",
                "high-probability",
                "optimization",
                "iteration",
                "complexity",
                "second-order",
                "random",
                "model",
                "stationarity"
            ]
        }
    },
    {
        "id": "5b340e47-04e6-4daa-951b-a8e741f54936",
        "title": "",
        "chunk_text": "However, their zeroth-order probabilistic oracle still requires the noise to follow a sub-exponential distribution. Assuming the noise in the zeroth-order oracle following a sub-exponential distribution leads to a significant gap between non-asymptotic and asymptotic analyses, as such an oracle condition is not required in asymptotic analysis (Na et al., 2022a, 2023; Fang et al., 2024b).",
        "metadata": {
            "author": "",
            "keywords": [
                "sub-exponential",
                "oracle",
                "noise",
                "distribution",
                "zeroth-order",
                "Fang",
                "probabilistic",
                "requires",
                "follow",
                "asymptotic"
            ]
        }
    },
    {
        "id": "9c5c55f2-1098-4768-b2ba-56e9a6f37b9c",
        "title": "",
        "chunk_text": "The sub-exponential noise condition can be restrictive, as it implies the existence of infinite-order moments, thereby excluding noise from any heavy-tailed distribution where only a finite number of moments are bounded. More importantly, there is no clear justification for the discrepancy between the noise conditions in the zeroth-order oracle and those in the first- and second-order oracles; the latter oracles do allow for heavy-tailed noise (e.g., only a finite second moment).",
        "metadata": {
            "author": "",
            "keywords": [
                "noise",
                "finite",
                "heavy-tailed",
                "restrictive",
                "bounded",
                "moments",
                "sub-exponential",
                "implies",
                "existence",
                "infinite-order"
            ]
        }
    },
    {
        "id": "d32b990e-2474-4fb1-9f41-ad475960b3b2",
        "title": "",
        "chunk_text": "This discrepancy highlights the desire to relax light-tailed noise in the zeroth-order oracle to heavy-tailed noise when analyzing the complexity of the methods. In this paper, we make progress toward that goal. We introduce a trust-region SSQP method for constrained optimization in the random model setup that accommodates irreducible and heavy-tailed noise, and establish its high-probability iteration complexity bounds for achieving both first- and second-order ϵ-stationarities.",
        "metadata": {
            "author": "",
            "keywords": [
                "noise",
                "heavy-tailed",
                "discrepancy",
                "highlights",
                "desire",
                "relax",
                "light-tailed",
                "zeroth-order",
                "oracle",
                "analyzing"
            ]
        }
    },
    {
        "id": "16c28a86-051d-442e-a367-c0663179fc26",
        "title": "",
        "chunk_text": "Our key contributions are summarized as follows. (a) Compared to Na et al. (2022a, 2023); Fang et al. (2024b), we allow irreducible noise in the zeroth-, first-, and second-order oracles, while their framework requires the noise to diminish with fixed prob- ability. In addition, we extend the second-order asymptotic convergence guarantee from Fang et al. (2024b) to establish the first second-order non-asymptotic guarantee for constrained problems.",
        "metadata": {
            "author": "",
            "keywords": [
                "Fang",
                "Compared",
                "second-order",
                "key",
                "contributions",
                "summarized",
                "noise",
                "guarantee",
                "ability",
                "zeroth"
            ]
        }
    },
    {
        "id": "3c7a55a6-a897-47f6-ba78-b1b390324a6f",
        "title": "",
        "chunk_text": "(b) Compared to the step-search SSQP method in Berahas et al. (2025), we study a trust-region SSQP method and provide a complexity analysis for second-order stationarity. As a classic complement to the line-search scheme, the trust-region scheme computes the search direction and selects the stepsize jointly.",
        "metadata": {
            "author": "",
            "keywords": [
                "Compared",
                "SSQP",
                "Berahas",
                "method",
                "step-search",
                "trust-region",
                "scheme",
                "stationarity",
                "study",
                "provide"
            ]
        }
    },
    {
        "id": "2efad675-4d24-4cf2-a5c4-be925284a19e",
        "title": "",
        "chunk_text": "One significant difference from the line-search scheme is that the trust-region SQP subprob- lems remain well-defined without any Hessian regularizations, which are often necessary in the line- search scheme to ensure the search direction to be a decent direction. (c) Compared to Cao et al. (2023); Berahas et al.",
        "metadata": {
            "author": "",
            "keywords": [
                "SQP",
                "Hessian",
                "scheme",
                "search",
                "direction",
                "subprob",
                "lems",
                "regularizations",
                "line",
                "significant"
            ]
        }
    },
    {
        "id": "a2a04c74-61e1-4efe-b2a0-c3beec14800a",
        "title": "",
        "chunk_text": "(2025) for unconstrained or constrained first-order complexity analysis, we refine the definition of the stopping criterion of the iteration by using the de- terministic KKT residual for first-order stationarity, and the maximum of deterministic KKT residual and deterministic negative curvature of the reduced Hessian for second-order stationarity. Cao et al. (2023) adopted a stopping criterion that depends on the Lipschitz constant and the irreducible noise level, while Berahas et al.",
        "metadata": {
            "author": "",
            "keywords": [
                "KKT",
                "stationarity",
                "residual",
                "first-order",
                "deterministic",
                "Hessian",
                "analysis",
                "terministic",
                "stopping",
                "criterion"
            ]
        }
    },
    {
        "id": "a9ace061-1441-4335-90c2-f464ce1ea7b1",
        "title": "",
        "chunk_text": "(2025) defined the stopping criterion based on the local model reduction of the merit function. Although different stopping criteria lead to the same complexity bounds, our def- inition seems more natural while necessitating additional efforts in the analysis. (d) Compared to the complexity analysis in Cao et al. (2023); Berahas et al. (2025), we significantly relax the parametric sub-exponential distribution assumption on the zeroth-order oracle noise.",
        "metadata": {
            "author": "",
            "keywords": [
                "stopping",
                "defined",
                "function",
                "criterion",
                "based",
                "local",
                "model",
                "reduction",
                "merit",
                "complexity"
            ]
        }
    },
    {
        "id": "b6f032da-ad6f-46fa-9b28-ce04e3c0a2ae",
        "title": "",
        "chunk_text": "Specifi- cally, instead of requiring infinite-order moments, we assume the noise has a bounded 1+δ moment for any (small) δ > 0. The key technical tools we leverage are the Burkholder-type inequality (Burkholder, 1973; Chen and Sung, 2020) and the martingale Fuk-Nagaev inequality (Fuk, 1973; Nagaev, 1979; Fan 3 et al., 2017), which help us quantify the tail behavior of the accumulated oracle noise.",
        "metadata": {
            "author": "",
            "keywords": [
                "Specifi",
                "cally",
                "small",
                "bounded",
                "noise",
                "inequality",
                "moments",
                "moment",
                "requiring",
                "infinite-order"
            ]
        }
    },
    {
        "id": "7c6a95b4-882b-4a3b-8687-4e8ac98bf78f",
        "title": "",
        "chunk_text": "When δ > 1, our complexity results suggest that the method can find a (first-order or second-order) stationary point within finite iterations almost surely. This matches the result in Berahas et al. (2025), where infinite- order moments are imposed.",
        "metadata": {
            "author": "",
            "keywords": [
                "first-order",
                "second-order",
                "stationary",
                "surely",
                "complexity",
                "suggest",
                "method",
                "find",
                "point",
                "finite"
            ]
        }
    },
    {
        "id": "e826491d-e61f-406f-adc6-a05829d0935f",
        "title": "",
        "chunk_text": "We highlight that our relaxation of the noise condition is achieved purely through sharper analysis and is independent of various line-search or trust-region designs, suggesting that the results in the aforementioned literature should also hold under heavy-tailed noise condition utilizing our analytical techniques. Under irreducible and heavy-tailed oracle noise conditions, we establish first- and second-order high-probability iteration complexity bounds for our trust-region SSQP method.",
        "metadata": {
            "author": "",
            "keywords": [
                "noise",
                "designs",
                "suggesting",
                "techniques",
                "condition",
                "trust-region",
                "heavy-tailed",
                "highlight",
                "relaxation",
                "achieved"
            ]
        }
    },
    {
        "id": "f40f7f26-9e51-4c78-9ea6-fa77411c1e4a",
        "title": "",
        "chunk_text": "In particular, we es- tablish that the method achieves a first-order ϵ-stationary point in O(ϵ−2) iterations and a second- order ϵ-stationary point in O(ϵ−3) iterations with high probability, provided that ϵ is chosen above a threshold determined by the irreducible noise levels. These results align with existing arguments for irreducible and light-tailed noise. Specifically, our results corroborate the first-order high-probability complexity bound established by Berahas et al.",
        "metadata": {
            "author": "",
            "keywords": [
                "point",
                "iterations",
                "ϵ-stationary",
                "tablish",
                "order",
                "probability",
                "provided",
                "levels",
                "irreducible",
                "noise"
            ]
        }
    },
    {
        "id": "1f7e8588-e401-4dc4-9764-7ce418e41eed",
        "title": "",
        "chunk_text": "(2025) for a step-search SSQP method in constrained stochastic optimization, as well as the first- and second-order high-probability complexity bounds de- rived by Cao et al. (2023) for a trust-region method in unconstrained stochastic optimization. We im- plement our method on problems from the CUTEst test set. The actual performance validates our the- oretical analysis. 1.1 Notation We use ∥·∥to denote the ℓ2 norm for vectors and the operator norm for matrices.",
        "metadata": {
            "author": "",
            "keywords": [
                "SSQP",
                "Cao",
                "optimization",
                "method",
                "stochastic",
                "rived",
                "step-search",
                "constrained",
                "second-order",
                "high-probability"
            ]
        }
    },
    {
        "id": "15db2b16-dc7c-44a5-83ed-cab77087226b",
        "title": "",
        "chunk_text": "I denotes the identity matrix and 0 denotes the zero vector/matrix, whose dimensions are clear from the context. For the constraints c(x) : Rd →Rm, we let G(x) := ∇c(x) ∈Rm×d denote its Jacobian matrix and let ci(x) denote the i-th constraint for 1 ≤i ≤m (the subscript indexes the iteration). Define P(x) = I − G(x)T [G(x)G(x)T ]−1G(x) to be the projection matrix to the null space of G(x).",
        "metadata": {
            "author": "",
            "keywords": [
                "matrix",
                "denotes",
                "denote",
                "vector",
                "context",
                "identity",
                "dimensions",
                "clear",
                "Jacobian",
                "iteration"
            ]
        }
    },
    {
        "id": "16eec39d-027e-4e02-a45d-0252d30da680",
        "title": "",
        "chunk_text": "Then, we let Z(x) ∈ Rd×(d−m) form the orthonormal bases of ker(G(x)) such that Z(x)T Z(x) = I and Z(x)Z(x)T = P(x). Throughout the paper, we use an overline to denote a stochastic estimate of a quantity. For example, ¯f(x) denotes an estimate of f(x). 1.2 Structure of the paper We introduce the computation of trial steps and the algorithm design in Section 2. Probabilistic oracles that allow irreducible and heavy-tailed noise are introduced in Section 3.",
        "metadata": {
            "author": "",
            "keywords": [
                "Section",
                "form",
                "ker",
                "orthonormal",
                "bases",
                "estimate",
                "paper",
                "Structure",
                "quantity",
                "denote"
            ]
        }
    },
    {
        "id": "0db1df4a-8634-4361-be2a-8b1179a840e6",
        "title": "",
        "chunk_text": "In Section 4, we establish first- and second-order high-probability complexity bounds. Numerical experiments are presented in Section 5, and conclusions are presented in Section 6. 2 A Trust-Region SSQP Method Let L(x, λ) = f(x)+λT c(x) be the Lagrangian function of Problem (1), where λ ∈Rm represents the Lagrangian multipliers. For any ϵ > 0, we call (x∗, λ∗) a first-order ϵ-stationary point of (1) if ∥∇L(x∗, λ∗)∥= \u0012∇xL(x∗, λ∗) ∇λL(x∗, λ∗) \u0013 = \u0012∇f(x∗) + G(x∗)T λ∗ c(x∗) \u0013 ≤ϵ.",
        "metadata": {
            "author": "",
            "keywords": [
                "Section",
                "Lagrangian",
                "bounds",
                "presented",
                "establish",
                "second-order",
                "high-probability",
                "complexity",
                "Problem",
                "SSQP"
            ]
        }
    },
    {
        "id": "d9119ceb-4061-40f8-b65e-aec30a57dd5a",
        "title": "",
        "chunk_text": "(2) We call (x∗, λ∗) a second-order ϵ-stationary point if, in addition to (2), τ(x∗, λ∗) ≥−ϵ, 4 where τ(x∗, λ∗) denotes the smallest eigenvalue of the reduced Lagrangian Hessian Z(x∗)T ∇2 xL(x∗, λ∗)Z(x∗). Here, we define ∇2 xL(x, λ) = ∇2f(x) + Pm i=1 λi∇2ci(x) as the Lagrangian Hessian with respect to the primal variable x. We denote the optimality residual by ∥∇xL(x, λ)∥, the feasibility residual by ∥∇λL(x, λ)∥(i.e., ∥c(x)∥), and the KKT residual by ∥∇L(x, λ)∥.",
        "metadata": {
            "author": "",
            "keywords": [
                "Lagrangian",
                "Hessian",
                "residual",
                "call",
                "second-order",
                "ϵ-stationary",
                "point",
                "addition",
                "smallest",
                "eigenvalue"
            ]
        }
    },
    {
        "id": "68efbae9-4bf8-4fd9-99ed-547325363f18",
        "title": "",
        "chunk_text": "Given the k-th iterate (xk, λk), we denote gk = ∇f(xk), ∇2fk = ∇2f(xk), and ¯gk, ¯∇2fk as their estimates, which are constructed via probabilistic oracles in Section 3. Similarly, we denote ck, Gk, {∇2ci k}m i=1. We define the estimated Lagrangian gradient as ¯∇Lk = ( ¯∇xLk, ck) with ¯∇xLk = ¯gk + GT k λk, and the estimated Lagrangian Hessian (with respect to x) as ¯∇2 xLk = ¯∇2fk + Pm i=1 λi k∇2ci k.",
        "metadata": {
            "author": "",
            "keywords": [
                "Section",
                "xLk",
                "denote",
                "Lagrangian",
                "iterate",
                "estimates",
                "k-th",
                "constructed",
                "probabilistic",
                "oracles"
            ]
        }
    },
    {
        "id": "d9db60e3-06b4-4796-b1cd-3ddd80b8cf39",
        "title": "",
        "chunk_text": "In the remainder of this section, we first introduce step computation, a key component of our algorithm, and then describe the algorithm.",
        "metadata": {
            "author": "",
            "keywords": [
                "algorithm",
                "section",
                "computation",
                "remainder",
                "introduce",
                "step",
                "key",
                "component",
                "describe"
            ]
        }
    },
    {
        "id": "53b964b5-1a5a-4b5d-9ba7-02daba426033",
        "title": "",
        "chunk_text": "2.1 Step computation Given the iterate xk and the trust-region radius ∆k in the k-th iteration, we formulate the trust-region SSQP subproblem to solve for the trial step ∆xk by using a quadratic approximation of the objective and a linear approximation of the constraints in (1), with an additional trust-region constraint: min ∆x∈Rd 1 2∆xT ¯Hk∆x + ¯gT k ∆x, s.t. ck + Gk∆x = 0, ∥∆x∥≤∆k, (3) where ¯Hk approximates the Lagrangian Hessian ∇2 xLk.",
        "metadata": {
            "author": "",
            "keywords": [
                "trust-region",
                "Step",
                "approximation",
                "Hessian",
                "SSQP",
                "Lagrangian",
                "min",
                "xLk",
                "radius",
                "iteration"
            ]
        }
    },
    {
        "id": "f8329c69-587e-4aed-aaf2-fc8fd6c0f6e3",
        "title": "",
        "chunk_text": "Due to the presence of the trust-region con- straint, we need to deal with a potential infeasibility issue: {∆x ∈Rd : ck + Gk∆x = 0} ∩{∆x ∈Rd : ∥∆x∥≤∆k} = ∅. To address this issue, we follow the existing literature (Vardi, 1985; Byrd et al., 1987; Omojokun, 1989; Fang et al., 2024a,b) by relaxing the linear constraint in (3) via the trial step decomposition. In par- ticular, we decompose the trial step ∆xk into two orthogonal components, ∆xk = wk +tk, which are computed separately.",
        "metadata": {
            "author": "",
            "keywords": [
                "issue",
                "straint",
                "Due",
                "con",
                "trial",
                "presence",
                "trust-region",
                "deal",
                "potential",
                "infeasibility"
            ]
        }
    },
    {
        "id": "144b0477-a416-448c-8c37-bc70e493ee38",
        "title": "",
        "chunk_text": "The normal step wk ∈im(GT k ) accounts for feasibility, while the tangential step tk ∈ker(Gk) accounts for optimality. To satisfy the trust-region constraint, we further decompose the trust-region radius to control the lengths of the normal and tangential steps, respectively. Depending on whether we aim to achieve first- or second-order convergence, the trial step ∆xk is computed using either a gradient step or an eigen step.",
        "metadata": {
            "author": "",
            "keywords": [
                "accounts",
                "step",
                "ker",
                "normal",
                "tangential",
                "feasibility",
                "optimality",
                "trust-region",
                "constraint",
                "steps"
            ]
        }
    },
    {
        "id": "2796ec0a-0ef7-46c7-a5d8-655aa0da096f",
        "title": "",
        "chunk_text": "The gradient step reduces the KKT residual to achieve first- order convergence, whereas the eigen step explores the negative curvature of the reduced Lagrangian Hessian to achieve second-order convergence. However, when searching for second-order stationary points, the Maratos effect (Conn et al., 2000) can cause iterates to stagnate at saddle points.",
        "metadata": {
            "author": "",
            "keywords": [
                "convergence",
                "achieve",
                "KKT",
                "Lagrangian",
                "Hessian",
                "step",
                "order",
                "second-order",
                "points",
                "gradient"
            ]
        }
    },
    {
        "id": "c9cf22a4-e624-4619-b39e-46ea2d2cde57",
        "title": "",
        "chunk_text": "To mitigate this special issue in constrained optimization, we compute a second-order correction (SOC) step and append it to the trial step whenever the Maratos effect is detected. 2.1.1 Gradient steps When computing the gradient step, we decompose the trust-region radius as ˘∆k = ∥cRS k ∥ ∥¯∇LRS k ∥· ∆k and e∆k = ∥¯∇xLRS k ∥ ∥¯∇LRS k ∥· ∆k to control the lengths of wk and tk, respectively, and have ∆xk = wk + tk.",
        "metadata": {
            "author": "",
            "keywords": [
                "SOC",
                "Maratos",
                "LRS",
                "step",
                "Gradient",
                "optimization",
                "correction",
                "detected",
                "mitigate",
                "special"
            ]
        }
    },
    {
        "id": "e690127d-482b-4378-8560-eb56ecd66379",
        "title": "",
        "chunk_text": "Here, cRS k := ck/∥Gk∥, ¯∇xLRS k := ¯∇xLk/∥¯Hk∥, ¯∇LRS k := ( ¯∇xLRS k , cRS k ) denote rescaled feasibility, optimality, and KKT 5 residual vectors, respectively. Decomposing the radius based on the rescaled residuals preserves the scale invariance property, so that if the objective and/or constraints are scaled by a (positive) scalar, the radius decomposition and further step computation remain unchanged.",
        "metadata": {
            "author": "",
            "keywords": [
                "cRS",
                "xLRS",
                "LRS",
                "KKT",
                "xLk",
                "optimality",
                "denote",
                "feasibility",
                "vectors",
                "rescaled"
            ]
        }
    },
    {
        "id": "2ec86abf-d780-43ea-953c-56c6e282208f",
        "title": "",
        "chunk_text": "Such a desirable property cannot be achieved if using original residuals ∥ck∥and ∥¯∇xLk∥in the above radius decomposition. Suppose Gk has full row rank, we define vk := −GT k [GkGT k ]−1ck. Without the trust-region constraint, ck+Gk∆xk = 0 would imply wk = vk since Gktk = 0. However, the trust-region constraint enforces us to set the normal step wk by shrinking vk as wk = ¯γkvk with ¯γk := min{ ˘∆k/∥vk∥, 1}.",
        "metadata": {
            "author": "",
            "keywords": [
                "xLk",
                "residuals",
                "decomposition",
                "desirable",
                "property",
                "achieved",
                "original",
                "radius",
                "trust-region",
                "constraint"
            ]
        }
    },
    {
        "id": "749b6b9e-a892-4fa4-94be-aa87c71974b9",
        "title": "",
        "chunk_text": "(4) The tangential step is set as tk = Zkuk, where Zk ∈Rd×(d−m) forms the orthonormal bases of ker(Gk) and uk ∈Rd−m. We compute uk by (inexactly) solving the following subproblem: min u∈Rd−m m(u) := 1 2uT ZT k ¯HkZku + (¯gk + ¯Hkwk)T Zku, s.t. ∥u∥≤e∆k. (5) We require the inexact solution uk to achieve at least a fixed fraction κfcd ∈(0, 1] of the Cauchy re- duction on m(·); that is m(uk) −m(0) ≤−κfcd 2 ∥ZT k (¯gk + ¯Hkwk)∥min \u001a e∆k, ∥ZT k (¯gk + ¯Hkwk)∥ ∥ZT k ¯HkZk∥ \u001b .",
        "metadata": {
            "author": "",
            "keywords": [
                "Hkwk",
                "Zkuk",
                "forms",
                "ker",
                "tangential",
                "step",
                "set",
                "orthonormal",
                "bases",
                "min"
            ]
        }
    },
    {
        "id": "ca74d1ea-fa0d-4c4e-b84f-cf5fa9a7a0a1",
        "title": "",
        "chunk_text": "(6) Condition (6) is standard in the trust-region literature, which is achievable by computing the Cauchy point of (5) or applying various approaches to solve (5), including the two-dimensional subspace minimization method, the dogleg method, and the Steihaug’s algorithm (see Nocedal and Wright, 2006, Chapter 4 for more details). 2.1.2 Eigen steps We define ¯τk to be the smallest eigenvalue of ZT k ¯HkZk and ¯τ + k := | min{¯τk, 0}|.",
        "metadata": {
            "author": "",
            "keywords": [
                "method",
                "Condition",
                "Chapter",
                "Wright",
                "Cauchy",
                "Steihaug",
                "Nocedal",
                "literature",
                "solve",
                "including"
            ]
        }
    },
    {
        "id": "3c9e1f2c-bf59-4b7f-9f24-56f4f5e0cdf3",
        "title": "",
        "chunk_text": "Parallel to the gradient step ∆xk = wk + tk, the normal step wk is still computed as in (4) but the radius decomposition is replaced by ˘∆k = ∥cRS k ∥ ∥(cRS k , ¯τ RS+ k )∥ · ∆k and e∆k = ¯τ RS+ k ∥(cRS k , ¯τ RS+ k )∥ · ∆k, where ¯τ RS+ k := ¯τ + k /∥¯Hk∥is the rescaled negative curvature. The tangential step is still tk = Zkuk with uk (inexactly) solved from (5), but the reduction condition is adjusted to (¯gk + ¯Hkwk)T Zkuk ≤0, ∥uk∥≤e∆k, uT k ZT k ¯HkZkuk ≤−κfcd · ¯τ + k e∆2 k.",
        "metadata": {
            "author": "",
            "keywords": [
                "cRS",
                "step",
                "Zkuk",
                "Parallel",
                "curvature",
                "gradient",
                "normal",
                "computed",
                "radius",
                "decomposition"
            ]
        }
    },
    {
        "id": "701c58fc-7235-4fc7-b297-4ff4dfd7a103",
        "title": "",
        "chunk_text": "(7) When the above conditions are satisfied, uk achieves the curvature reduction: m(uk) −m(0) ≤−κfcd 2 ¯τ + k e∆2 k < 0, where we use κfcd ∈(0, 1] to denote the fraction in both gradient steps and eigen steps for simplicity. 6 Conditions in (7) are standard in the literature (e.g., see Chapter 7.5 in Conn et al., 2000) and can be satisfied by computing uk in different approaches.",
        "metadata": {
            "author": "",
            "keywords": [
                "κfcd",
                "steps",
                "conditions",
                "reduction",
                "simplicity",
                "satisfied",
                "achieves",
                "curvature",
                "denote",
                "fraction"
            ]
        }
    },
    {
        "id": "242f2c9d-c1b5-49e4-bf8f-f4bd586cafff",
        "title": "",
        "chunk_text": "For example, we can compute ¯ζk, the eigenvector of ZT k ¯HkZk corresponding to the eigenvalue ¯τk, and then rescale it as ¯ζRS k := ±¯ζk · e∆k/∥¯ζk∥. The rescaled vector ¯ζRS k satisfies (7) with κfcd = 1, provided that ± is properly chosen. In practice, it suffices to compute a good approximation of ¯ζk, which can be achieved by employing methods such as truncated conjugate gradient and truncated Lanczos methods. 2.1.3 Second-order correction steps It is observed by Byrd et al.",
        "metadata": {
            "author": "",
            "keywords": [
                "ζRS",
                "HkZk",
                "eigenvalue",
                "eigenvector",
                "rescale",
                "compute",
                "methods",
                "truncated",
                "Lanczos",
                "Second-order"
            ]
        }
    },
    {
        "id": "8692e788-e0e1-4667-81b1-ca777f550868",
        "title": "",
        "chunk_text": "(1987) that when xk is close to a saddle point, the (gradient or eigen) step ∆xk may increase f(x) and ∥c(x)∥simultaneously. Since no reduction is made by ∆xk, the current iterate is not updated. Even worse, this issue cannot be resolved by reducing the stepsize, implying that the iterate is trapped at the saddle point. This phenomenon, known as the (second-order) Maratos effect, arises from the inaccurate linear approximation of constraints.",
        "metadata": {
            "author": "",
            "keywords": [
                "step",
                "simultaneously",
                "point",
                "gradient",
                "eigen",
                "saddle",
                "close",
                "increase",
                "iterate",
                "Maratos"
            ]
        }
    },
    {
        "id": "c7d0680d-2631-4b53-a833-1ae5af3a1f3e",
        "title": "",
        "chunk_text": "To address this, the second-order correction (SOC) step dk is computed to better capture the curvature of the constraints: dk = −GT k [GkGT k ]−1 {c(xk + ∆xk) −ck −Gk∆xk} . (8) We will not compute SOC steps in every iteration, but only when the Maratos effect is likely to occur, that is, when the iterate fails to update and is close to the feasible region (i.e.{x : c(x) = 0}). 2.2 Trust-region SSQP design We now present the design of our trust-region SSQP method and summarize it in Algorithm 1.",
        "metadata": {
            "author": "",
            "keywords": [
                "SOC",
                "Maratos",
                "SSQP",
                "correction",
                "constraints",
                "GkGT",
                "iteration",
                "occur",
                "region",
                "Trust-region"
            ]
        }
    },
    {
        "id": "3d208688-5bc3-4c47-8049-ee0f41d9e5d4",
        "title": "",
        "chunk_text": "Each iteration of the method comprises four components: (i) estimating the objective value, gradient, and Hessian using the probabilistic oracles introduced later in Section 3; (ii) computing a trial step and/or an SOC step as described in Section 2.1; (iii) updating the parameter of the merit function; and (iv) updating the iterate and the trust-region radius. We use α to indicate the final goal of stationarity, with α = 0 for first-order stationarity and α = 1 for second-order stationarity.",
        "metadata": {
            "author": "",
            "keywords": [
                "Section",
                "updating",
                "step",
                "Hessian",
                "SOC",
                "gradient",
                "iii",
                "components",
                "estimating",
                "computing"
            ]
        }
    },
    {
        "id": "ba892cfd-d64e-4ec7-b8b8-4184a93a245b",
        "title": "",
        "chunk_text": "In the k-th iteration, given the iterate xk, the trust-region radius ∆k, and the merit parameter ¯µk, we first obtain the gradient estimate ¯gk. Then, we compute the La- grangian multiplier ¯λk = −[GkGT k ]−1Gk¯gk and the Lagrangian gradient ¯∇Lk = ¯gk + GT k ¯λk. For the Hessian estimate, • if α = 0: we generate any matrix ¯Hk to approximate the Lagrangian Hessian ∇2 xLk and set ¯τ + k = 0.",
        "metadata": {
            "author": "",
            "keywords": [
                "Lagrangian",
                "gradient",
                "estimate",
                "iteration",
                "radius",
                "parameter",
                "Hessian",
                "k-th",
                "iterate",
                "trust-region"
            ]
        }
    },
    {
        "id": "d84a1d4b-e2e3-4670-9d0f-b2378fa8b15c",
        "title": "",
        "chunk_text": "• if α = 1: we obtain the Hessian estimate ¯∇2fk, compute ¯Hk = ¯∇2fk + Pm i=1 ¯λi k∇2ci k, and set ¯τk as the smallest eigenvalue of ZT k ¯HkZk and ¯τ + k = | min{¯τk, 0}|. Then, we decide whether to perform a gradient step or an eigen step by checking ∥¯∇Lk∥min \u001a ∆k, ∥¯∇Lk∥ ∥¯Hk∥ \u001b ≥¯τ + k ∆k (∆k + ∥ck∥) . (9) If (9) holds, we compute the gradient step ∆xk as in Section 2.1.1; otherwise, we compute the eigen step ∆xk as in Section 2.1.2.",
        "metadata": {
            "author": "",
            "keywords": [
                "compute",
                "min",
                "Hessian",
                "Section",
                "step",
                "estimate",
                "set",
                "HkZk",
                "obtain",
                "smallest"
            ]
        }
    },
    {
        "id": "3097eb27-ed84-4004-9269-2173a9a51d43",
        "title": "",
        "chunk_text": "Next, we define the predicted (local) model reduction of an (estimated) ℓ2 merit function Lµ(x) = f(x) + µ∥c(x)∥as Predk = ¯gT k ∆xk + 1 2∆xT k ¯Hk∆xk + ¯µk(∥ck + Gk∆xk∥−∥ck∥), (10) 7 and update the merit parameter ¯µk ←ρ¯µk until Predk ≤−κfcd 2 max \u001a ∥¯∇Lk∥min \u001a ∆k, ∥¯∇Lk∥ ∥¯Hk∥ \u001b , ¯τ + k ∆k (∆k + ∥ck∥) \u001b . (11) With the updated merit parameter, we compute the (estimated) actual reduction of the merit function L¯µk(x).",
        "metadata": {
            "author": "",
            "keywords": [
                "merit",
                "Predk",
                "estimated",
                "parameter",
                "reduction",
                "function",
                "local",
                "κfcd",
                "max",
                "min"
            ]
        }
    },
    {
        "id": "9011d168-64f9-4e68-82ec-8395080e67e7",
        "title": "",
        "chunk_text": "Specifically, we set xsk = xk+∆xk, obtain the function estimates ¯fk and ¯fsk, and compute the (estimated) actual reduction as Aredk = ¯L¯µk(xsk) −¯L¯µk(xk) = ¯fsk −¯fk + ¯µk(∥csk∥−∥ck∥). (12) Finally, we check the following conditions to update xk and ∆k: (a): Aredk −ϑα Predk ≥η and (b): max \u001a ∥¯∇Lk∥ max{1, ∥¯Hk∥}, ¯τ + k \u001b ≥η∆k, (13) where ϑα = 2ϵf if α = 0 and ϑα = 2ϵf +ϵ3/2 g if α = 1. (ϵf, ϵg > 0 are irreducible noise levels in the ob- jective value and gradient estimates, respectively.",
        "metadata": {
            "author": "",
            "keywords": [
                "fsk",
                "xsk",
                "Aredk",
                "max",
                "Specifically",
                "estimated",
                "csk",
                "estimates",
                "obtain",
                "actual"
            ]
        }
    },
    {
        "id": "17a7039d-fcc6-4cd2-8571-f64fdc25829a",
        "title": "",
        "chunk_text": "See Section 3 for details). • Case 1: (13a) holds. We update the iterate as xk+1 = xsk. Furthermore, if (13b) holds, we increase the trust-region radius by ∆k+1 = min{γ∆k, ∆max}. Otherwise, we decrease the trust-region radius by ∆k+1 = ∆k/γ. • Case 2: (13a) does not hold and α = 1. In this case, we decide whether to perform a SOC step to recheck (13a). If ∥ck∥≤r, we compute a SOC step dk (cf. Section 2.1.3), set xsk = xk +∆xk +dk and re-estimate ¯fsk.",
        "metadata": {
            "author": "",
            "keywords": [
                "Case",
                "Section",
                "SOC",
                "details",
                "trust-region",
                "radius",
                "xsk",
                "holds",
                "step",
                "min"
            ]
        }
    },
    {
        "id": "b176dcf9-dc4c-4a50-ae8c-2e660601d461",
        "title": "",
        "chunk_text": "Then, we recompute Aredk as in (12) and recheck (13a). If (13a) holds, we go to Case 1 above; if (13a) does not hold, we go to Case 3 below. Otherwise, if ∥ck∥> r, the SOC step is not triggered and we directly go to Case 3 below. • Case 3: (13a) does not hold and α = 0. We do not update the current iterate by setting xk+1 = xk, and decrease the trust-region radius as ∆k+1 = ∆k/γ.",
        "metadata": {
            "author": "",
            "keywords": [
                "Case",
                "Aredk",
                "recheck",
                "recompute",
                "hold",
                "SOC",
                "holds",
                "step",
                "triggered",
                "directly"
            ]
        }
    },
    {
        "id": "024caa56-20ff-4ecd-bcd3-6dc10bb04a08",
        "title": "",
        "chunk_text": "When ϑα = 0, (13a) reduces to the standard condition in both deterministic and stochastic trust- region methods (see Powell and Yuan, 1990; Byrd et al., 1987; Omojokun, 1989; Heinkenschloss and Ridzal, 2014; Bandeira et al., 2014; Fang et al., 2024b). Inspired by Cao et al. (2023), we introduce a relaxation term ϑα in (13a) to accommodate the irreducible noise in the probabilistic oracles.",
        "metadata": {
            "author": "",
            "keywords": [
                "Omojokun",
                "Yuan",
                "Byrd",
                "Heinkenschloss",
                "Ridzal",
                "Bandeira",
                "Fang",
                "Powell",
                "reduces",
                "trust"
            ]
        }
    },
    {
        "id": "976993f2-766f-488f-87b0-56b60b593649",
        "title": "",
        "chunk_text": "Since both Aredk and Predk are negative, subtracting a ϑα > 0 in the numerator increases the value on the left-hand side of (13a), making it more likely for the iterate to be updated. When xk is updated, we determine whether to increase the trust-region radius based on (13b). In principle, (13b) quantifies how confident we are about the trust-region approximation at the given stationarity.",
        "metadata": {
            "author": "",
            "keywords": [
                "Aredk",
                "Predk",
                "updated",
                "negative",
                "subtracting",
                "making",
                "numerator",
                "left-hand",
                "side",
                "iterate"
            ]
        }
    },
    {
        "id": "cf0712e7-724a-4c54-b870-33a4f54016d9",
        "title": "",
        "chunk_text": "When (13b) is satisfied, the trust-region radius is increased to allow for a larger trial step in the next iteration, potentially enabling greater progress. Conversely, if (13b) is not satisfied, the trust-region radius is reduced to ensure more cautious movement in the subsequent iteration. We end this section by formalizing the randomness.",
        "metadata": {
            "author": "",
            "keywords": [
                "satisfied",
                "trust-region",
                "radius",
                "iteration",
                "potentially",
                "progress",
                "increased",
                "larger",
                "trial",
                "step"
            ]
        }
    },
    {
        "id": "206046ac-6745-4e14-9908-81f7cb2d30d8",
        "title": "",
        "chunk_text": "We define F−1 ⊆F0 ⊆F1 · · · as a filtration of σ-algebras, where Fk−1 = σ({xi}k i=0), ∀k ≥0, contains all the randomness before performing the k-th iteration. Defining Fk−0.5 = σ({xi}k i=0 ∪{¯gk, ¯∇2fk}), we find for all k ≥0, σ(xk, ∆k) ⊆Fk−1 and σ(∆xk, ¯λk, ¯µk, dk) ⊆Fk−0.5. 8 Algorithm 1 A Trust-Region SSQP Method 1: Input: Initial iterate x0 and radius ∆0 = ∆max, and parameters η ∈(0, 1), κfcd ∈(0, 1], ¯µ0, r, ϵf, ϵg > 0, ρ, γ > 1.",
        "metadata": {
            "author": "",
            "keywords": [
                "σ-algebras",
                "iteration",
                "define",
                "filtration",
                "randomness",
                "performing",
                "k-th",
                "Algorithm",
                "Input",
                "Method"
            ]
        }
    },
    {
        "id": "f33cb3fd-90a2-4c57-bb0c-2858d093e7a5",
        "title": "",
        "chunk_text": "2: Set α = 0 for first-order stationarity and α = 1 for second-order stationarity. 3: for k = 0, 1, · · · do 4: Obtain ¯gk and compute ¯λk and ¯∇Lk. 5: If α = 1, obtain ¯∇2fk, compute ¯Hk and the smallest eigenvalue ¯τk of ZT k ¯HkZk, and set ¯τ + k = | min{¯τk, 0}|. Otherwise, let ¯Hk be certain approximation of ∇2Lk and set ¯τ + k = 0. 6: If (9) holds, compute ∆xk as a gradient step; otherwise, compute ∆xk as an eigen step. 7: Perform ¯µk ←ρ¯µk until Predk satisfies (11).",
        "metadata": {
            "author": "",
            "keywords": [
                "compute",
                "stationarity",
                "Obtain",
                "Set",
                "first-order",
                "second-order",
                "step",
                "Perform",
                "Predk",
                "HkZk"
            ]
        }
    },
    {
        "id": "94ec361b-aa2e-474a-9574-1d03ea19d095",
        "title": "",
        "chunk_text": "8: Set xsk = xk + ∆xk, obtain ¯fk, ¯fsk, and compute Aredk as in (12). 9: if (13a) holds then ▷(Case 1) 10: Set xk+1 = xsk. 11: if (13b) holds then 12: Set ∆k+1 = min{γ∆k, ∆max}. 13: else 14: Set ∆k+1 = ∆k/γ. 15: end if 16: else if α = 1 and ∥ck∥≤r then ▷(Case 2) 17: Compute SOC step dk, set xsk = xk + ∆xk + dk, re-estimate ¯fsk, and recompute Aredk. 18: If (13a) holds, perform Lines 10-15; otherwise, perform Line 20. 19: else ▷(Case 3) 20: Set xk+1 = xk, ∆k+1 = ∆k/γ.",
        "metadata": {
            "author": "",
            "keywords": [
                "Set",
                "Case",
                "xsk",
                "holds",
                "Aredk",
                "obtain",
                "fsk",
                "compute",
                "perform",
                "Lines"
            ]
        }
    },
    {
        "id": "67fd66b8-f623-4f9c-a3f6-50b4d9d5cfc8",
        "title": "",
        "chunk_text": "21: end if 22: Set ¯µk+1 = ¯µk. 23: end for 3 Probabilistic Oracles with Irreducible and Heavy-Tailed Noise In this section, we introduce probabilistic oracles that accommodate irreducible and heavy-tailed noise. In each iteration, the zeroth-, first-, and second-order probabilistic oracles generate estimates of the ob- jective value ¯f(x, ξ), the objective gradient ¯g(x, ξ), and the objective Hessian ¯∇2f(x, ξ), respectively, where ξ denotes a random variable defined on some probability space.",
        "metadata": {
            "author": "",
            "keywords": [
                "Set",
                "Probabilistic",
                "Oracles",
                "end",
                "Irreducible",
                "Noise",
                "Heavy-Tailed",
                "objective",
                "Hessian",
                "section"
            ]
        }
    },
    {
        "id": "1eb9a747-c562-4232-92c6-9d3b50b9b416",
        "title": "",
        "chunk_text": "These estimates are required to satisfy certain adaptive accuracy conditions with a high but fixed probability. Our oracle design does not prespecify a specific method for generating estimates and allows for estimates to have irreducible noise (ϵf, ϵg, ϵh below), leading to biased estimates. This is a significant extension of Bandeira et al. (2014); Blanchet et al. (2019); Fang et al. (2024b); Na et al. (2022a, 2023), which required the esti- mation noise to diminish with a fixed probability.",
        "metadata": {
            "author": "",
            "keywords": [
                "estimates",
                "probability",
                "satisfy",
                "adaptive",
                "accuracy",
                "conditions",
                "high",
                "fixed",
                "required",
                "Blanchet"
            ]
        }
    },
    {
        "id": "b64a13d8-0922-4995-8932-ece1b514d73d",
        "title": "",
        "chunk_text": "Furthermore, we remove all parametric assumptions about the estimation noise. In particular, Cao et al. (2023); Berahas et al. (2025) assumed that the noise in the zeroth-order oracle follows a sub- exponential distribution in each iteration (see (19) below). Such a condition essentially assumes the ex- istence of infinite-order moments, thereby excluding all distributions with heavy tails and limiting the scope of problems their analysis can cover.",
        "metadata": {
            "author": "",
            "keywords": [
                "remove",
                "parametric",
                "assumptions",
                "estimation",
                "Cao",
                "noise",
                "Berahas",
                "assumed",
                "exponential",
                "iteration"
            ]
        }
    },
    {
        "id": "bdb3772c-bd09-4e17-84db-8a848424e65d",
        "title": "",
        "chunk_text": "Additionally, the light-tailed noise condition in the zeroth-order oracle contrasts with the noise conditions in the first- and second-order oracles, which indeed allow for heavy-tailed noise. To resolve this discrepancy, as shown in (18), we relax the noise 9 condition to require only the existence of a (1+δ)-order moment for any small δ > 0, thereby covering noise from broad class of heavy-tailed distributions.",
        "metadata": {
            "author": "",
            "keywords": [
                "noise",
                "Additionally",
                "condition",
                "heavy-tailed",
                "light-tailed",
                "zeroth-order",
                "contrasts",
                "second-order",
                "oracle",
                "oracles"
            ]
        }
    },
    {
        "id": "5e10cbba-6677-435d-886b-5b5e72bae088",
        "title": "",
        "chunk_text": "Let ϵh, ϵg, ϵf > 0, eϵf ∈(0, ϵf], κh, κg, κf ≥0, and ph, pg, pf ∈(0, 1) be user-specified parameters, ∆k denote the trust-region radius, and recall that α = 0 corresponds to finding first-order stationary points and α = 1 corresponds to finding second-order stationary points. We first introduce the probabilistic second-order oracle defined at the current iterate xk.",
        "metadata": {
            "author": "",
            "keywords": [
                "corresponds",
                "finding",
                "stationary",
                "points",
                "eϵf",
                "parameters",
                "radius",
                "second-order",
                "user-specified",
                "denote"
            ]
        }
    },
    {
        "id": "7b45d743-bfb9-4a11-ad79-6a58d2537c5b",
        "title": "",
        "chunk_text": "The or- acle indicates that the noise of the objective Hessian estimate satisfies an accuracy condition of ϵh + O(∆k) with probability at least 1 −ph; that is, it allows for irreducible noise no less than ϵh even if ∆k →0 (cf. Fang et al. (2024b), Corollary 4.14). Definition 3.1 (Probabilistic second-order oracle). When α = 1, given xk, the oracle computes ¯∇2fk := ¯∇2f(xk, ξh k), an estimate of the objective Hessian ∇2fk, such that Ak = \b ∥¯∇2fk −∇2fk∥≤ϵh + κh∆k satisfies P(Ak | Fk−1) ≥1 −ph.",
        "metadata": {
            "author": "",
            "keywords": [
                "Corollary",
                "Fang",
                "Hessian",
                "noise",
                "objective",
                "acle",
                "accuracy",
                "condition",
                "probability",
                "irreducible"
            ]
        }
    },
    {
        "id": "390f05fb-5cee-4115-bd9f-416aa85bd3ad",
        "title": "",
        "chunk_text": "(14) Recall from Section 2.2 that for first-order convergence (α = 0), we do not have to estimate the ob- jective Hessian. Instead, we only require an arbitrary matrix ¯Hk to superficially approximate the La- grangian Hessian, as long as its norm is bounded with fixed probability. Specifically, when α = 0, we impose (14) but re-define Ak = \b ∥¯Hk∥≤κB for some constant κB ≥1. Next, we introduce the probabilistic first-order oracle defined at xk.",
        "metadata": {
            "author": "",
            "keywords": [
                "Hessian",
                "Recall",
                "Section",
                "convergence",
                "jective",
                "estimate",
                "first-order",
                "Specifically",
                "matrix",
                "grangian"
            ]
        }
    },
    {
        "id": "280a184a-3ffb-4c6b-9180-e171c233bfdd",
        "title": "",
        "chunk_text": "It indicates that the noise of the gradient estimate satisfies an accuracy condition of ϵg +O(∆α+1 k ) with probability at least 1−pg; thus, it allows for irreducible noise with a level at least ϵg. Definition 3.2 (Probabilistic first-order oracle). Given xk, the oracle computes ¯gk := ¯g(xk, ξg k), an estimate of the objective gradient gk, such that Bk = {∥¯gk −gk∥≤ϵg + κg∆α+1 k } satisfies P(Bk | Fk−1) ≥1 −pg.",
        "metadata": {
            "author": "",
            "keywords": [
                "noise",
                "accuracy",
                "condition",
                "probability",
                "irreducible",
                "level",
                "Probabilistic",
                "gradient",
                "estimate",
                "satisfies"
            ]
        }
    },
    {
        "id": "10f60655-3d18-4157-b05e-c1f890d20275",
        "title": "",
        "chunk_text": "(15) Finally, we introduce the zeroth-order probabilistic oracle, which is defined at both the current it- erate xk and the trial iterate xsk (since we need to evaluate the objective value for both points). Here, xsk = xk + ∆xk if the SOC step is not performed and xsk = xk + ∆xk + dk if the SOC step is performed. Definition 3.3 (Probabilistic zeroth-order oracle).",
        "metadata": {
            "author": "",
            "keywords": [
                "Finally",
                "xsk",
                "SOC",
                "probabilistic",
                "oracle",
                "erate",
                "points",
                "step",
                "introduce",
                "defined"
            ]
        }
    },
    {
        "id": "7f47846a-56d0-4695-a366-3efa4a535a13",
        "title": "",
        "chunk_text": "Given xk and xsk, the oracle computes ¯f(xk, ξf k) and ¯f(xsk, ξf sk), which are estimates of the objective function value f(xk) and f(xsk). Let e(x, ξ) = | ¯f(x, ξ) −f(x)| be the absolute error with ek := e(xk, ξf k) and esk := e(xsk, ξf sk). The zeroth-order oracle has to satisfy the following three conditions. •(i) The absolute errors ek and esk are sufficiently small with a fixed probability: Ck = \b max (ek, esk) ≤ϵf + κf∆α+2 k satisfies P(Ck | Fk−1/2) ≥1 −pf.",
        "metadata": {
            "author": "",
            "keywords": [
                "xsk",
                "esk",
                "computes",
                "absolute",
                "estimates",
                "objective",
                "function",
                "oracle",
                "error",
                "errors"
            ]
        }
    },
    {
        "id": "a4507ca9-3298-415c-b23c-0ae3b6af2a7a",
        "title": "",
        "chunk_text": "(16) •(ii) The mean absolute errors are sufficiently small: max \b E[ek | Fk−1], E[esk | Fk−1/2] ≤eϵf. (17) •(iii) One of the following tail conditions is satisfied: (iii.1) Heavy-tailed condition. {ek}k have bounded 1 + δ moment for some δ > 0 (same for {esk}k). In other words, for some constant Υf > 0, we have max n E h |ek −E [ek | Fk−1]|1+δ | Fk−1 i , E h esk −E \u0002 esk | Fk−1/2 \u0003 1+δ | Fk−1/2 io ≤Υf. (18) 10 (iii.2) Sub-exponential tail condition.",
        "metadata": {
            "author": "",
            "keywords": [
                "esk",
                "eϵf",
                "small",
                "absolute",
                "errors",
                "sufficiently",
                "Heavy-tailed",
                "max",
                "tail",
                "condition"
            ]
        }
    },
    {
        "id": "f70ddc4b-9e40-4e3f-85b0-7e0258666b45",
        "title": "",
        "chunk_text": "For some constants v, b > 0, ek and esk satisfy max n E h eλ(ek−E[ek|Fk−1]) | Fk−1 i , E h eλ(esk−E[esk|Fk−1/2]) | Fk−1/2 io ≤exp \u0012λ2v2 2 \u0013 , ∀λ ∈[0, 1 b]. (19) When (iii.1) heavy-tailed condition is satisfied, we say ek, esk are generated via the proba- bilistic heavy-tailed zeroth-order oracle, while when (iii.2) sub-exponential condition is satisfied, we say ek, esk are generated via the probabilistic sub-exponential zeroth-order oracle. Remark 3.4.",
        "metadata": {
            "author": "",
            "keywords": [
                "esk",
                "exp",
                "satisfied",
                "oracle",
                "generated",
                "constants",
                "satisfy",
                "max",
                "condition",
                "zeroth-order"
            ]
        }
    },
    {
        "id": "7c34b9e3-1db3-496b-8cdb-3366c4e81316",
        "title": "",
        "chunk_text": "We further discuss the probabilistic zeroth-order oracle in this remark. • The conditions (i) and (ii) in (16) and (17) are standard in the literature (Berahas et al., 2025; Cao et al., 2023), which, however, do not provide any tail information regarding the noise. Thus, existing literature often additionally imposed (iii.2) for the considered zero-order oracle.",
        "metadata": {
            "author": "",
            "keywords": [
                "remark",
                "discuss",
                "probabilistic",
                "zeroth-order",
                "oracle",
                "Berahas",
                "Cao",
                "literature",
                "conditions",
                "noise"
            ]
        }
    },
    {
        "id": "60ed2f74-36b3-49dc-aebb-de4f1183c6d9",
        "title": "",
        "chunk_text": "• For the sub-exponential oracle, if eϵf < ϵf, then condition (16) can be implied by (19), provided that v, b are sufficiently small (i.e., the probability mass of ek, esk is almost concentrated around their means). See Remark 4.22 for a rigorous discussion. However, (19) should only be used to capture the tail behavior of ek, esk, and requiring infinitesimally small scalars of v, b is overly stringent for the oracle (though the tail behavior remains sub-exponential).",
        "metadata": {
            "author": "",
            "keywords": [
                "eϵf",
                "condition",
                "provided",
                "oracle",
                "esk",
                "implied",
                "sufficiently",
                "probability",
                "mass",
                "concentrated"
            ]
        }
    },
    {
        "id": "916dac8f-ea20-4afe-9b49-56543aa954b8",
        "title": "",
        "chunk_text": "Thus, one may not need to consider the interconnections among conditions (16)–(19). • The heavy-tailed condition in (iii.1) significantly relaxes the sub-exponential condition in (iii.2) by reducing the requirement from the existence of infinite-order moments to a finite 1+δ moment.",
        "metadata": {
            "author": "",
            "keywords": [
                "condition",
                "interconnections",
                "conditions",
                "significantly",
                "finite",
                "heavy-tailed",
                "relaxes",
                "sub-exponential",
                "reducing",
                "requirement"
            ]
        }
    },
    {
        "id": "6a9778db-70f4-409e-ae25-c3bb45410cb6",
        "title": "",
        "chunk_text": "Lever- aging the Burkholder-type inequality (Burkholder, 1973; Chen and Sung, 2020) and the martingale Fuk-Nagaev inequality (Fuk, 1973; Nagaev, 1979; Fan et al., 2017), we establish high-probability com- plexity bounds for any δ > 0. However, we would like to emphasize a crucial gap in the convergence properties between the cases δ ∈(0, 1] and δ ∈(1, ∞).",
        "metadata": {
            "author": "",
            "keywords": [
                "Burkholder",
                "Fuk",
                "Nagaev",
                "inequality",
                "Chen",
                "Sung",
                "Fan",
                "Lever",
                "Burkholder-type",
                "aging"
            ]
        }
    },
    {
        "id": "af117217-8211-4f03-86f7-6d65d4ddb703",
        "title": "",
        "chunk_text": "As demonstrated in Theorem 4.16 and the sub- sequent discussion, when δ ∈(1, ∞), the algorithm finds a (first- and second-order) ϵ-stationary point in a finite number of iterations almost surely. In contrast, this almost-sure convergence property is not guaranteed when δ ∈(0, 1], indicating that a bounded second moment in the objective value esti- mation noise is insufficient to ensure almost-sure convergence.",
        "metadata": {
            "author": "",
            "keywords": [
                "Theorem",
                "sequent",
                "discussion",
                "second-order",
                "ϵ-stationary",
                "surely",
                "demonstrated",
                "algorithm",
                "finds",
                "point"
            ]
        }
    },
    {
        "id": "f0a3f669-6f67-4b63-8e68-9e3afc8fc392",
        "title": "",
        "chunk_text": "This discrepancy highlights a key gap between our non-asymptotic analysis and the asymptotic analysis in Fang et al. (2024b), where almost- sure convergence can be established when the noise in objective value estimation has a bounded second moment. • In the zeroth-order oracle, we follow the existing literature and impose conditions on the absolute objective value estimation errors ek and esk separately.",
        "metadata": {
            "author": "",
            "keywords": [
                "Fang",
                "analysis",
                "discrepancy",
                "highlights",
                "key",
                "gap",
                "non-asymptotic",
                "asymptotic",
                "objective",
                "estimation"
            ]
        }
    },
    {
        "id": "85765262-361c-4ce9-98ac-6b0d020d3d17",
        "title": "",
        "chunk_text": "However, we note that imposing accuracy conditions on the error difference esk −ek is sufficient for the analysis. That is, we can redefine ek := |( ¯fsk −¯fk) −(fsk −fk)| and impose oracle conditions on the newly define ek. To end this section, we discuss the sample size required to satisfy the above introduced probabilis- tic oracles when we draw independent samples from a distribution P and use sample average as the es- timates of the objective quantities.",
        "metadata": {
            "author": "",
            "keywords": [
                "fsk",
                "conditions",
                "esk",
                "analysis",
                "note",
                "imposing",
                "accuracy",
                "error",
                "difference",
                "sufficient"
            ]
        }
    },
    {
        "id": "ae372385-3a8c-499a-9e9f-30ddf48fef7e",
        "title": "",
        "chunk_text": "Denoting the sample sets for estimating ¯∇2fk, ¯gk, ¯fk, ¯fsk by ξh k, ξg k, ξf k, ξf sk respectively, and the sample size by |·|, when each realization of the objective Hessian, gradient, and value estimates is unbiased with bounded variance, the oracle conditions are satisfied provided |ξh k| ≥ Ch ph(ϵh + κh∆k)2 , |ξg k| ≥ Cg pg(ϵg + κg∆α+1 k )2 , min{|ξf k|, |ξf sk|} ≥ Cf min{pf(ϵf + κf∆α+2 k )2,eϵ2 f} (20) 11 for some constants Ch, Cg, Cf > 0 (by Chebyshev’s inequality).",
        "metadata": {
            "author": "",
            "keywords": [
                "min",
                "sample",
                "Hessian",
                "Chebyshev",
                "gradient",
                "Denoting",
                "estimating",
                "fsk",
                "variance",
                "provided"
            ]
        }
    },
    {
        "id": "c8c17313-767b-471b-8d83-d561ac24a85e",
        "title": "",
        "chunk_text": "Such results indicate that samples of sizes O(ϵ−2 h ), O(ϵ−2 g ), O(eϵ−2 f ) are sufficient for generating Hessian, gradient and value estimates. 4 High-Probability Complexity Bounds In this section, we analyze the first- and second-order high-probability complexity bounds for the present trust-region SSQP method. In Section 4.1, we introduce fundamental lemmas.",
        "metadata": {
            "author": "",
            "keywords": [
                "Hessian",
                "Complexity",
                "section",
                "gradient",
                "estimates",
                "Bounds",
                "results",
                "samples",
                "sizes",
                "sufficient"
            ]
        }
    },
    {
        "id": "f835423b-cfc3-479f-87aa-4460abac5f4a",
        "title": "",
        "chunk_text": "In Sections 4.2 and 4.3, we establish the complexity bounds under probabilistic heavy-tailed and sub-exponential zeroth-order oracles, respectively. We first state the assumption. Assumption 4.1. Let Ω⊆Rd be an open convex set containing the iterates and trial points {xk, xsk}. The objective f(x) is twice continuously differentiable and bounded below by finf over Ω. The gradi- ent ∇f(x) and Hessian ∇2f(x) are both Lipschitz continuous over Ω, with constants L∇f and L∇2f, respectively.",
        "metadata": {
            "author": "",
            "keywords": [
                "Sections",
                "oracles",
                "assumption",
                "establish",
                "complexity",
                "bounds",
                "probabilistic",
                "heavy-tailed",
                "sub-exponential",
                "zeroth-order"
            ]
        }
    },
    {
        "id": "2a126663-372e-477d-af7c-8328123e177c",
        "title": "",
        "chunk_text": "Analogously, the constraint c(x) is twice continuously differentiable, its Jacobian G(x) is Lipschitz continuous over Ωwith the constant LG. For 1 ≤i ≤m, the Hessian of the i-th constraint, ∇2ci(x), is Lipschitz continuous over Ωwith constant the L∇2c. We also assume that there exist constants κc, κ∇f, κ1,G, κ2,G > 0 such that ∥ck∥≤κc, ∥∇fk∥≤κ∇f, κ1,G · I ⪯GkGT k ⪯κ2,G · I, ∀k ≥0.",
        "metadata": {
            "author": "",
            "keywords": [
                "Lipschitz",
                "Ωwith",
                "Jacobian",
                "Analogously",
                "continuous",
                "differentiable",
                "constraint",
                "continuously",
                "constant",
                "Hessian"
            ]
        }
    },
    {
        "id": "96256f12-ab68-4b25-8ad1-b66e558d477a",
        "title": "",
        "chunk_text": "Assumption 4.1 is standard in the SQP literature (Byrd et al., 1987; Powell and Yuan, 1990; El-Alem, 1991; Conn et al., 2000; Berahas et al., 2021, 2024; Curtis et al., 2024b; Fang et al., 2024a,b). To analyze first-order complexity, it suffices to assume that f(x) and c(x) are continuously differen- tiable, without conditions on Hessians ∇2f(x) and ∇2ci(x). Assumption 4.1 implies that Gk has full row rank, √κ1,G ≤∥Gk∥≤√κ2,G and ∥GT k [GkGT k ]−1∥≤1/√κ1,G.",
        "metadata": {
            "author": "",
            "keywords": [
                "Byrd",
                "Powell",
                "Yuan",
                "Conn",
                "Berahas",
                "Curtis",
                "Fang",
                "Assumption",
                "SQP",
                "El-Alem"
            ]
        }
    },
    {
        "id": "73366981-8fda-4803-a4d1-3aff3c30463e",
        "title": "",
        "chunk_text": "Consequently, both the true La- grangian multiplier λk = −[GkGT k ]−1Gk∇fk and the estimated counterpart ¯λk = −[GkGT k ]−1Gk¯gk are well defined. Additionally, ∥∇2f(x)∥≤L∇f and ∥∇2ci(x)∥≤LG for 1 ≤i ≤m over Ω. 4.1 Fundamental lemmas In this section, we analyze basic properties of the trust-region SSQP method. Lemmas in this section apply to both the heavy-tailed and the sub-exponential oracles. We first show that on the event Ak ∩Bk (cf.",
        "metadata": {
            "author": "",
            "keywords": [
                "GkGT",
                "grangian",
                "counterpart",
                "defined",
                "true",
                "multiplier",
                "estimated",
                "lemmas",
                "section",
                "Fundamental"
            ]
        }
    },
    {
        "id": "0179bb1d-0e9e-4048-9997-17a5c3d17e7c",
        "title": "",
        "chunk_text": "(14) and (15)), the Hessian estimate ¯Hk constructed for second-order convergence has an upper bound. Lemma 4.2. Under Assumption 4.1 with α = 1, there exists a positive constant κB ≥1 such that ∥¯Hk∥≤κB on the event Ak ∩Bk. Proof. See Appendix A.1. ■ In the next lemma, we demonstrate that for the second-order stationarity, the difference between the true Lagrangian Hessian ∇2 xLk and its estimate ¯Hk is bounded by O(∆k)+O(ϵh+ϵg) on the event Ak ∩Bk.",
        "metadata": {
            "author": "",
            "keywords": [
                "bound",
                "Hessian",
                "constructed",
                "convergence",
                "upper",
                "estimate",
                "Lemma",
                "event",
                "second-order",
                "Assumption"
            ]
        }
    },
    {
        "id": "5f28f645-30dc-4056-9281-cb5c4666700c",
        "title": "",
        "chunk_text": "Furthermore, the difference between τ + k := | min{τk, 0}| and its estimate ¯τ + k is bounded by the same quantity. This lemma ensures that ¯τ + k is an accurate estimate of τ + k provided that both the objective gradient and Hessian estimates are accurate. Lemma 4.3. Under Assumption 4.1 with α = 1, we have ∥∇2 xLk−¯Hk∥≤ϵH+κH∆k and |τ + k −¯τ + k | ≤ ϵH + κH∆k on the event Ak ∩Bk, where κH := κh + √mκgLG∆max √κ1,G and ϵH := ϵh + √mLG √κ1,G ϵg. 12 Proof. See Appendix A.2.",
        "metadata": {
            "author": "",
            "keywords": [
                "min",
                "estimate",
                "quantity",
                "difference",
                "bounded",
                "accurate",
                "lemma",
                "Hessian",
                "estimates",
                "Proof"
            ]
        }
    },
    {
        "id": "d87c100e-b78c-491d-b9a2-afa4bab26fc5",
        "title": "",
        "chunk_text": "■ Let us define Lsk ¯µk := L¯µk(xsk) and Lk ¯µk := L¯µk(xk), where ¯µk is the merit parameter in the k-th iteration after the update (11). Here, xsk = xk + ∆xk if the SOC step is not performed and xsk = xk+∆xk+dk if the SOC step is performed. The following two lemmas examine the difference between the reduction in the merit function Lsk ¯µk−Lk ¯µk and the predicted reduction Predk in (10) among the first T iterations, for any finite T.",
        "metadata": {
            "author": "",
            "keywords": [
                "xsk",
                "Lsk",
                "SOC",
                "update",
                "merit",
                "step",
                "define",
                "parameter",
                "k-th",
                "performed"
            ]
        }
    },
    {
        "id": "c7106927-6a58-4ba9-8590-844390887d5b",
        "title": "",
        "chunk_text": "We first show that on the event Ak ∩Bk and without the SOC step, the difference has an upper bound ϵg∆k + O(∆2 k). Lemma 4.4. Under Assumption 4.1, for any T ≥1 and any 0 ≤k ≤T −1, on the event Ak ∩Bk and suppose the SOC step is not performed, we have Lsk ¯µk −Lk ¯µk −Predk ≤ϵg∆k + Υ1∆2 k, where Υ1 = κg max{1, ∆max} + 1 2(L∇f + κB + ¯µT−1LG). Proof. See Appendix A.3. ■ Next, we show that on the event Ak ∩Bk with the SOC step, the difference between the two reduc- tions is ϵ3/2 g +O(∆2 k +∆3 k).",
        "metadata": {
            "author": "",
            "keywords": [
                "SOC",
                "event",
                "step",
                "upper",
                "bound",
                "max",
                "show",
                "difference",
                "Predk",
                "Assumption"
            ]
        }
    },
    {
        "id": "b7a21625-a282-4a8d-81de-dd83e6404cc3",
        "title": "",
        "chunk_text": "Compared to Lemma 4.4, the error bound includes an additional constant ϵ3/2 g , while the linear term of ∆k is removed. Lemma 4.5. Under Assumption 4.1, for any T ≥1 and any 0 ≤k ≤T −1, on the event Ak ∩Bk and suppose the SOC step is performed, we have Lsk ¯µk −Lk ¯µk −Predk ≤ϵ3/2 g + \u00121 2ϵh + √mLG 2√κ1,G ϵg \u0013 ∆2 k + Υ2∆3 k, where Υ2 = κg + 1 + L∇2f + κh 2 + L2 G∆max(0.5L∇f + √m¯µT−1LG) κ1,G + 0.5√mL∇2c(L∇f∆max + κ∇f) + 0.5√mLG(κg∆max + L∇f + 2¯µT−1LG) √κ1,G . Proof. See Appendix A.4.",
        "metadata": {
            "author": "",
            "keywords": [
                "max",
                "Lemma",
                "mLG",
                "Compared",
                "constant",
                "removed",
                "error",
                "bound",
                "includes",
                "additional"
            ]
        }
    },
    {
        "id": "ed1e11e1-f37f-4af9-8b10-e50bfde7d427",
        "title": "",
        "chunk_text": "■ For our non-asymptotic analysis, we recall the definitions of first- and second-order stationarity points in Section 2 and define the stopping time based on ϵ-stationary points. Definition 4.6 (Stopping time). For first-order stationarity (α = 0), the stopping time is defined as the index of the first iteration in which the true KKT residual enters the neighborhood of radius ϵ: Tϵ = min{k : ∥∇Lk∥≤ϵ}.",
        "metadata": {
            "author": "",
            "keywords": [
                "stopping",
                "Section",
                "points",
                "time",
                "analysis",
                "non-asymptotic",
                "recall",
                "second-order",
                "define",
                "based"
            ]
        }
    },
    {
        "id": "472f90f8-95ab-414b-80c1-335c224149df",
        "title": "",
        "chunk_text": "For second-order stationarity (α = 1), the stopping time is defined as the index of the first iteration in which the true KKT residual and the true eigenvalue both enter the neighborhood of radius ϵ: Tϵ = min{k : max(∥∇Lk∥, τ + k ) ≤ϵ}. 13 We define the stopping time directly using the deterministic KKT residual and the negative eigen- value of the reduced Lagrangian Hessian. In comparison, Berahas et al.",
        "metadata": {
            "author": "",
            "keywords": [
                "true",
                "KKT",
                "stopping",
                "residual",
                "time",
                "min",
                "max",
                "stationarity",
                "second-order",
                "defined"
            ]
        }
    },
    {
        "id": "1a97b5ac-9422-40b0-bd32-0dc5d6015275",
        "title": "",
        "chunk_text": "(2025) defined the first-order stopping time indirectly through the model reduction of the merit function. When transforming the model reduction to the deterministic KKT residual, the minimum of the true merit parameter is in- volved, making it challenging to measure the true KKT residual and desiring a more involved analysis. In addition, the stopping time in Cao et al. (2023) depends on the Lipschitz constants and the irre- ducible noise level. Similar to Berahas et al. (2025); Cao et al.",
        "metadata": {
            "author": "",
            "keywords": [
                "KKT",
                "model",
                "reduction",
                "merit",
                "Cao",
                "defined",
                "function",
                "stopping",
                "time",
                "residual"
            ]
        }
    },
    {
        "id": "c33165a7-2d26-4e3e-9f99-493673e71cd3",
        "title": "",
        "chunk_text": "(2023), we require ϵ to exceed a threshold determined by irreducible noise levels ϵf, ϵg, and ϵh. Specifically, we impose the following assumption. Assumption 4.7. For the stationarity level ϵ, we assume If α = 0 : ϵ > 1 Υ3 s 8γ4¯µT−1ϵf κfcdη3¯µ0(p −1/2) + Υ4 Υ3 ϵg, If α = 1 : ϵ > 1 Υ5 3 s 2¯µT−1γ6 max{∆max, 1}(4ϵf + ϵ3/2 g ) κfcdη3¯µ0(p −1/2) + Υ6 Υ5 ϵg + Υ7 Υ5 ϵh, where Υ3, Υ4, Υ5, Υ6, Υ7 are constants independent to irreducible noise levels and are defined in Lemmas 4.8 and 4.9.",
        "metadata": {
            "author": "",
            "keywords": [
                "max",
                "noise",
                "assumption",
                "require",
                "exceed",
                "threshold",
                "determined",
                "irreducible",
                "levels",
                "Lemmas"
            ]
        }
    },
    {
        "id": "ddda2d0a-b24d-4edf-bbc0-3eded8096aaa",
        "title": "",
        "chunk_text": "Furthermore, the failure oracle probabilities ph, pg, pf are small enough such that 1 −ph −pg −2pf =: p ∈(1/2, 1). Here, p represents the lower bound of probability that Ak∩Bk∩Ck∩C′ k holds, where C′ k denotes the event that accurate ¯fk, ¯fsk are regenerated after computing the SOC step. For notational consistency, we assume C′ k holds when the SOC step is not performed.",
        "metadata": {
            "author": "",
            "keywords": [
                "SOC",
                "failure",
                "oracle",
                "probabilities",
                "small",
                "holds",
                "step",
                "accurate",
                "fsk",
                "represents"
            ]
        }
    },
    {
        "id": "953c096e-4905-4eb3-ab3a-d26ecb4e2f85",
        "title": "",
        "chunk_text": "As demonstrated in Assumption 4.7, for first-order stationarity, we need ϵ ≥O(√ϵf + ϵg); for second-order stationarity, we need ϵ ≥O( 3√ϵf + √ϵg + ϵh). These orders matches the ones in Berahas et al. (2025); Cao et al. (2023). In the next two lemmas, we show that before the algorithm terminates, if Ak ∩Bk ∩Ck ∩C′ k holds and the trust-region radius is sufficiently small, then both two conditions in (13) hold. We first consider α = 0. Lemma 4.8.",
        "metadata": {
            "author": "",
            "keywords": [
                "stationarity",
                "Assumption",
                "demonstrated",
                "first-order",
                "second-order",
                "Berahas",
                "Cao",
                "orders",
                "matches",
                "lemmas"
            ]
        }
    },
    {
        "id": "87263205-1c9a-4ea7-a611-8c1063d0b84a",
        "title": "",
        "chunk_text": "Under Assumptions 4.1, 4.7, and the event Ak ∩Bk ∩Ck ∩C′ k, for α = 0 and k < Tϵ, if ∆k ≤Υ3∥∇Lk∥−Υ4ϵg (21) with Υ3, Υ4 given by Υ3 = κfcd(1 −η) 4κf + κg + 2Υ1 + κB and Υ4 = κfcd(1 −η) + 2 4κf + κg + 2Υ1 + κB , then (13a) and (13b) both hold. Proof. See Appendix A.5. ■ In the next lemma, we show the same result holds for α = 1. Lemma 4.9.",
        "metadata": {
            "author": "",
            "keywords": [
                "κfcd",
                "Assumptions",
                "event",
                "lemma",
                "Appendix",
                "Proof",
                "hold",
                "holds",
                "show",
                "result"
            ]
        }
    },
    {
        "id": "e225bf51-a0bc-432e-bfce-9b81b7b76043",
        "title": "",
        "chunk_text": "Under Assumptions 4.1, 4.7, and the event Ak ∩Bk ∩Ck ∩C′ k, for α = 1 and k < Tϵ, if ∆k ≤Υ5 max{τ + k , ∥∇Lk∥} −Υ6ϵg −Υ7ϵh (22) 14 with Υ5, Υ6, Υ7 given by Υ5 = (1 −η)κfcd min{1, r} (4κf + κg) max{∆max, 1} + κB + 2Υ1 + 2Υ2 + (κH + η)(1 −η)κfcd min{1, r}, Υ6 = κfcd(1 −η) + 2 + 2√mLG/√κ1,G (4κf + κg) max{∆max, 1} + κB + 2Υ1 + 2Υ2 + (κH + η)(1 −η)κfcd min{1, r}, Υ7 = 2 4κf max{∆max, 1} + 2Υ1 + 2Υ2 + (κH + η)(1 −η)κfcd min{1, r}, then (13a) and (13b) both hold.",
        "metadata": {
            "author": "",
            "keywords": [
                "max",
                "κfcd",
                "min",
                "Assumptions",
                "mLG",
                "hold",
                "event"
            ]
        }
    },
    {
        "id": "552aa3c2-a38d-4ed4-8d06-74c79108e8a9",
        "title": "",
        "chunk_text": "Here, κH is derived in Lemma 4.3, Υ1 is defined in Lemma 4.4, and Υ2 is defined in Lemma 4.5. Proof. See Appendix A.6. ■ In the next lemma, we consider the reduction on the stochastic merit function by iterations where (13a) and (13b) both hold. For brevity, we define the function hα(·) as hα(∆) =      κfcd 2 η3∆2 for α = 0, κfcd 2 max{∆max, 1}η3∆3 for α = 1. Lemma 4.10.",
        "metadata": {
            "author": "",
            "keywords": [
                "Lemma",
                "defined",
                "κfcd",
                "max",
                "derived",
                "Appendix",
                "function",
                "Proof",
                "hold",
                "brevity"
            ]
        }
    },
    {
        "id": "a2f92d5d-4101-4c88-bbc4-4a6c72c03327",
        "title": "",
        "chunk_text": "Under Assumption 4.1 and the event Ak ∩Bk ∩Ck ∩C′ k, if (13a) and (13b) both hold in the k-th iteration, then we have ¯Lk+1 ¯µk −¯Lk ¯µk ≤−hα(∆k) + ϑα. Proof. See Appendix A.7. ■ To further proceed our analysis, we manipulate the lower bound of ϵ in Assumption 4.7 and define the following threshold b∆= ( Υ3ϵ −Υ4ϵg for α = 0, Υ5ϵ −Υ6ϵg −Υ7ϵh for α = 1. (23) Note that for k < Tϵ, we have ∥∇Lk∥> ϵ when α = 0 and max{∥∇Lk∥, τ + k } > ϵ when α = 1.",
        "metadata": {
            "author": "",
            "keywords": [
                "Assumption",
                "iteration",
                "event",
                "hold",
                "k-th",
                "Proof",
                "Appendix",
                "Note",
                "analysis",
                "max"
            ]
        }
    },
    {
        "id": "8107fe8d-36bc-470e-932d-ea94d05273e2",
        "title": "",
        "chunk_text": "There- fore, b∆is smaller than the upper bounds of conditions in Lemmas 4.8 and 4.9. This implies that when ∆k ≤b∆and Ak∩Bk∩Ck∩C′ k holds, conclusions in both lemmas will apply. Without loss of generality, we suppose b∆≤∆max. Otherwise, we can always increase ∆max to ensure this condition. In fact, when α = 0, b∆is independent of ∆max; thus, a large ∆max trivially ensures b∆≤∆max. When α = 1, by Assumption 4.7, b∆grows at a rate of 3√∆max; thus, increasing ∆max still ensures b∆≤∆max.",
        "metadata": {
            "author": "",
            "keywords": [
                "max",
                "Lemmas",
                "fore",
                "smaller",
                "upper",
                "bounds",
                "ensures",
                "Assumption",
                "holds",
                "conclusions"
            ]
        }
    },
    {
        "id": "dbea29c0-024a-494d-a336-a27110dfeb84",
        "title": "",
        "chunk_text": "The trust-region radius ∆k, k ≥0 always has the formula ∆k = γl∆0 with ∆0 = ∆max, where l is a non-positive integer. Therefore, we define the following threshold: b∆′ = max l∈Z {γl∆0 : γl∆0 ≤γ−1 b∆}. The threshold b∆′ is the largest trust-region radius that is less or equal to γ−1 b∆. Remark 4.11. We require b∆′ ≤γ−1 b∆instead of b∆′ ≤b∆. This additional γ−1 ensures that when ∆k ≤b∆′ and the trust-region radius increases in the k-th iteration, we will have ∆k+1 = γ∆k ≤∆max.",
        "metadata": {
            "author": "",
            "keywords": [
                "max",
                "trust-region",
                "radius",
                "formula",
                "integer",
                "non-positive",
                "threshold",
                "define",
                "Remark",
                "largest"
            ]
        }
    },
    {
        "id": "336e86a8-f923-45f8-bea2-31c8646812d4",
        "title": "",
        "chunk_text": "This property is essential to our analysis, as utilized in Lemma 4.12 (c). 15 To derive the complexity bounds, we categorize iterations k = 0, 1, · · · , T −1 into different classes by defining the following indicator variables: • Ik for accurate iteration: We set the indicator Ik = 1 if Ak ∩Bk ∩Ck ∩C′ k holds and call the k-th iteration accurate. Otherwise we set Ik = 0 and call the k-th iteration inaccurate.",
        "metadata": {
            "author": "",
            "keywords": [
                "Lemma",
                "analysis",
                "iteration",
                "call",
                "k-th",
                "property",
                "essential",
                "utilized",
                "set",
                "indicator"
            ]
        }
    },
    {
        "id": "9433f1f0-beb2-43d3-9f5c-879ff264e74e",
        "title": "",
        "chunk_text": "• Θk for sufficient iteration: We set the indicator Θk = 1 if (13a) and (13b) both hold and call the k-th iteration sufficient. Otherwise we set Θk = 0 and call the k-th iteration insufficient. • Λk for large iteration: We set the indicator Λk = 1 if min{∆k, ∆k+1} ≥b∆′ and call the k-th iteration large. We set Λk = 0 if max{∆k, ∆k+1} ≤b∆′ and call the k-th iteration small. Note that min{∆k, ∆k+1} ≥b∆′ and max{∆k, ∆k+1} ≤b∆′ are mutually disjoint.",
        "metadata": {
            "author": "",
            "keywords": [
                "call",
                "k-th",
                "iteration",
                "set",
                "sufficient",
                "indicator",
                "hold",
                "min",
                "max",
                "large"
            ]
        }
    },
    {
        "id": "49ef56d6-aa08-4a31-aa8b-1a8ba33e3596",
        "title": "",
        "chunk_text": "To see why, consider the only possible case where min{∆k, ∆k+1} = b∆′ = max{∆k, ∆k+1}, which implies ∆k = ∆k+1 = b∆′. However, by the definition of b∆′, we have b∆′ ≤γ−1∆max. According to the algorithm design, the trust-region radius updates as either ∆k+1 = γ∆k or ∆k+1 = ∆k/γ, leading to a contradiction. In the next lemma, we characterize the relations of iterations in different classes. Lemma 4.12.",
        "metadata": {
            "author": "",
            "keywords": [
                "max",
                "min",
                "implies",
                "case",
                "lemma",
                "definition",
                "design",
                "leading",
                "contradiction",
                "algorithm"
            ]
        }
    },
    {
        "id": "5d3db365-4162-4589-81c0-92cadc24a6e0",
        "title": "",
        "chunk_text": "Under Assumptions 4.1, 4.7, for any T ≥1, we have (a) PT−1 k=0 ΛkΘk ≥PT−1 k=0 Λk(1 −Θk) −logγ(∆0/b∆′); (b) PT−1 k=0 ΛkΘk ≥1 2 PT−1 k=0 Λk −0.5 logγ(∆0/b∆′); (c) PT−1 k=0 (1 −Λk)(1 −Θk) ≥PT−1 k=0 (1 −Λk)Θk; (d) PT k=0(1 −Λk)Ik ≤PT k=0(1 −Λk)(1 −Ik). Proof. See Appendix A.8. ■ Remark 4.13. We discuss the insights behind the above lemma.",
        "metadata": {
            "author": "",
            "keywords": [
                "ΛkΘk",
                "logγ",
                "Assumptions",
                "Appendix",
                "Remark",
                "Proof",
                "lemma",
                "discuss",
                "insights"
            ]
        }
    },
    {
        "id": "f303f97d-c6da-4df4-92c4-f8cc55f2fd2f",
        "title": "",
        "chunk_text": "• In Lemma 4.12 (a), we demonstrate that among the first T iterations, the number of iterations that are large and sufficient (i.e., ΛkΘk = 1) is no less that the number of iterations that are large and insufficient (i.e., Λk(1 −Θk) = 1), subtracted by a constant logγ(∆0/b∆′). This conclusion is intuitive: when an iteration is large, its trust-region radius is lower bounded by a constant b∆′.",
        "metadata": {
            "author": "",
            "keywords": [
                "Lemma",
                "number",
                "large",
                "ΛkΘk",
                "iterations",
                "constant",
                "sufficient",
                "insufficient",
                "subtracted",
                "logγ"
            ]
        }
    },
    {
        "id": "8036c72b-1a79-4955-b90d-2645dd169fcd",
        "title": "",
        "chunk_text": "Thus, to maintain this lower bound, among all large iterations, the number of sufficient iterations (in which the radius increases) must be no less than the number of insufficient iterations (in which the radius decreases), except for a constant logγ(∆0/b∆′) representing the smallest number of iterations to reduce the radius from ∆0 to b∆′.",
        "metadata": {
            "author": "",
            "keywords": [
                "number",
                "radius",
                "iterations",
                "bound",
                "increases",
                "decreases",
                "logγ",
                "representing",
                "maintain",
                "lower"
            ]
        }
    },
    {
        "id": "c734fb81-34f7-4abb-a1a5-bd1bf8374104",
        "title": "",
        "chunk_text": "• In Lemma 4.12 (c), we show that the number of iterations that are small and insufficient (i.e., (1 −Λk)(1 −Θk) = 1) is no less than the number of iterations that are small and sufficient (i.e., (1 − Λk)Θk = 1). In fact, when an iteration is small, its trust-region radius is upper bounded by b∆′.",
        "metadata": {
            "author": "",
            "keywords": [
                "number",
                "Lemma",
                "small",
                "iterations",
                "insufficient",
                "sufficient",
                "show",
                "iteration",
                "fact",
                "trust-region"
            ]
        }
    },
    {
        "id": "de7ee924-3204-45ce-8d0e-9bed6a1732ca",
        "title": "",
        "chunk_text": "Thus, to maintain the upper bound, among all small iterations, the number of iterations that are sufficient (in which the radius increases) must be no more than the number of iterations that are insufficient (in which the radius decreases). The next lemma follows from the conclusion that P(Ik = 1 | Fk−1/2) ≥p (recall p = 1−ph −pg − 2pf) and the Azuma-Hoeffding inequality, which establishes a probabilistic bound for the number of accurate iterations. 16 Lemma 4.14.",
        "metadata": {
            "author": "",
            "keywords": [
                "iterations",
                "number",
                "radius",
                "bound",
                "sufficient",
                "increases",
                "insufficient",
                "decreases",
                "lemma",
                "maintain"
            ]
        }
    },
    {
        "id": "13387b50-c7af-4331-a3c5-27ee0aae3ce4",
        "title": "",
        "chunk_text": "For all T ≥1, bp ∈[0, p) with p ∈(1/2, 1), and both α = 0 and α = 1, we have P \"T−1 X k=0 Ik < bp T # ≤exp \u001a −(bp −p)2 2p2 T \u001b . Proof. See Appendix A.9. ■ In the following lemma, we consider an event that will happen with probability zero. Lemma 4.15. For all T ≥1, p ∈(1/2, 1), and both α = 0 and α = 1, we have P ( Tϵ > T −1, T−1 X k=0 Ik ≥pT, T−1 X k=0 ΘkIkΛk < \u0012 p −1 2 \u0013 T −1 2 logγ \u0012∆0 b∆ \u0013 −1 ) = 0. Proof. See Appendix A.10.",
        "metadata": {
            "author": "",
            "keywords": [
                "Proof",
                "Appendix",
                "exp",
                "lemma",
                "event",
                "happen",
                "probability",
                "ΘkIkΛk",
                "logγ"
            ]
        }
    },
    {
        "id": "898c64b1-535a-4950-acb9-45a1390800ec",
        "title": "",
        "chunk_text": "■ Until now, we have presented all the preparatory lemmas that hold for both heavy-tailed and sub- exponential zeroth-order oracles. In the next two subsections, we establish the first- and second-order complexity bounds for these two oracles separately.",
        "metadata": {
            "author": "",
            "keywords": [
                "exponential",
                "oracles",
                "presented",
                "preparatory",
                "lemmas",
                "hold",
                "heavy-tailed",
                "zeroth-order",
                "subsections",
                "separately"
            ]
        }
    },
    {
        "id": "a77cddb8-fec3-4a08-80d6-e33f55ed7664",
        "title": "",
        "chunk_text": "4.2 Complexity bounds with heavy-tailed zeroth-order oracle In this section, we establish the first and second-order high-probability complexity bounds when the noise of the objective value estimate follows the probabilistic heavy-tailed zeroth-order oracle (i.e., conditions (i), (ii), and (iii.1) of Definition 3.3). We will demonstrate that P{Tϵ ≤T −1} ≥a probability that converges to 1 as T →∞.",
        "metadata": {
            "author": "",
            "keywords": [
                "Complexity",
                "zeroth-order",
                "Definition",
                "bounds",
                "heavy-tailed",
                "oracle",
                "conditions",
                "section",
                "establish",
                "second-order"
            ]
        }
    },
    {
        "id": "bec0d066-9d19-454d-97c3-281b0751a49b",
        "title": "",
        "chunk_text": "In the next theorem, we establish a generic iteration complexity bound of our method, which uni- fies the analysis of first and second-order stationarity. The corresponding complexity bounds are then derived in subsequent corollaries. Theorem 4.16. Under Assumptions 4.1, 4.7, and the probabilistic heavy-tailed zeroth-order oracle (i.e., bounded 1 + δ moment), for any s ≥0, when bp ∈ 1 2 + ¯µT−1(2ϵf + ϑα + 2s) ¯µ0hα(γ−2 b∆) , p !",
        "metadata": {
            "author": "",
            "keywords": [
                "complexity",
                "method",
                "uni",
                "fies",
                "stationarity",
                "theorem",
                "establish",
                "generic",
                "iteration",
                "analysis"
            ]
        }
    },
    {
        "id": "a7d5b77c-6c37-4c8c-a7d2-de18f1840af6",
        "title": "",
        "chunk_text": "(24) and T > bp −0.5 ¯µT−1 −2ϵf + ϑα + 2s ¯µ0hα(γ−2 b∆) !−1 · f(x0) + ¯µ0∥c(x0)∥−finf ¯µ0hα(γ−2 b∆) + logγ(∆0/b∆) + 2 2¯µT−1 ! , (25) we have P{Tϵ ≤T −1} ≥1 −exp \u0012 −(p −bp)2 2p2 T \u0013 −2 exp  −2(ϵf −eϵf + s)2 (3 + δ)2e1+δΥ 2 1+δ f · T  − 32+δΥf (ϵf −eϵf + s)1+δ · T −δ. (26) 17 Proof. See Appendix B. ■ In the above theorem, the existence of s ≥0 is ensured by Assumption 4.7 along with the defini- tions of b∆in (23) and ϑα in (13).",
        "metadata": {
            "author": "",
            "keywords": [
                "exp",
                "eϵf",
                "finf",
                "logγ",
                "δΥf",
                "Proof",
                "Appendix",
                "Assumption",
                "theorem",
                "defini"
            ]
        }
    },
    {
        "id": "d5611e5d-be3a-4404-a4f5-058384a718a7",
        "title": "",
        "chunk_text": "In particular, we have 0.5+ ¯µT−1(2ϵf +ϑα)/{¯µ0hα(γ−2 b∆)} < p for both α = 0 and α = 1. With Theorem 4.16, we derive in the following corollary the complexity bound for first-order ϵ-stationarity. The corollary follows directly from substituting hα(γ−2 b∆) = κfcd 2 η3γ−4(Υ3ϵ −Υ4ϵg)2 and ϑα = 2ϵf for α = 0 into the conclusion in Theorem 4.16. Corollary 4.17.",
        "metadata": {
            "author": "",
            "keywords": [
                "Theorem",
                "corollary",
                "ϵ-stationarity",
                "κfcd",
                "derive",
                "complexity",
                "bound",
                "first-order",
                "directly",
                "substituting"
            ]
        }
    },
    {
        "id": "01e92d61-43e6-402d-b628-06024a4d8059",
        "title": "",
        "chunk_text": "Under conditions of Theorem 4.16, for first-order stationarity (α = 0), the conclu- sion (26) holds given bp ∈ \u00121 2 + 4γ4¯µT−1(2ϵf + s) ¯µ0κfcdη3(Υ3ϵ −Υ4ϵg)2 , p \u0013 (27) and T > \u0012 bp −0.5 ¯µT−1 − 4γ4(2ϵf + s) ¯µ0κfcdη3(Υ3ϵ −Υ4ϵg)2 \u0013−1 · \u00122γ4(f(x0) + ¯µ0∥c(x0)∥−finf) ¯µ0κfcdη3(Υ3ϵ −Υ4ϵg)2 + logγ{∆0/(Υ3ϵ −Υ4ϵg)} + 2 2¯µT−1 \u0013 .",
        "metadata": {
            "author": "",
            "keywords": [
                "Theorem",
                "sion",
                "finf",
                "logγ",
                "stationarity",
                "conclu",
                "holds",
                "conditions",
                "first-order"
            ]
        }
    },
    {
        "id": "76618c82-34f7-41d6-96e3-f9cb01392dab",
        "title": "",
        "chunk_text": "(28) Next, we establish the high-probability complexity bound for second-order ϵ-stationarity by sub- stituting hα(γ−2 b∆) = κfcd 2 max{∆max,1}η3γ−6(Υ5ϵ−Υ6ϵg −Υ7ϵh)3 and ϑα = 2ϵf +ϵ3/2 g for α = 1 into the conclusion in Theorem 4.16. Corollary 4.18. Under conditions of Theorem 4.16, for second-order stationarity (α = 1), the con- clusion (26) holds given bp ∈ 1 2 + 2γ6¯µT−1 max{∆max, 1}(4ϵf + ϵ3/2 g + 2s) ¯µ0κfcdη3(Υ5ϵ −Υ6ϵg −Υ7ϵh)3 , p !",
        "metadata": {
            "author": "",
            "keywords": [
                "max",
                "Theorem",
                "κfcd",
                "stituting",
                "second-order",
                "establish",
                "high-probability",
                "complexity",
                "bound",
                "ϵ-stationarity"
            ]
        }
    },
    {
        "id": "e8eca701-566f-459f-a0f6-7fbdb07b3020",
        "title": "",
        "chunk_text": "(29) and T > bp −0.5 ¯µT−1 −2γ6 max{∆max, 1}(4ϵf + ϵ3/2 g + 2s) ¯µ0κfcdη3(Υ5ϵ −Υ6ϵg −Υ7ϵh)3 !−1 · 2γ6 max{∆max, 1}(f(x0) + ¯µ0∥c(x0)∥−finf) ¯µ0κfcdη3(Υ5ϵ −Υ6ϵg −Υ7ϵh)3 + logγ ∆0 Υ5ϵ−Υ6ϵg−Υ7ϵh + 2 2¯µT−1 ! . (30) The above corollaries indicate that our method achieves an iteration complexity of O(ϵ−2) for find- ing a first-order ϵ-stationary point and O(ϵ−3) for finding a second-order ϵ-stationary point with high probability. These complexity bounds match the results in Berahas et al.",
        "metadata": {
            "author": "",
            "keywords": [
                "max",
                "finf",
                "logγ",
                "ϵ-stationary",
                "point",
                "complexity",
                "Berahas",
                "find",
                "ing",
                "probability"
            ]
        }
    },
    {
        "id": "8059dc4f-2dd3-4a70-9d66-365113eddbb6",
        "title": "",
        "chunk_text": "(2025) for a step-search SSQP method in identifying a first-order ϵ-stationary point in constrained optimization, and the results in Cao et al. (2023) for a trust-region method in identifying both first- and second-order ϵ- stationary points in unconstrained optimization. However, our analysis only requires ek and esk to have a finite 1 + δ moment, rather than the sub-exponential tail condition assumed in Cao et al. (2023); Berahas et al. (2025), thereby allowing for noise with heavy tails.",
        "metadata": {
            "author": "",
            "keywords": [
                "SSQP",
                "Cao",
                "optimization",
                "method",
                "identifying",
                "step-search",
                "first-order",
                "ϵ-stationary",
                "constrained",
                "results"
            ]
        }
    },
    {
        "id": "8af5b49f-5721-4df9-951c-20b2bd460a97",
        "title": "",
        "chunk_text": "18 Remark 4.19. In this remark, we demonstrate the techniques used in establishing the iteration com- plexity and discuss the sample complexity. • The heavy-tailed nature of the noise introduces significant challenges in complexity analysis. To this end, we employ the Burkholder-type inequality (Burkholder, 1973; Chen and Sung, 2020) and martin- gale Fuk–Nagaev inequality (Fuk, 1973; Nagaev, 1979; Fan et al., 2017) to study the tail behavior of ek and esk. In contrast, Cao et al.",
        "metadata": {
            "author": "",
            "keywords": [
                "Remark",
                "Fuk",
                "Nagaev",
                "complexity",
                "inequality",
                "Burkholder",
                "Chen",
                "Sung",
                "Fan",
                "Cao"
            ]
        }
    },
    {
        "id": "caa1b60f-c797-454d-9760-bd186c17fedf",
        "title": "",
        "chunk_text": "(2023); Berahas et al. (2025) utilized concentration techniques for sub-exponential variables, which we also apply in Section 4.3. We mention that the techniques used in analyzing heavy-tailed noise are independent to the algorithm designs, implying that same analysis can be applied to established trust-region and line-search methods (Cao et al., 2023; Berahas et al., 2025) and derive the same complexity bounds.",
        "metadata": {
            "author": "",
            "keywords": [
                "Berahas",
                "Section",
                "techniques",
                "Cao",
                "utilized",
                "variables",
                "designs",
                "implying",
                "methods",
                "bounds"
            ]
        }
    },
    {
        "id": "f6e9dafa-dfa9-4e23-a0af-7d5837c43398",
        "title": "",
        "chunk_text": "• Due to the heavy-tailed noise, P{Tϵ ≤T −1} no longer grows to 1 at an exponential rate as in Cao et al. (2023); Berahas et al. (2025), but is instead reduced to a polynomial rate of O(T −δ) for δ > 0. When δ > 1, it follows from (26) that the failure probability is summable, i.e., P∞ T=1(1 −P{Tϵ ≤ T −1}) < ∞.",
        "metadata": {
            "author": "",
            "keywords": [
                "Due",
                "Cao",
                "noise",
                "heavy-tailed",
                "longer",
                "grows",
                "exponential",
                "rate",
                "Berahas",
                "summable"
            ]
        }
    },
    {
        "id": "ad881042-9e09-4dd4-b3d3-f3a59c221542",
        "title": "",
        "chunk_text": "By the Borel–Cantelli lemma, this ensures that for both first-order and second-order stationarity, our method finds an ϵ-stationary point in a finite number of iterations with probability 1. However, this argument cannot hold for δ ∈(0, 1], illustrating a gap of the asymptotic behavior between δ ∈(0, 1] and δ > 1. • We analyze the total sample complexity of the method, following the discussion at the end of Section 3.",
        "metadata": {
            "author": "",
            "keywords": [
                "Borel",
                "Cantelli",
                "lemma",
                "stationarity",
                "probability",
                "ensures",
                "first-order",
                "second-order",
                "finds",
                "ϵ-stationary"
            ]
        }
    },
    {
        "id": "7d6ceb0c-c98d-46fc-8b08-b5d55bc2bc5d",
        "title": "",
        "chunk_text": "By Assumption 4.7, achieving first-order stationarity requires ϵ ≥O(√ϵf + ϵg), leading to the relations ϵf ≈O(ϵ2) and ϵg ≈O(ϵ). Similarly, for second-order stationarity, where ϵ ≥ O( 3√ϵf + √ϵg + ϵh), we obtain ϵf ≈O(ϵ3), ϵg ≈O(ϵ2), and ϵh ≈O(ϵ).",
        "metadata": {
            "author": "",
            "keywords": [
                "Assumption",
                "achieving",
                "leading",
                "stationarity",
                "first-order",
                "requires",
                "relations",
                "Similarly",
                "second-order",
                "obtain"
            ]
        }
    },
    {
        "id": "1ac28011-bbc9-4b16-b880-f3c4da28c2a0",
        "title": "",
        "chunk_text": "Since estimating the objective value, gradient, and Hessian in each iteration requires sample sizes of O(eϵ−2 f ), O(ϵ−2 g ), and O(ϵ−2 h ) (see (20)), we conclude that when eϵf = O(ϵf), achieving first-order ϵ- stationarity requires O(ϵ−4) and O(ϵ−2) samples per iteration to estimate the objective value and gra- dient, respectively. Given O(ϵ−2) iteration complexity, the total sample complexity amounts to O(ϵ−6).",
        "metadata": {
            "author": "",
            "keywords": [
                "objective",
                "Hessian",
                "requires",
                "iteration",
                "gradient",
                "dient",
                "eϵf",
                "achieving",
                "stationarity",
                "gra"
            ]
        }
    },
    {
        "id": "df217ed3-9462-4fc1-b17c-3f6d8dc8d276",
        "title": "",
        "chunk_text": "Analogously, for second-order ϵ-stationarity, O(ϵ−6), O(ϵ−4), and O(ϵ−2) samples are required per iteration for estimating the objective value, gradient, and Hessian, respectively. Combined with O(ϵ−3) iteration complexity, the total sample complexity amounts to O(ϵ−9). We mention that the total sample complexity of O(ϵ−6) for first-order ϵ-stationarity matches the result in Jin et al.",
        "metadata": {
            "author": "",
            "keywords": [
                "Hessian",
                "Analogously",
                "gradient",
                "sample",
                "complexity",
                "iteration",
                "total",
                "second-order",
                "required",
                "estimating"
            ]
        }
    },
    {
        "id": "e5054c9b-a960-486f-b7d4-12de2b730bbc",
        "title": "",
        "chunk_text": "(2024), which analyzed a trust-region method for identifying first-order ϵ-stationary points in unconstrained optimization. However, this result is worse than that in Curtis et al. (2023b), which established a total sample complexity of O(ϵ−4) for a line-search method. The discrepancy arises because both our method and Jin et al. (2024) require objective value estimation at each iteration, leading to a per-iteration sample complexity of O(ϵ−4).",
        "metadata": {
            "author": "",
            "keywords": [
                "optimization",
                "method",
                "analyzed",
                "trust-region",
                "identifying",
                "first-order",
                "ϵ-stationary",
                "points",
                "unconstrained",
                "complexity"
            ]
        }
    },
    {
        "id": "feafea5e-4880-47c5-9d30-8d3debb6261a",
        "title": "",
        "chunk_text": "If we adopt an objective-value-free design as in Curtis et al. (2023b), where only gradient estimates are required, the per-iteration sample complexity will reduce to O(ϵ−2), yielding a total sample complexity of O(ϵ−4). However, as demon- strated in Berahas et al. (2025), objective-value-free methods tend to perform worse in practice than those using objective value estimates, especially in the presence of irreducible noise.",
        "metadata": {
            "author": "",
            "keywords": [
                "Curtis",
                "design",
                "adopt",
                "sample",
                "complexity",
                "estimates",
                "Berahas",
                "required",
                "yielding",
                "gradient"
            ]
        }
    },
    {
        "id": "61fcc3e0-71ee-48c1-8493-454dd2d2d9f9",
        "title": "",
        "chunk_text": "4.3 Complexity bounds with sub-exponential zeroth-order oracle In this section, we suppose that the noise in the objective value estimate follows the probabilistic sub- exponential zeroth-order oracle (i.e., conditions (i), (ii), and (iii.2) of Definition 3.3), and we demon- strate that the same high-probability complexity bounds hold as in the case of heavy-tailed oracles.",
        "metadata": {
            "author": "",
            "keywords": [
                "Complexity",
                "Definition",
                "bounds",
                "zeroth-order",
                "conditions",
                "oracle",
                "section",
                "exponential",
                "demon",
                "strate"
            ]
        }
    },
    {
        "id": "032c83f9-41c4-45ca-bd2a-6691076424b6",
        "title": "",
        "chunk_text": "19 In the next theorem, we establish a generic iteration complexity bound for both first- and second- order ϵ-stationarity under the sub-exponential oracle. Theorem 4.20. Under Assumptions 4.1, 4.7, and the probabilistic sub-exponential zeroth-order oracle, for any s ≥0, when bp satisfies condition (24) and T satisfies condition (25), we have P{Tϵ ≤T −1} ≥1−exp \u001a −(p −bp)2 2p2 T \u001b −exp \u001a −1 2 min \u0012(ϵf −eϵf + s)2 v2 , (ϵf −eϵf + s) b \u0013 T \u001b . (31) Proof. See Appendix C.1.",
        "metadata": {
            "author": "",
            "keywords": [
                "theorem",
                "oracle",
                "exp",
                "eϵf",
                "order",
                "condition",
                "establish",
                "generic",
                "iteration",
                "complexity"
            ]
        }
    },
    {
        "id": "769f29d5-78e4-4454-9a91-71e5a9a8f532",
        "title": "",
        "chunk_text": "■ Plugging the formulas of ϑα and hα(γ−2 b∆) for α = 0 or 1 into (24) and (25), we immediately know that Corollaries 4.17 and 4.18 remain true for the sub-exponential oracle. We combine them into the following corollary. Corollary 4.21. Under conditions of Theorem 4.20, for first-order stationarity (i.e., α = 0), the conclusion (31) holds when bp satisfies condition (27) and T satisfies condition (28).",
        "metadata": {
            "author": "",
            "keywords": [
                "Plugging",
                "Corollaries",
                "remain",
                "oracle",
                "corollary",
                "formulas",
                "immediately",
                "true",
                "sub-exponential",
                "satisfies"
            ]
        }
    },
    {
        "id": "910b1792-0358-43d3-9417-b33b915f9997",
        "title": "",
        "chunk_text": "For second-order stationarity (i.e., α = 1), the conclusion (31) holds when bp satisfies condition (29) and T satisfies condition (30). The corollary suggests that the method enjoys an iteration complexity of O(ϵ−2) for finding a first- order ϵ-stationary point and O(ϵ−3) for finding a second-order ϵ-stationary point with high probability. These results align with the conclusions in Berahas et al. (2025); Cao et al. (2023) as well as those under the heavy-tailed oracle in Section 4.2.",
        "metadata": {
            "author": "",
            "keywords": [
                "condition",
                "satisfies",
                "stationarity",
                "holds",
                "second-order",
                "finding",
                "ϵ-stationary",
                "point",
                "Cao",
                "Berahas"
            ]
        }
    },
    {
        "id": "1c441b37-db85-42ad-adf3-995519fb0b95",
        "title": "",
        "chunk_text": "Comparing (31) with (26), we observe that P{Tϵ ≤T−1} approaches 1 at an exponential rate. This faster convergence is attributed to the more restrictive sub- exponential condition (19). Technically, applying the Chernoff bound to analyze the concentration of the accumulated oracle noise ek and esk leads to an exponential decay rate in T. Remark 4.22.",
        "metadata": {
            "author": "",
            "keywords": [
                "Comparing",
                "approaches",
                "exponential",
                "observe",
                "rate",
                "Chernoff",
                "Technically",
                "condition",
                "Remark",
                "faster"
            ]
        }
    },
    {
        "id": "74bd240b-0840-4254-a760-0dc2f42460c2",
        "title": "",
        "chunk_text": "In this remark, we echo Remark 3.4 and revisit the probabilistic sub-exponential zeroth-order oracle by exploring the connection between (16) and (19). We show that when eϵf < ϵf and v, b are sufficiently small in (19) — that is, the probability mass of ek, esk is largely concentrated around their means — then (16) holds with a proper choice of pf. Lemma 4.23.",
        "metadata": {
            "author": "",
            "keywords": [
                "remark",
                "echo",
                "revisit",
                "probabilistic",
                "sub-exponential",
                "zeroth-order",
                "oracle",
                "exploring",
                "connection",
                "Lemma"
            ]
        }
    },
    {
        "id": "8a6f49ce-2b6c-4fca-8a61-02c91ccec8da",
        "title": "",
        "chunk_text": "Under the probabilistic sub-exponential zeroth-order oracle (i.e., conditions (i), (ii), and (iii.2) of Definition 3.3), the event Ck satisfies P(Ck | Fk−1/2) ≥1 −2 exp \u001a −1 2 min \u0012(ϵf −eϵf)2 v2 , ϵf −eϵf b \u0013\u001b . (32) Proof. See Appendix C.2. ■ It follows from the lemma that when eϵf < ϵf and v, b are sufficiently small, (19) implies (16) with pf ≤2 exp{−0.5 min((ϵf −eϵf)2/v2, (ϵf −eϵf)/b)}.",
        "metadata": {
            "author": "",
            "keywords": [
                "Proof",
                "Definition",
                "eϵf",
                "exp",
                "min",
                "conditions",
                "oracle",
                "probabilistic",
                "sub-exponential",
                "zeroth-order"
            ]
        }
    },
    {
        "id": "21c2513f-938a-428e-8956-fba54836031a",
        "title": "",
        "chunk_text": "5 Numerical Experiments In this section, we demonstrate the empirical performance of the trust-region SSQP method (Algo- rithm 1). We apply the methods to a subset of equality-constrained problems from the CUTEst test set 20 (Gould et al., 2014) to find first- and second-order stationary points, referred to as TR-SSQP and TR- SSQP2, respectively.",
        "metadata": {
            "author": "",
            "keywords": [
                "Algo",
                "Numerical",
                "rithm",
                "Experiments",
                "SSQP",
                "section",
                "demonstrate",
                "empirical",
                "performance",
                "trust-region"
            ]
        }
    },
    {
        "id": "3c2c9d71-b34b-4b79-b4d7-f99137c562d6",
        "title": "",
        "chunk_text": "The methods are compared with the line-search SSQP method (Berahas et al., 2025, referred to as LS-SSQP), where a similar stochastic oracle enabling irreducible noise with first- order complexity guarantee is studied. We introduce the algorithm setup in Section 5.1. In Section 5.2, we examine the performance of the algorithms for various values of ϵ without irreducible noise. In Section 5.3, we examine their perfor- mance under different combinations of irreducible noise.",
        "metadata": {
            "author": "",
            "keywords": [
                "Section",
                "Berahas",
                "SSQP",
                "irreducible",
                "noise",
                "referred",
                "LS-SSQP",
                "order",
                "studied",
                "methods"
            ]
        }
    },
    {
        "id": "85ebbb4c-eaa1-4123-93ff-059f65e16b4e",
        "title": "",
        "chunk_text": "5.1 Algorithm setup For all methods, we generate samples and construct estimates of objective quantities in each iteration by sample average. Given a sample ξ, we denote the realizations of objective value, gradient, and Hes- sian at x as F(x, ξ), ∇F(x, ξ), and ∇2F(x, ξ). We allow samples to be dependent within the iteration, but independent across iterations. Additionally, the sample size for constructing each estimate is limited to at most 104.",
        "metadata": {
            "author": "",
            "keywords": [
                "Algorithm",
                "objective",
                "methods",
                "average",
                "sample",
                "Hes",
                "setup",
                "generate",
                "construct",
                "quantities"
            ]
        }
    },
    {
        "id": "40d2061e-352d-41e4-b6de-7f801de02c70",
        "title": "",
        "chunk_text": "For the LS-SSQP method, following the notation in Berahas et al. (2025), we generate sample sets ξg k, ξf k, and ξf sk to estimate gradient and objective values, such that their sample sizes satisfy |ξg k| ≥ Cg pg max \b ϵg, κFO ¯αk∆l(xk, ¯τk, ¯gk, ¯dk) 2 and min{|ξf k|, |ξf sk|} ≥Cf pfϵ2 f , where Cg, Cf are positive constants. We set ϵτ = 10−2, ¯τ−1 = σ = 0.1, γ = 0.5, θ = 10−4, α0 = αmax = 1, κFO = 0.05, pg = 0.1, pf = 0.1, Cg = Cf = 5.",
        "metadata": {
            "author": "",
            "keywords": [
                "Berahas",
                "κFO",
                "method",
                "LS-SSQP",
                "notation",
                "sample",
                "satisfy",
                "max",
                "min",
                "constants"
            ]
        }
    },
    {
        "id": "463cd337-b2ec-474e-921a-4f2de46c51ee",
        "title": "",
        "chunk_text": "We set Hessian matrix ¯Hk = I and solve all SSQP subproblems exactly. For TR-SSQP and TR-SSQP2, we construct estimates of objective Hessians, gradients, and values by generating samples ξh k, ξg k, and ξf k, ξf sk following (20). We set ¯µ0 = 1, ∆0 = ∆max = 5, ρ = 1.2, γ = 1.5, κf = κh = κg = 0.05, pf = ph = pg = 0.1, η = 0.4, Ch = Cg = Cf = 5, r = 0.01. To solve (5), we use IPOPT solver (W¨achter and Biegler, 2005).",
        "metadata": {
            "author": "",
            "keywords": [
                "SSQP",
                "matrix",
                "Hessian",
                "Hessians",
                "set",
                "subproblems",
                "solve",
                "gradients",
                "Biegler",
                "IPOPT"
            ]
        }
    },
    {
        "id": "8cef9781-10e9-4154-8e48-33ac0d264fcc",
        "title": "",
        "chunk_text": "Since trust-region methods allow indefinite Hessian matrices, same as Fang et al. (2024a,b), we try four different ¯Hk for TR-SSQP in all problems. 1. Identity matrix (Id). It has been used in various existing SSQP literature, especially for line- search methods (see, e.g., Berahas et al., 2021, 2024, 2025; Na et al., 2022a, 2023). 2. Symmetric rank-one (SR1) update.",
        "metadata": {
            "author": "",
            "keywords": [
                "Hessian",
                "Fang",
                "matrices",
                "trust-region",
                "indefinite",
                "methods",
                "Identity",
                "Berahas",
                "Symmetric",
                "problems"
            ]
        }
    },
    {
        "id": "4ac09575-6eb3-4368-9430-e12b37b2de05",
        "title": "",
        "chunk_text": "We initialize ¯H0 = I and for k ≥1, ¯Hk is updated as ¯Hk = ¯Hk−1 + (yk−1 −¯Hk−1∆xk−1)(yk−1 −¯Hk−1∆xk−1)T (yk−1 −¯Hk−1∆xk−1)T ∆xk−1 where yk−1 = ¯∇xLk −¯∇xLk−1, and ∆xk−1 = xk −xk−1. We employ SR1 instead of BFGS since SR1 can generate indefinite matrices, which can be preferable for constrained problems (Khalfan et al., 1993). 3. Estimated Hessian (EstH).",
        "metadata": {
            "author": "",
            "keywords": [
                "xLk",
                "initialize",
                "updated",
                "Khalfan",
                "BFGS",
                "Estimated",
                "Hessian",
                "employ",
                "matrices",
                "problems"
            ]
        }
    },
    {
        "id": "60a63fc1-e14b-4a07-95c0-cf954a3baf91",
        "title": "",
        "chunk_text": "In the k-th iteration, we estimate the objective Hessian matrix ¯∇2fk using one single sample and set ¯Hk = ¯∇2 xLk = ¯∇2fk + Pm i=1 ¯λi k∇2ci k, where ¯λk is the least- squares Lagrangian multiplier introduced in Section 2.2. 4. Averaged Hessian (AveH). In the k-th iteration, we estimate the Hessian matrix ¯∇2 xLk as in EstH but set ¯Hk = 1 50 Pk i=k−49 ¯∇2 xLi. Na et al.",
        "metadata": {
            "author": "",
            "keywords": [
                "Hessian",
                "Section",
                "Averaged",
                "Lagrangian",
                "iteration",
                "matrix",
                "AveH",
                "k-th",
                "estimate",
                "set"
            ]
        }
    },
    {
        "id": "3e8f826b-9e17-4e68-ad95-deef8c00dd18",
        "title": "",
        "chunk_text": "(2022b); Na and Mahoney (2025) demonstrated that Hessian averaging facilitates stochastic Newton methods achieving faster (local) conver- gence.",
        "metadata": {
            "author": "",
            "keywords": [
                "local",
                "conver",
                "gence",
                "Mahoney",
                "Hessian",
                "Newton",
                "demonstrated",
                "faster",
                "averaging",
                "facilitates"
            ]
        }
    },
    {
        "id": "390ec3e1-415c-4493-8ac1-7ca537c3107f",
        "title": "",
        "chunk_text": "21 In addition, the objective model estimates are generated based on deterministic evaluations pro- vided by the CUTEst package, with noise (denoted by rand) following four different distributions: (i) standard normal distribution, (ii) t-distribution with degree of freedom 4, (iii) log-normal distribution with location-scale parameters (µ = 0, σ = 1), and (iv) Weibull distribution with scale-shape parame- ters (λ = 1, k = 1).",
        "metadata": {
            "author": "",
            "keywords": [
                "distribution",
                "Weibull",
                "iii",
                "ters",
                "addition",
                "pro",
                "vided",
                "package",
                "noise",
                "denoted"
            ]
        }
    },
    {
        "id": "c5d05b5f-ff55-4c2b-b186-39ef0f0f41da",
        "title": "",
        "chunk_text": "Noise from the normal distribution exhibits sub-exponential tail, while noise from the other three distributions are popular heavy-tailed noise. Specifically, for distributions (i) and (ii), we let F(xk, ξ) = fk +σ ·rand, ∇F(xk, ξ) = ∇fk +1·σ ·rand with 1 being the d-dimensional all-one vector, and [∇2F(xk, ξ)]i,j = (∇2fk)i,j +σ ·rand.",
        "metadata": {
            "author": "",
            "keywords": [
                "Noise",
                "rand",
                "tail",
                "normal",
                "exhibits",
                "sub-exponential",
                "popular",
                "heavy-tailed",
                "distributions",
                "Specifically"
            ]
        }
    },
    {
        "id": "8bc99377-8bef-4363-a383-59c4e3b1205e",
        "title": "",
        "chunk_text": "For distributions (iii) and (iv), rand is replaced by δ · rand, with δ being a Rademacher variable (i.e., P(δ = 1) = P(δ = −1) = 0.5), so that the noise becomes symmetric. Throughout the experiments, we set σ = 10−2. We set the maximum iteration budget to be 105 and define the stopping time as in Definition 4.6. The values of ϵ in stopping time are specified in each experiment below.",
        "metadata": {
            "author": "",
            "keywords": [
                "rand",
                "Rademacher",
                "iii",
                "distributions",
                "variable",
                "symmetric",
                "replaced",
                "noise",
                "set",
                "stopping"
            ]
        }
    },
    {
        "id": "dc4befae-b3d7-4ef6-93fe-c2377e5ab43d",
        "title": "",
        "chunk_text": "For each algorithm and each problem, under every combination of irreducible noise level and ϵ, we report the average of Tϵ over five independent runs. 5.2 Algorithms performance without irreducible noise In this section, we aim to validate our complexity bounds in Theorems 4.16 and 4.20, which state that achieving first-order ϵ-stationarity requires at most O(ϵ−2) iterations, while achieving second-order ϵ-stationarity requires at most O(ϵ−3) iterations with high probability.",
        "metadata": {
            "author": "",
            "keywords": [
                "problem",
                "runs",
                "irreducible",
                "noise",
                "requires",
                "iterations",
                "combination",
                "level",
                "report",
                "average"
            ]
        }
    },
    {
        "id": "5f96ff0a-26bf-4c1a-ad42-797263732fc4",
        "title": "",
        "chunk_text": "To allow for arbitrarily small ϵ, we set ϵf = ϵg = ϵh = 0. We vary ϵ ∈{10−1, 10−2, 10−3, 10−4} and report the average stopping time in Figure 1. First, we observe that as ϵ decreases, all methods require more iterations to converge. Specifically, when ϵ is reduced by a factor of 10, Figure 1 shows that the stopping time of TR-SSQP increases by a factor between 10 and 100, while that of TR-SSQP2 increases by approximately 100.",
        "metadata": {
            "author": "",
            "keywords": [
                "Figure",
                "arbitrarily",
                "small",
                "set",
                "stopping",
                "time",
                "factor",
                "increases",
                "vary",
                "report"
            ]
        }
    },
    {
        "id": "c8f71784-7566-4200-b1fa-e7a542d4f03a",
        "title": "",
        "chunk_text": "This behavior aligns with our theoretical results, though it is important to emphasize that our analysis provides only a worst-case guarantee. Due to this worst-case nature, the actual iteration complexity may grow more slowly than predicted when gradually decreasing ϵ. Second, we observe that for TR-SSQP methods, performance varies across the four Hessian approx- imations.",
        "metadata": {
            "author": "",
            "keywords": [
                "results",
                "guarantee",
                "worst-case",
                "behavior",
                "aligns",
                "theoretical",
                "important",
                "emphasize",
                "analysis",
                "imations"
            ]
        }
    },
    {
        "id": "abb409b3-d0f0-40b4-a0b9-95c289a4c9b7",
        "title": "",
        "chunk_text": "TR-SSQP-Id and LS-SSQP exhibit comparable performance, while TR-SSQP-SR1 exhibits the most unstable performance when ϵ is small. This instability is largely attributed to the Hessian updating scheme, which can accumulate estimation errors over iterations. In contrast, TR-SSQP with the averaged Hessian (TR-SSQP-AveH) achieves the best performance, followed by TR-SSQP- EstH, with their advantage becoming particularly pronounced as ϵ decreases.",
        "metadata": {
            "author": "",
            "keywords": [
                "small",
                "Hessian",
                "performance",
                "LS-SSQP",
                "comparable",
                "unstable",
                "exhibit",
                "exhibits",
                "TR-SSQP",
                "EstH"
            ]
        }
    },
    {
        "id": "9ec4b573-503c-4cd3-8281-9d7c3dfc7d20",
        "title": "",
        "chunk_text": "This highlights the benefits of fully exploring Hessian information in the algorithm design. Moreover, we observe that the noise distribution has a limited impact on the stopping time for most methods. As shown in the figure, changes in the noise distribution result in no significant variation in the average stopping time for the majority of methods. However, TR-SSQP-AveH is an exception. Even when ϵ = 10−4, this method can terminate within 10 iterations on certain datasets under normal noise.",
        "metadata": {
            "author": "",
            "keywords": [
                "Hessian",
                "noise",
                "design",
                "time",
                "highlights",
                "benefits",
                "fully",
                "exploring",
                "information",
                "algorithm"
            ]
        }
    },
    {
        "id": "4b18496a-b057-4e2b-9dd1-d04c8fd9b50c",
        "title": "",
        "chunk_text": "In contrast, under heavy-tailed noise, TR-SSQP-AveH requires at least 100 iterations to con- verge. This difference arises because heavy-tailed noise is more widely distributed than normal noise, introducing greater variability and randomness into the estimates of the objective models. Conse- quently, this increased uncertainty slows down the convergence of TR-SSQP-AveH under such noise distributions.",
        "metadata": {
            "author": "",
            "keywords": [
                "verge",
                "noise",
                "heavy-tailed",
                "contrast",
                "requires",
                "iterations",
                "con",
                "Conse",
                "quently",
                "introducing"
            ]
        }
    },
    {
        "id": "7f69c766-6426-4ff1-a77f-f784d1098ccf",
        "title": "",
        "chunk_text": "This phenomenon is less apparent for other methods, as they require around 100 iter- ations to converge even under normal noise when ϵ = 10−4. 22 (a) Normal distribution (b) t-distribution (c) Log-normal distribution (d) Weibull distribution Figure 1: Averaged stopping time Tϵ with noise from four different distributions. In every plot, the first four boxes correspond to TR-SSQP with different choices of ¯Hk. The fifth box corresponds to TR-SSQP2, and the last box corresponds to LS-SSQP.",
        "metadata": {
            "author": "",
            "keywords": [
                "normal",
                "Log-normal",
                "Weibull",
                "Figure",
                "Averaged",
                "distribution",
                "iter",
                "t-distribution",
                "noise",
                "methods"
            ]
        }
    },
    {
        "id": "dd0f2f74-3cbd-4547-993c-9ffb9f4eef33",
        "title": "",
        "chunk_text": "5.3 Algorithms performance with irreducible noise In this section, we evaluate the performance of all methods under different combinations of irreducible noise levels. To limit the number of total combinations, we set the default values as ϵ = 10−2, ϵf = 10−4, ϵg = 10−2, ϵh = 10−2, and vary one parameter at a time while keeping the others fixed. In partic- ular, we vary ϵ ∈{10−1, 10−3}, ϵf ∈{10−3, 10−5}, ϵg ∈{10−1, 10−3}, and ϵh ∈{10−1, 10−3}.",
        "metadata": {
            "author": "",
            "keywords": [
                "irreducible",
                "noise",
                "performance",
                "Algorithms",
                "combinations",
                "section",
                "levels",
                "evaluate",
                "methods",
                "vary"
            ]
        }
    },
    {
        "id": "cb219c11-7672-43d3-bfdc-c992f56d3c84",
        "title": "",
        "chunk_text": "The de- fault setup ensures that the condition ϵ ≥O(√ϵf + ϵg) for first-order stationarity is satisfied (cf. Assumption 4.7). For consistency, we use the same default setup for second-order stationarity. When introducing irreducible noise to each method, we sample a Rademacher variable δ and directly add the noise δϵh, δϵg, δϵf to the corresponding objective quantity estimates. The Rademacher variable ensures that the irreducible noise is two-side distributed.",
        "metadata": {
            "author": "",
            "keywords": [
                "Assumption",
                "Rademacher",
                "setup",
                "stationarity",
                "fault",
                "satisfied",
                "noise",
                "condition",
                "first-order",
                "ensures"
            ]
        }
    },
    {
        "id": "7c653c5b-1330-4d1e-b5d1-35e311463e0b",
        "title": "",
        "chunk_text": "Results are presented as performance profiles with respect to iterations. As in Berahas et al. (2025), we employ the convergence metric ∥∇L(x0)∥−∥∇L(x)∥≥(1 −εpp) (∥∇L(x0)∥−∥∇Lb∥). Here, x0 is the initial iterate, and ∥∇Lb∥represents the best KKT residual value identified by any algorithm 23 Figure 2: Performance profiles with noise following a normal distribution. Each line represents a differ- ent method. The first column corresponds to the default irreducible noise levels with varying ϵ.",
        "metadata": {
            "author": "",
            "keywords": [
                "Results",
                "iterations",
                "performance",
                "profiles",
                "presented",
                "respect",
                "Berahas",
                "Figure",
                "represents",
                "noise"
            ]
        }
    },
    {
        "id": "ce4b2a15-572b-445c-840e-25b91167b4ce",
        "title": "",
        "chunk_text": "Each row of the last two columns corresponds to varying ϵf, ϵg and ϵh, respectively. for the given problem instance within the iteration budget in the experiments of Section 5.2 (no irre- ducible noise). The prescribed tolerance is set to εpp = 10−3. This convergence metric extends the ap- proach in Mor´e and Wild (2009) to accommodate the constrained optimization setting and the noncon- vexity of the objective function.",
        "metadata": {
            "author": "",
            "keywords": [
                "row",
                "columns",
                "corresponds",
                "varying",
                "Section",
                "Wild",
                "irre",
                "ducible",
                "noise",
                "Mor´e"
            ]
        }
    },
    {
        "id": "54e33b38-46df-49e1-b6db-261ca5dac1a0",
        "title": "",
        "chunk_text": "The algorithm terminates when any of the following criteria are met: the stopping time is reached, the convergence metric is satisfied, or the iteration budget is exhausted. We present the experimental results in Figures 2, 3, 4, and 5, corresponding to noise from the nor- mal distribution, t-distribution, log-normal distribution, and Weibull distribution, respectively.",
        "metadata": {
            "author": "",
            "keywords": [
                "distribution",
                "met",
                "reached",
                "satisfied",
                "exhausted",
                "algorithm",
                "terminates",
                "criteria",
                "stopping",
                "time"
            ]
        }
    },
    {
        "id": "7b5265f5-4539-4ffe-a9d2-e855fd073028",
        "title": "",
        "chunk_text": "In each figure, the first column represents the performance of all methods under the default irreducible noise levels with varying ϵ. The last two columns represent the performance of TR-SSQP and TR- SSQP2 under varying irreducible noise levels with the default ϵ.",
        "metadata": {
            "author": "",
            "keywords": [
                "performance",
                "noise",
                "levels",
                "default",
                "irreducible",
                "figure",
                "varying",
                "methods",
                "represents",
                "represent"
            ]
        }
    },
    {
        "id": "591ba21a-8e2e-4eed-987a-ba0dd0bed023",
        "title": "",
        "chunk_text": "For clarity, we trim the performance ratio (x-axis) based on two conditions: (1) if all methods converge, we trim the performance ratio after the last method converges; (2) if some methods diverge on some problems, we trim the performance ra- tio when no method converges on new problems within the iteration budget. 24 Figure 3: Performance profiles with noise following a t-distribution. Each line represents a different method.",
        "metadata": {
            "author": "",
            "keywords": [
                "trim",
                "performance",
                "converges",
                "ratio",
                "problems",
                "x-axis",
                "method",
                "Figure",
                "clarity",
                "based"
            ]
        }
    },
    {
        "id": "8b1d5bea-40a2-4387-87fb-e1f66c313b55",
        "title": "",
        "chunk_text": "The first column corresponds to the default irreducible noise levels with varying ϵ. Each row of the last two columns corresponds to varying ϵf, ϵg and ϵh, respectively. Comparing corresponding plots across the four figures, we first observe that all methods are robust to the noise distribution. For any fixed combination of ϵ, ϵf, ϵg, ϵh, changing the noise distribution re- sults in limited differences in the performance ratios. Next, we investigate the impact of varying ϵ, ϵf, ϵg, ϵh.",
        "metadata": {
            "author": "",
            "keywords": [
                "varying",
                "corresponds",
                "noise",
                "default",
                "irreducible",
                "levels",
                "distribution",
                "column",
                "columns",
                "row"
            ]
        }
    },
    {
        "id": "4804e78d-5f28-4f04-b208-fdca30cb427d",
        "title": "",
        "chunk_text": "• ϵ : From the first column of the four figures, we observe that as ϵ decreases from 10−1 to 10−2, all methods require more iterations to converge, increasing by a factor of 10 to 100. This observation aligns well with our theoretical complexity bounds. However, when ϵ further decreases to 10−3, all methods fail to converge on some datasets within the iteration budget.",
        "metadata": {
            "author": "",
            "keywords": [
                "figures",
                "increasing",
                "methods",
                "converge",
                "column",
                "observe",
                "require",
                "factor",
                "decreases",
                "bounds"
            ]
        }
    },
    {
        "id": "dcfb9f49-f097-430a-a9e2-e322a02f723d",
        "title": "",
        "chunk_text": "This is primarily because, given the default irreducible noise levels, ϵ = 10−3 no longer satisfies the lower bound in Assumption 4.7. That being said, the impact of decreasing ϵ is less notable for TR-SSQP-AveH and TR-SSQP- EstH, highlighting the advantages of exploring Hessian information in the algorithm design.",
        "metadata": {
            "author": "",
            "keywords": [
                "Assumption",
                "levels",
                "primarily",
                "default",
                "irreducible",
                "noise",
                "longer",
                "satisfies",
                "lower",
                "bound"
            ]
        }
    },
    {
        "id": "c195dfb9-3a82-43e9-b802-97e5af950e14",
        "title": "",
        "chunk_text": "• ϵf : Comparing the first row of the second and third columns with the default setting (middle plot in 25 Figure 4: Performance profiles with noise following a log-normal-distribution. Each line represents a different method. The first column corresponds to the default irreducible noise levels with varying ϵ. Each row of the last two columns corresponds to varying ϵf, ϵg and ϵh, respectively.",
        "metadata": {
            "author": "",
            "keywords": [
                "Figure",
                "Comparing",
                "Performance",
                "setting",
                "middle",
                "default",
                "plot",
                "profiles",
                "noise",
                "row"
            ]
        }
    },
    {
        "id": "1d06fdb7-ebe0-40ee-beaa-1adc9ef8b4f0",
        "title": "",
        "chunk_text": "the first column) in each figure, we observe that increasing ϵf from 10−5 to 10−4 approximately triples the performance ratio. This is expected since, by ϵ ≈O(√ϵf +ϵg), a 10-fold increase in ϵf leads to at most a √ 10-fold increase in the performance ratio. This pattern no longer holds when ϵf is further increased, as all methods fail to converge on some datasets (due to violation of Assumption 4.7).",
        "metadata": {
            "author": "",
            "keywords": [
                "ratio",
                "performance",
                "increase",
                "column",
                "figure",
                "approximately",
                "observe",
                "increasing",
                "triples",
                "Assumption"
            ]
        }
    },
    {
        "id": "60f9b9f4-33fc-4be2-84de-90794dc0054a",
        "title": "",
        "chunk_text": "• ϵg : Comparing the second row of the last two columns with the default setting, we observe that when ϵg increases from 10−3 to 10−2, the performance ratio also increases by a factor of 3. This observation aligns with our analysis, as under Assumption 4.7, a 10-fold increase in ϵg should not result in more than a 10-fold increase in the performance ratio. However, when ϵg further increases from 10−2 to 10−1, the performance of all methods deteriorates rapidly.",
        "metadata": {
            "author": "",
            "keywords": [
                "Comparing",
                "performance",
                "increases",
                "ratio",
                "setting",
                "row",
                "columns",
                "default",
                "observe",
                "factor"
            ]
        }
    },
    {
        "id": "28e7fb03-5b0c-41c6-82d0-341a101c2280",
        "title": "",
        "chunk_text": "As shown in the middle plot, all methods fail to converge on more than 20% of problem instances. • ϵh : The third row of the last two columns illustrates the impact of varying ϵh. We observe that TR-SSQP-AveH and TR-SSQP-EstH behave robustly to changes in ϵh, primarily because eigenvalues 26 Figure 5: Performance profiles with noise following a Weibull distribution. Each line represents a different method. The first column corresponds to the default irreducible noise levels with varying ϵ.",
        "metadata": {
            "author": "",
            "keywords": [
                "plot",
                "instances",
                "shown",
                "middle",
                "fail",
                "converge",
                "problem",
                "Figure",
                "varying",
                "noise"
            ]
        }
    },
    {
        "id": "707ede67-d600-4238-bfd4-7759a2c0dac7",
        "title": "",
        "chunk_text": "Each row of the last two columns corresponds to varying ϵf, ϵg and ϵh, respectively. are not included in the definition of the stopping time for first-order stationarity. In contrast, the per- formance of TR-SSQP2 is significantly affected by the increase in ϵh. When ϵh increases from 10−3 to 10−2, TR-SSQP2 requires approximately 5 times more iterations to converge.",
        "metadata": {
            "author": "",
            "keywords": [
                "respectively.",
                "stationarity",
                "row",
                "columns",
                "corresponds",
                "varying",
                "included",
                "definition",
                "stopping",
                "first-order"
            ]
        }
    },
    {
        "id": "ceacb671-bf46-4fb7-9c15-817a95248c97",
        "title": "",
        "chunk_text": "This observation also aligns with our theoretical analysis, as for second-order stationarity, ϵ ≥O( 3√ϵf + √ϵg + ϵh) implies that a 10-fold increase in ϵh should lead to no more than a 10-fold increase in the performance ratio. Furthermore, we observe that TR-SSQP2 fails to converge on approximately 25% of problem in- stances when ϵh is further increased from 10−2 to 10−1, while TR-SSQP-AveH and TR-SSQP-EstH still perform well in this case for finding first-order stationary points.",
        "metadata": {
            "author": "",
            "keywords": [
                "increase",
                "analysis",
                "stationarity",
                "implies",
                "ratio",
                "observation",
                "aligns",
                "theoretical",
                "second-order",
                "lead"
            ]
        }
    },
    {
        "id": "1394669d-4bc7-490b-99eb-1f3be8eea293",
        "title": "",
        "chunk_text": "27 6 Conclusion In this paper, we derived high-probability complexity bounds for a trust-region Stochastic Sequential Quadratic Programming (SSQP) method in identifying first- and second-order ϵ-stationary points in equality-constrained optimization problems. Our method extends the existing literature by allowing irreducible and heavy-tailed noise in objective function estimation.",
        "metadata": {
            "author": "",
            "keywords": [
                "SSQP",
                "Conclusion",
                "Programming",
                "Stochastic",
                "Sequential",
                "Quadratic",
                "paper",
                "problems",
                "method",
                "derived"
            ]
        }
    },
    {
        "id": "6c765f3f-49f7-45f3-8e54-d1bc76ef5fde",
        "title": "",
        "chunk_text": "Under weaker oracle conditions, we established that our method achieves an iteration complexity of O(ϵ−2) for identifying a first-order ϵ-stationary point and O(ϵ−3) for identifying a second-order ϵ-stationary point with high probability, provided that ϵ is above a threshold determined by the irreducible noise level. We validated our the- oretical analysis through numerical experiments on problems from the CUTEst dataset. References A. S. Bandeira, K. Scheinberg, and L. N. Vicente.",
        "metadata": {
            "author": "",
            "keywords": [
                "identifying",
                "ϵ-stationary",
                "point",
                "conditions",
                "probability",
                "provided",
                "level",
                "weaker",
                "oracle",
                "established"
            ]
        }
    },
    {
        "id": "73673a21-4f94-4542-a065-820b572ebd35",
        "title": "",
        "chunk_text": "Convergence of trust-region methods based on probabilistic models. SIAM Journal on Optimization, 24(3):1238–1264, 2014. F. Beiser, B. Keith, S. Urbainczyk, and B. Wohlmuth. Adaptive sampling strategies for risk-averse stochastic optimization with constraints. IMA Journal of Numerical Analysis, 43(6):3729–3765, 2023. A. S. Berahas, F. E. Curtis, D. Robinson, and B. Zhou. Sequential quadratic optimization for nonlinear equality constrained stochastic optimization.",
        "metadata": {
            "author": "",
            "keywords": [
                "Optimization",
                "Convergence",
                "models",
                "Journal",
                "SIAM",
                "trust-region",
                "methods",
                "based",
                "probabilistic",
                "Beiser"
            ]
        }
    },
    {
        "id": "ca06bd20-accf-42e5-a9e9-8a2f1ec81086",
        "title": "",
        "chunk_text": "SIAM Journal on Optimization, 31(2): 1352–1379, 2021. A. S. Berahas, R. Bollapragada, and B. Zhou. An adaptive sampling sequential quadratic program- ming method for equality constrained stochastic optimization. arXiv preprint arXiv:2206.00712, 2022. A. S. Berahas, J. Shi, Z. Yi, and B. Zhou. Accelerating stochastic sequential quadratic program- ming for equality constrained optimization using predictive variance reduction. Computational Optimization and Applications, 86(1):79–116, 2023. A. S.",
        "metadata": {
            "author": "",
            "keywords": [
                "SIAM",
                "Journal",
                "Berahas",
                "Zhou",
                "Optimization",
                "Bollapragada",
                "program",
                "quadratic",
                "ming",
                "equality"
            ]
        }
    },
    {
        "id": "f0987244-780f-4276-b78d-def85a8212fc",
        "title": "",
        "chunk_text": "Berahas, F. E. Curtis, M. J. O’Neill, and D. P. Robinson. A stochastic sequential quadratic op- timization algorithm for nonlinear-equality-constrained optimization with rank-deficient jacobians. Mathematics of Operations Research, 49(4):2212–2248, 2024. A. S. Berahas, M. Xie, and B. Zhou. A sequential quadratic programming method with high- probability complexity bounds for nonlinear equality-constrained stochastic optimization. SIAM Journal on Optimization, 35(1):240–269, 2025. D. P.",
        "metadata": {
            "author": "",
            "keywords": [
                "Curtis",
                "Robinson",
                "Berahas",
                "O’Neill",
                "optimization",
                "Research",
                "sequential",
                "quadratic",
                "stochastic",
                "Xie"
            ]
        }
    },
    {
        "id": "3245af45-10c6-4e9e-ad80-642596b1329c",
        "title": "",
        "chunk_text": "Bertsekas. Constrained Optimization and Lagrange Multiplier Methods. Elsevier, 1982. J. T. Betts. Practical Methods for Optimal Control and Estimation Using Nonlinear Programming. Society for Industrial and Applied Mathematics, 2010. J. Blanchet, C. Cartis, M. Menickelly, and K. Scheinberg. Convergence rate analysis of a stochastic trust-region method via supermartingales. INFORMS Journal on Optimization, 1(2):92–119, 2019. 28 P. T. Boggs and J. W. Tolle. Sequential quadratic programming.",
        "metadata": {
            "author": "",
            "keywords": [
                "Bertsekas",
                "Optimization",
                "Programming",
                "Methods",
                "Lagrange",
                "Multiplier",
                "Betts",
                "Mathematics",
                "Blanchet",
                "Cartis"
            ]
        }
    },
    {
        "id": "4568b80c-fa79-40fc-912b-cd18cd376680",
        "title": "",
        "chunk_text": "Acta Numerica, 4:1–51, 1995. R. Bollapragada, C. Karamanli, B. Keith, B. Lazarov, S. Petrides, and J. Wang. An adaptive sampling augmented lagrangian method for stochastic optimization with deterministic constraints. Computers & Mathematics with Applications, 149:239–258, 2023. D. L. Burkholder. Distribution function inequalities for martingales. the Annals of Probability, 1(1): 19–42, 1973. R. H. Byrd, R. B. Schnabel, and G. A. Shultz.",
        "metadata": {
            "author": "",
            "keywords": [
                "Numerica",
                "Acta",
                "Bollapragada",
                "Karamanli",
                "Keith",
                "Lazarov",
                "Petrides",
                "Wang",
                "Mathematics",
                "Applications"
            ]
        }
    },
    {
        "id": "3aeee748-f4b0-40fc-a1fd-d7b03be72954",
        "title": "",
        "chunk_text": "A trust region algorithm for nonlinearly constrained optimization. SIAM Journal on Numerical Analysis, 24(5):1152–1170, 1987. L. Cao, A. S. Berahas, and K. Scheinberg. First- and second-order high probability complexity bounds for trust-region methods with noisy oracles. Mathematical Programming, 207(1–2):55–106, 2023. P. Chen and S. H. Sung. Rosenthal type inequalities for random variables. Journal of Mathematical Inequalities, 14(2):305–18, 2020. A. Choromanska, M. Henaff, M. Mathieu, G. B.",
        "metadata": {
            "author": "",
            "keywords": [
                "optimization",
                "SIAM",
                "Analysis",
                "trust",
                "region",
                "algorithm",
                "nonlinearly",
                "constrained",
                "Numerical",
                "Journal"
            ]
        }
    },
    {
        "id": "be77e0f0-5e79-459b-b08a-28abc9f75eab",
        "title": "",
        "chunk_text": "Arous, and Y. LeCun. The loss surfaces of multilayer networks. In Artificial intelligence and statistics, pages 192–204. PMLR, 2015. A. R. Conn, N. I. M. Gould, and P. L. Toint. Trust Region Methods. Society for Industrial and Applied Mathematics, 2000. S. Cuomo, V. S. Di Cola, F. Giampaolo, G. Rozza, M. Raissi, and F. Piccialli. Scientific machine learning through physics–informed neural networks: Where we are and what’s next. Journal of Scientific Computing, 92(3), 2022. F. E. Curtis, V.",
        "metadata": {
            "author": "",
            "keywords": [
                "Arous",
                "LeCun",
                "networks",
                "PMLR",
                "Scientific",
                "Artificial",
                "Conn",
                "Gould",
                "Toint",
                "Methods"
            ]
        }
    },
    {
        "id": "f08fd4e0-6d92-44b0-a9b8-01bc7ba75799",
        "title": "",
        "chunk_text": "Kungurtsev, D. P. Robinson, and Q. Wang. A stochastic-gradient-based interior- point algorithm for solving smooth bound-constrained optimization problems. arXiv preprint arXiv:2304.14907, 2023a. F. E. Curtis, M. J. O’Neill, and D. P. Robinson. Worst-case complexity of an sqp method for nonlinear equality constrained stochastic optimization. Mathematical Programming, 205(1–2): 431–483, 2023b. F. E. Curtis, D. P. Robinson, and B. Zhou.",
        "metadata": {
            "author": "",
            "keywords": [
                "Robinson",
                "Wang",
                "Kungurtsev",
                "Curtis",
                "interior",
                "arXiv",
                "optimization",
                "Programming",
                "Zhou",
                "point"
            ]
        }
    },
    {
        "id": "74d7c105-ca1e-48dd-8afb-e86f4753d2ce",
        "title": "",
        "chunk_text": "Sequential quadratic optimization for stochastic optimization with deterministic nonlinear inequality and equality constraints. SIAM Journal on Optimization, 34(4):3592–3622, 2024a. F. E. Curtis, D. P. Robinson, and B. Zhou. A stochastic inexact sequential quadratic optimization algorithm for nonlinear equality-constrained optimization. INFORMS Journal on Optimization, 6 (3–4):173–195, 2024b. F. E. Curtis, X. Jiang, and Q. Wang.",
        "metadata": {
            "author": "",
            "keywords": [
                "optimization",
                "Curtis",
                "Journal",
                "constraints",
                "quadratic",
                "SIAM",
                "Sequential",
                "deterministic",
                "inequality",
                "equality"
            ]
        }
    },
    {
        "id": "a417acf5-c448-4d49-b563-132f6c814e1f",
        "title": "",
        "chunk_text": "Almost-sure convergence of iterates and multipliers in stochastic sequential quadratic optimization. Journal of Optimization Theory and Applications, 204(2), 2025. 29 Y. N. Dauphin, R. Pascanu, C. Gulcehre, K. Cho, S. Ganguli, and Y. Bengio. Identifying and attacking the saddle point problem in high-dimensional non-convex optimization. Advances in neural information processing systems, 27, 2014. M. El-Alem.",
        "metadata": {
            "author": "",
            "keywords": [
                "optimization",
                "Almost-sure",
                "Applications",
                "convergence",
                "iterates",
                "multipliers",
                "stochastic",
                "sequential",
                "quadratic",
                "Theory"
            ]
        }
    },
    {
        "id": "2ab483c4-98c8-49f9-88e4-9c84e09a86d4",
        "title": "",
        "chunk_text": "A global convergence theory for the celis–dennis–tapia trust-region algorithm for constrained optimization. SIAM Journal on Numerical Analysis, 28(1):266–290, 1991. X. Fan, I. Grama, and Q. Liu. Deviation inequalities for martingales with applications. Journal of Mathematical Analysis and Applications, 448(1):538–566, 2017. Y. Fang, S. Na, M. W. Mahoney, and M. Kolar. Fully stochastic trust-region sequential quadratic programming for equality-constrained optimization problems.",
        "metadata": {
            "author": "",
            "keywords": [
                "dennis",
                "celis",
                "tapia",
                "Analysis",
                "applications",
                "SIAM",
                "global",
                "convergence",
                "theory",
                "algorithm"
            ]
        }
    },
    {
        "id": "06f0b012-4750-4a27-a773-626eb17cb197",
        "title": "",
        "chunk_text": "SIAM Journal on Optimization, 34 (2):2007–2037, 2024a. Y. Fang, S. Na, M. W. Mahoney, and M. Kolar. Trust-region sequential quadratic programming for stochastic optimization with random models. arXiv preprint arXiv:2409.15734, 2024b. D. H. Fuk. Certain probabilistic inequalities for martingales. Sibirskii Matematicheskii Zhurnal, 14 (1):185–193, 1973. N. I. M. Gould, D. Orban, and P. L. Toint.",
        "metadata": {
            "author": "",
            "keywords": [
                "SIAM",
                "Journal",
                "Optimization",
                "Fang",
                "Mahoney",
                "Kolar",
                "arXiv",
                "Fuk",
                "Zhurnal",
                "Gould"
            ]
        }
    },
    {
        "id": "cbed5471-b7a0-418b-9e08-9fa3ca8781a8",
        "title": "",
        "chunk_text": "Cutest: a constrained and unconstrained testing environment with safe threads for mathematical optimization. Computational Optimization and Applications, 60(3):545–557, 2014. M. Heinkenschloss and D. Ridzal. A matrix-free trust-region sqp method for equality constrained optimization. SIAM Journal on Optimization, 24(3):1507–1541, 2014. P. Jain, C. Jin, S. Kakade, and P. Netrapalli. Global convergence of non-convex gradient descent for computing matrix squareroot.",
        "metadata": {
            "author": "",
            "keywords": [
                "optimization",
                "Cutest",
                "Applications",
                "unconstrained",
                "testing",
                "environment",
                "safe",
                "threads",
                "mathematical",
                "constrained"
            ]
        }
    },
    {
        "id": "d7a549a2-ed28-49e8-b299-a54e83500cba",
        "title": "",
        "chunk_text": "In Artificial Intelligence and Statistics, pages 479–488. PMLR, 2017. B. Jin, K. Scheinberg, and M. Xie. Sample complexity analysis for adaptive optimization algorithms with stochastic oracles. Mathematical Programming, 209(1–2):651–679, 2024. H. F. Khalfan, R. H. Byrd, and R. B. Schnabel. A theoretical and experimental study of the symmetric rank-one update. SIAM Journal on Optimization, 3(1):1–24, 1993. W. Kuang, M. Anitescu, and S. Na.",
        "metadata": {
            "author": "",
            "keywords": [
                "Statistics",
                "Artificial",
                "Intelligence",
                "pages",
                "PMLR",
                "optimization",
                "Jin",
                "Scheinberg",
                "Xie",
                "Programming"
            ]
        }
    },
    {
        "id": "87c151e6-4ac6-4dea-8231-fc828eae2500",
        "title": "",
        "chunk_text": "Online covariance matrix estimation in sketched newton methods. arXiv preprint arXiv:2502.07114, 2025. Y. Lou, S. Sun, and J. Nocedal. Design guidelines for noise-tolerant optimization with applications in robust design. arXiv preprint arXiv:2401.15007, 2024. J. J. Mor´e and S. M. Wild. Benchmarking derivative-free optimization algorithms. SIAM Journal on Optimization, 20(1):172–191, 2009. S. Na and M. W. Mahoney.",
        "metadata": {
            "author": "",
            "keywords": [
                "arXiv",
                "Online",
                "methods",
                "optimization",
                "preprint",
                "covariance",
                "matrix",
                "estimation",
                "sketched",
                "newton"
            ]
        }
    },
    {
        "id": "e8154016-9551-4df5-ab85-d3577e0148e2",
        "title": "",
        "chunk_text": "Statistical inference of constrained stochastic optimization via sketched sequential quadratic programming. Journal of Machine Learning Research, 26(33), 2025. S. Na, M. Anitescu, and M. Kolar. An adaptive stochastic sequential quadratic programming with differentiable exact augmented lagrangians. Mathematical Programming, 199(1–2):721–791, 2022a. 30 S. Na, M. Derezi´nski, and M. W. Mahoney. Hessian averaging in stochastic newton methods achieves superlinear convergence.",
        "metadata": {
            "author": "",
            "keywords": [
                "programming",
                "Statistical",
                "quadratic",
                "Research",
                "stochastic",
                "inference",
                "constrained",
                "optimization",
                "sketched",
                "sequential"
            ]
        }
    },
    {
        "id": "4c95875d-ae50-4213-a1bf-e434d58f1853",
        "title": "",
        "chunk_text": "Mathematical Programming, 201(1–2):473–520, 2022b. S. Na, M. Anitescu, and M. Kolar. Inequality constrained stochastic nonlinear optimization via active-set sequential quadratic programming. Mathematical Programming, 202(1–2):279–353, 2023. S. V. Nagaev. Large deviations of sums of independent random variables. The Annals of Probability, pages 745–789, 1979. J. Nocedal and S. Wright. Numerical Optimization. Springer New York, 2006. E. O. Omojokun.",
        "metadata": {
            "author": "",
            "keywords": [
                "Programming",
                "Mathematical",
                "Anitescu",
                "Kolar",
                "optimization",
                "Nagaev",
                "Probability",
                "Wright",
                "York",
                "Omojokun"
            ]
        }
    },
    {
        "id": "13792ca6-3729-4eed-aea0-858625fd1171",
        "title": "",
        "chunk_text": "Trust region algorithms for optimization with nonlinear equality and inequality constraints. PhD thesis, University of Colorado at Boulder, CO, 1989. F. Oztoprak, R. Byrd, and J. Nocedal. Constrained optimization in the presence of noise. SIAM Journal on Optimization, 33(3):2118–2136, 2023. M. J. D. Powell and Y. Yuan. A trust region algorithm for equality constrained optimization. Mathematical Programming, 49(1–3):189–211, 1990. S. Qiu and V. Kungurtsev.",
        "metadata": {
            "author": "",
            "keywords": [
                "optimization",
                "constraints",
                "University",
                "Boulder",
                "nonlinear",
                "inequality",
                "region",
                "Colorado",
                "Oztoprak",
                "Byrd"
            ]
        }
    },
    {
        "id": "b66416d8-aad3-474b-a68f-1003419d6767",
        "title": "",
        "chunk_text": "A sequential quadratic programming method for optimization with stochastic objective functions, deterministic inequality constraints and robust subproblems. arXiv preprint arXiv:2302.07947, 2023. T. Santoso, S. Ahmed, M. Goetschalckx, and A. Shapiro. A stochastic programming approach for supply chain network design under uncertainty. European Journal of Operational Research, 167 (1):96–115, 2005. A. Shapiro, D. Dentcheva, and A. Ruszczynski.",
        "metadata": {
            "author": "",
            "keywords": [
                "Shapiro",
                "functions",
                "deterministic",
                "subproblems",
                "sequential",
                "quadratic",
                "method",
                "optimization",
                "objective",
                "inequality"
            ]
        }
    },
    {
        "id": "cedeba2e-d6eb-462c-81d4-4666563ee8de",
        "title": "",
        "chunk_text": "Lectures on Stochastic Programming: Modeling and Theory, Third Edition. Society for Industrial and Applied Mathematics, 2021. S. Sun and J. Nocedal. A trust region method for noisy unconstrained optimization. Mathematical Programming, 202(1–2):445–472, 2023. S. Sun and J. Nocedal. A trust-region algorithm for noisy equality constrained optimization. arXiv preprint arXiv:2411.02665, 2024. A. Vardi.",
        "metadata": {
            "author": "",
            "keywords": [
                "Modeling",
                "Theory",
                "Edition",
                "Stochastic",
                "Programming",
                "Nocedal",
                "Sun",
                "Lectures",
                "Mathematics",
                "Industrial"
            ]
        }
    },
    {
        "id": "1c3954ec-eca2-4b66-97fb-8e5c640b9052",
        "title": "",
        "chunk_text": "A trust region algorithm for equality constrained minimization: Convergence properties and implementation. SIAM Journal on Numerical Analysis, 22(3):575–591, 1985. A. W¨achter and L. T. Biegler. On the implementation of an interior-point filter line-search algorithm for large-scale nonlinear programming. Mathematical Programming, 106(1):25–57, 2005. U. C¸akmak and S. ¨Ozekici. Portfolio optimization in stochastic markets. Mathematical Methods of Operations Research, 63(1):151–168, 2005.",
        "metadata": {
            "author": "",
            "keywords": [
                "Convergence",
                "minimization",
                "programming",
                "SIAM",
                "Analysis",
                "trust",
                "region",
                "equality",
                "constrained",
                "properties"
            ]
        }
    },
    {
        "id": "7b75ef17-e571-487d-a7bd-8c1b23251bcc",
        "title": "",
        "chunk_text": "31 A Proofs of Section 4.1 A.1 Proof of Lemma 4.2 Proof. Recall that ¯Hk = ¯∇2fk + Pm i=1 ¯λi k∇2ci k, we have ∥¯Hk∥≤∥¯∇2fk −∇2fk∥+ ∥∇2fk∥+ ∥ m X i=1 (¯λi k −λi k)∇2ci k∥+ ∥ m X i=1 λi k∇2ci k∥ ≤∥¯∇2fk −∇2fk∥+ ∥∇2fk∥+ ∥¯λk −λk∥ \b m X i=1 ∥∇2ci k∥2 1/2 + ∥λk∥ \b m X i=1 ∥∇2ci k∥ 1/2 ≤∥¯∇2fk −∇2fk∥+ L∇f + √mLG √κ1,G ∥¯gk −gk∥+ √mLGκ∇f √κ1,G , where the last inequality follows from Assumption 4.1 and the definitions of λk and ¯λk.",
        "metadata": {
            "author": "",
            "keywords": [
                "Proof",
                "Section",
                "Lemma",
                "Proofs",
                "Assumption",
                "mLG",
                "mLGκ",
                "Recall",
                "inequality",
                "definitions"
            ]
        }
    },
    {
        "id": "ed9f865c-4bd7-4514-bfc3-dd03a4bf1898",
        "title": "",
        "chunk_text": "On the event Ak ∩Bk, ∥¯∇2fk −∇2fk∥≤ϵh + κh∆k and ∥¯gk −gk∥≤ϵg + κg∆2 k. Since ∆k ≤∆max, it follows that ∥¯Hk∥≤ϵh + κh∆max + L∇f + √mLG √κ1,G (ϵg + κg∆2 max + κ∇f). By setting κB = max{1, ϵh + κh∆max + L∇f + √mLG/√κ1,G · (ϵg + κg∆2 max + κ∇f)}, we complete the proof. ■ A.2 Proof of Lemma 4.3 Proof.",
        "metadata": {
            "author": "",
            "keywords": [
                "max",
                "proof",
                "mLG",
                "event",
                "Lemma",
                "setting",
                "complete"
            ]
        }
    },
    {
        "id": "8a09740d-8256-413f-915e-ce34138e2b4c",
        "title": "",
        "chunk_text": "We have ∥∇2 xLk −¯Hk∥= ∥∇2fk −¯∇2fk + m X i=1 (λi k −¯λi k)∇2ci k∥ ≤∥∇2fk −¯∇2fk∥+ ∥¯λk −λk∥ \b m X i=1 ∥∇2ci k∥2}1/2 ≤∥∇2fk −¯∇2fk∥+ √mLG √κ1,G ∥gk −¯gk∥ (Assumption 4.1) ≤ \u0012 κh + √mκgLG∆max √κ1,G \u0013 | {z } κH ∆k + \u0012 ϵh + √mLG √κ1,G ϵg \u0013 | {z } ϵH , where the last inequality is due to the event Ak ∩Bk and ∆k ≤∆max. Next, we show |τk −¯τk| ≤ κH∆k + ϵH. Let ¯ζk be a normalized eigenvector corresponding to ¯τk, then τk −¯τk ≤¯ζT k \u0002 ZT k (∇2 xLk −¯Hk)Zk \u0003 ¯ζk ≤∥∇2 xLk −¯Hk∥≤κH∆k + ϵH.",
        "metadata": {
            "author": "",
            "keywords": [
                "mLG",
                "max",
                "Assumption",
                "xLk",
                "mκgLG",
                "inequality",
                "due",
                "event",
                "show",
                "normalized"
            ]
        }
    },
    {
        "id": "a7ea98ac-fa7c-4660-a62c-93bcc5a1bb94",
        "title": "",
        "chunk_text": "Let ζk be a normalized eigenvector corresponding to τk, then ¯τk −τk ≤ζT k \u0002 ZT k ( ¯Hk −∇2 xLk)Zk \u0003 ζk ≤∥∇2 xLk −¯Hk∥≤κH∆k + ϵH. Combining the above two displays, we have |τk −¯τk| ≤κH∆k + ϵH, which implies |τ + k −¯τ + k | ≤ κH∆k + ϵH. We complete the proof. ■ 32 A.3 Proof of Lemma 4.4 Proof. Since the SOC step is not performed, xsk = xk + ∆xk.",
        "metadata": {
            "author": "",
            "keywords": [
                "xLk",
                "proof",
                "normalized",
                "eigenvector",
                "Lemma",
                "SOC",
                "Combining",
                "displays",
                "implies",
                "xsk"
            ]
        }
    },
    {
        "id": "479f3049-f3ad-4008-8ddd-6dbaae98890e",
        "title": "",
        "chunk_text": "By the definition of ℓ2 merit function Lµ(x) and (10), we have Lsk ¯µk −Lk ¯µk −Predk = fsk + ¯µk∥csk∥−fk −¯gT k ∆xk −1 2∆xT k ¯Hk∆xk −¯µk∥ck + Gk∆xk∥ . By the Taylor expansion of f(x) and the Lipschitz continuity of ∇f(x), we have fsk −fk −¯gT k ∆xk ≤(gk −¯gk)T ∆xk + 1 2L∇f∥∆xk∥2. Similarly, we have ∥csk∥−∥ck + Gk∆xk∥ ≤∥csk −ck −Gk∆xk∥≤1 2LG∥∆xk∥2.",
        "metadata": {
            "author": "",
            "keywords": [
                "csk",
                "fsk",
                "Predk",
                "Lsk",
                "Taylor",
                "Lipschitz",
                "merit",
                "definition",
                "function",
                "expansion"
            ]
        }
    },
    {
        "id": "d0b86af4-74e9-4de2-ac25-58a7446c913d",
        "title": "",
        "chunk_text": "Recall that the monotonic updating scheme of the merit parameter implies ¯µk ≤¯µT−1 for all k ≤T−1, and Assumption 4.1 and Lemma 4.2 imply ∥¯Hk∥≤κB under Ak. Combining the above two displays and using Cauchy-Schwartz inequality lead to Lsk ¯µk −Lk ¯µk −Predk ≤∥gk −¯gk∥∥∆xk∥+ 1 2(L∇f + κB + ¯µT−1LG)∥∆xk∥2. On the event Bk, we have ∥gk −¯gk∥≤ϵg + κg max{1, ∆max}∆k. Combining ∥∆xk∥≤∆k with the above display, we complete the proof. ■ A.4 Proof of Lemma 4.5 Proof.",
        "metadata": {
            "author": "",
            "keywords": [
                "Assumption",
                "Lemma",
                "imply",
                "proof",
                "Recall",
                "implies",
                "Predk",
                "Combining",
                "monotonic",
                "updating"
            ]
        }
    },
    {
        "id": "a4001f06-050c-4843-9472-abe7fd95f1df",
        "title": "",
        "chunk_text": "We have Lsk ¯µk −Lk ¯µk −Predk = fsk + ¯µk∥csk∥−fk −¯gT k ∆xk −1 2∆xT k ¯Hk∆xk −¯µk∥ck + Gk∆xk∥ ≤ fsk −fk −¯gT k ∆xk −1 2∆xT k ¯Hk∆xk + ¯µT−1∥csk −ck −Gk∆xk∥, (A.1) where we have used Assumption 4.1. First, we analyze the second term in (A.1). Since the SOC step is performed, we have xsk = xk + ∆xk + dk. For 1 ≤i ≤m, by the Taylor expansion, we obtain |ci sk −ci k −(∇ci k)T ∆xk| (8) = |ci sk −ci(xk + ∆xk) −(∇ci k)T dk| ≤LG(∥∆xk∥∥dk∥+ ∥dk∥2). Therefore, ∥csk −ck −Gk∆xk∥≤√mLG(∥∆xk∥∥dk∥+ ∥dk∥2).",
        "metadata": {
            "author": "",
            "keywords": [
                "fsk",
                "Predk",
                "Lsk",
                "Assumption",
                "csk",
                "SOC",
                "Taylor",
                "analyze",
                "term",
                "performed"
            ]
        }
    },
    {
        "id": "942de153-7706-4c3f-bd2b-1981984d42f6",
        "title": "",
        "chunk_text": "For ∥dk∥, we have ∥dk∥≤∥GT k [GkGT k ]−1∥∥c(xk + ∆xk) −ck −Gk∆xk∥≤ LG √κ1,G ∆2 k. (A.2) Combining the last two results and using the fact that ∆k ≤∆max, we have ∥csk −ck −Gk∆xk∥≤mLG∥∆xk∥∥dk∥+ mLG∥dk∥2 ≤ \u0012√mL2 G √κ1,G + √mL3 G∆max κ1,G \u0013 ∆3 k. (A.3) 33 Next, we analyze the first term in (A.1).",
        "metadata": {
            "author": "",
            "keywords": [
                "max",
                "mLG",
                "GkGT",
                "Combining",
                "csk",
                "results",
                "fact",
                "analyze",
                "term"
            ]
        }
    },
    {
        "id": "d166b513-0267-4a43-9914-5855ecda2c42",
        "title": "",
        "chunk_text": "For some points ϕ1 between [xk+∆xk, xk+∆xk+dk] and ϕ2 between [xk, xk + ∆xk], we have fsk = f(xk + ∆xk + dk) = f(xk + ∆xk) + ∇f(xk + ∆xk)T dk + 1 2dT k ∇2f(ϕ1)dk = fk + gT k ∆xk + ∇f(xk + ∆xk)T dk + 1 2∆xT k ∇2f(ϕ2)∆xk + 1 2dT k ∇2f(ϕ1)dk. Define eλk = −[GkGT k ]−1Gk∇f(xk + ∆xk).",
        "metadata": {
            "author": "",
            "keywords": [
                "points",
                "fsk",
                "Define",
                "eλk",
                "GkGT"
            ]
        }
    },
    {
        "id": "7a74ff46-a5ef-4530-8d08-295a92e0a4eb",
        "title": "",
        "chunk_text": "By the Taylor expansion, we have ∇f(xk + ∆xk)T dk (8) = eλT k [c(xk + ∆xk) −ck −Gk∆xk] = m X i=1 eλi k[ci(xk + ∆xk) −ci k −(∇ci k)T ∆xk] = 1 2 m X i=1 eλi k∆xT k ∇2ci(ϕi 3)∆xk, where the points {ϕi 3}m i=1 are between [xk, xk + ∆xk]. Recall that ¯Hk = ¯∇2fk + Pm i=1 ¯λi k∇2ci k.",
        "metadata": {
            "author": "",
            "keywords": [
                "eλi",
                "Taylor",
                "expansion",
                "eλT",
                "points",
                "Recall"
            ]
        }
    },
    {
        "id": "378c59d7-7f80-429a-bc12-e2e27720fc9d",
        "title": "",
        "chunk_text": "We combine the above two displays and have fsk −fk −¯gT k ∆xk −1 2∆xT k ¯Hk∆xk = (gk −¯gk)T ∆xk + 1 2∆xT k \u0000∇2f(ϕ2) −∇2fk \u0001 ∆xk + 1 2∆xT k \u0000∇2fk −¯∇2fk \u0001 ∆xk + 1 2 m X i=1 \u0000λi k −¯λi k \u0001 ∆xT k ∇2ci k∆xk + 1 2 m X i=1 \u0010 eλi k −λi k \u0011 ∆xT k ∇2ci k∆xk + 1 2dT k ∇2f (ϕ1) dk + 1 2 m X i=1 eλi k∆xT k \u0000∇2ci(ϕi 3) −∇2ci k \u0001 ∆xk ≤∥gk −¯gk∥∥∆xk∥+ L∇2f 2 ∥∆xk∥3 + 1 2∥∇2fk −¯∇2fk∥∥∆xk∥2 + √mLG 2 (∥λk −¯λk∥+ ∥eλk −λk∥)∥∆xk∥2 + L∇f 2 ∥dk∥2 + √mL∇2c 2 ∥eλk∥∥∆xk∥3 ≤∥gk −¯gk∥∆k + 1 2ϵh∆2 k + \u0012L∇2f + κh 2 \u0013 ∆3 k + √mLG 2 (∥λk −¯λk∥+ ∥eλk −λk∥)∆2 k + L∇f 2 ∥dk∥2 + √mL∇2c∥eλk∥ 2 ∆3 k (A.2) ≤∥gk −¯gk∥∆k + 1 2ϵh∆2 k + L∇2f + κh 2 + L∇fL2 G∆max 2κ1,G + √mL∇2c∥eλk∥ 2 !",
        "metadata": {
            "author": "",
            "keywords": [
                "eλk",
                "mLG",
                "eλi",
                "max",
                "fsk",
                "combine",
                "displays"
            ]
        }
    },
    {
        "id": "87c68221-602e-47b6-9088-48b0683f2b10",
        "title": "",
        "chunk_text": "∆3 k + √mLG 2 (∥λk −¯λk∥+ ∥eλk −λk∥)∆2 k, (A.4) where the second inequality is by Assumption 4.1 and the third inequality is by the event Ak and the fact that ∆xk ≤∆k (note that the SOC step is performed only when α = 1).",
        "metadata": {
            "author": "",
            "keywords": [
                "Assumption",
                "inequality",
                "mLG",
                "eλk",
                "SOC",
                "note",
                "event",
                "fact",
                "step",
                "performed"
            ]
        }
    },
    {
        "id": "82a2eeef-2a50-4b02-9596-3a091655d917",
        "title": "",
        "chunk_text": "Furthermore, on the event Bk, it follows from Assumption 4.1 that ∥λk −¯λk∥≤∥[GkGT k ]−1Gk∥∥gk −¯gk∥≤ κg √κ1,G ∆2 k + ϵg √κ1,G ≤κg∆max √κ1,G ∆k + ϵg √κ1,G , 34 ∥λk −eλk∥≤∥[GkGT k ]−1Gk∥∥gk −∇f(xk + ∆xk)∥≤ L∇f √κ1,G ∆k, ∥eλk∥≤∥[GkGT k ]−1Gk∥∥∇f(xk + ∆xk)∥≤ 1 √κ1,G (L∇f∥∆xk∥+ ∥gk∥) ≤L∇f∆max + κ∇f √κ1,G . To simplify the first term in (A.4), we claim that under Bk, ∥gk−¯gk∥≤(κg+1)∆2 k+min{ϵ3/2 g /∆k, ϵg}.",
        "metadata": {
            "author": "",
            "keywords": [
                "GkGT",
                "max",
                "eλk",
                "Assumption",
                "min",
                "event",
                "simplify",
                "term",
                "claim"
            ]
        }
    },
    {
        "id": "8d7d34b3-0d41-4930-bb05-8842417f5880",
        "title": "",
        "chunk_text": "To prove this, we notice that when ϵ3/2 g /∆k ≥ϵg, this claim is trivial due to the definition of Bk. When ϵ3/2 g /∆k < ϵg, we have ϵ1/2 g < ∆k. On the event Bk, we have ∥gk −¯gk∥≤ϵg + κg∆2 k < (κg + 1)∆2 k < (κg + 1)∆2 k + min{ϵ3/2 g /∆k, ϵg}. Thus the claim is proved.",
        "metadata": {
            "author": "",
            "keywords": [
                "prove",
                "notice",
                "trivial",
                "due",
                "definition",
                "claim",
                "min",
                "proved",
                "event"
            ]
        }
    },
    {
        "id": "7a4a52c4-2183-4461-b7c9-cf36acbd6cd5",
        "title": "",
        "chunk_text": "Combining this claim with the above displays, we have fsk −fk −¯gT k ∆xk −1 2∆xT k ¯Hk∆xk ≤ϵ3/2 g + \u00121 2ϵh + √mLG √κ1,G ϵg \u0013 ∆2 k + \u0012 κg + 1 + L∇2f + κh 2 \u0013 ∆3 k + \u0012L∇fL2 G∆max 2κ1,G + √mL∇2c(L∇f∆max + κ∇f) 2√κ1,G + √mLG(κg∆max + L∇f) 2√κ1,G \u0013 ∆3 k. (A.5) We complete the proof by combining (A.1), (A.3), and (A.5). ■ A.5 Proof of Lemma 4.8 Proof. We first notice that Assumption 4.7 implies the right-hand side of (21) is positive, thus the inequality is well defined. We first prove (13a) holds.",
        "metadata": {
            "author": "",
            "keywords": [
                "max",
                "mLG",
                "proof",
                "Combining",
                "displays",
                "fsk",
                "claim",
                "Lemma",
                "Assumption",
                "complete"
            ]
        }
    },
    {
        "id": "3deb78a2-aa98-437f-878f-e74d39c3c777",
        "title": "",
        "chunk_text": "By the algorithm design, we have ϑα = 2ϵf for α = 0. Thus, we only need to show that Aredk −2ϵf ≤ηPredk can be satisfied by (21) (recalling that Predk < 0). Since no SOC step will be performed when α = 0, xsk = xk + ∆xk and we have Aredk −2ϵf = ¯Lsk ¯µk −¯Lk ¯µk −2ϵf = ¯Lsk ¯µk −Lsk ¯µk + Lsk ¯µk −Lk ¯µk + Lk ¯µk −¯Lk ¯µk −2ϵf = ¯fsk −fsk + Lsk ¯µk −Lk ¯µk + fk −¯fk −2ϵf ≤| ¯fsk −fsk| + |fk −¯fk| + Predk + ϵg∆k + Υ1∆2 k −2ϵf.",
        "metadata": {
            "author": "",
            "keywords": [
                "Lsk",
                "fsk",
                "Aredk",
                "Predk",
                "design",
                "algorithm",
                "SOC",
                "ηPredk",
                "recalling",
                "show"
            ]
        }
    },
    {
        "id": "3bd5135e-a061-4e04-8671-ed9662cc6f5a",
        "title": "",
        "chunk_text": "(Lemma 4.4) Under Ck, when α = 0 we have |fk −¯fk| + | ¯fsk −fsk| ≤2ϵf + 2κf∆2 k. Since Predk ≤−κfcd 2 ∥¯∇Lk∥min \u001a ∆k, ∥¯∇Lk∥ ∥¯Hk∥ \u001b (21) = −κfcd 2 ∥¯∇Lk∥∆k, we only need to show 2κf∆2 k + ϵg∆k + Υ1∆2 k ≤κfcd 2 (1 −η)∥¯∇Lk∥∆k. (A.6) To this end, we note that (21) ⇒{(4κf + κg) + 2Υ1 + κB}∆k ≤κfcd(1 −η)∥∇Lk∥−{κfcd(1 −η) + 2} ϵg ⇒{(4κf + κfcd(1 −η)κg) + 2Υ1}∆k ≤κfcd(1 −η)∥∇Lk∥−{κfcd(1 −η) + 2} ϵg, since κfcd(1 −η) < 1. Rearranging the terms, we have (2κf + Υ1)∆k ≤κfcd 2 (1 −η)(∥∇Lk∥−κg∆k −ϵg) −ϵg.",
        "metadata": {
            "author": "",
            "keywords": [
                "κfcd",
                "fsk",
                "Lemma",
                "Predk",
                "min",
                "show",
                "end",
                "note",
                "Rearranging",
                "terms"
            ]
        }
    },
    {
        "id": "db2f52ba-a170-4688-9be7-5eb0e2070e1f",
        "title": "",
        "chunk_text": "(A.7) 35 Under Bk, when α = 0 we have ∥¯∇Lk∥≥∥∇Lk∥−κg∆k −ϵg. This, together with (A.7), implies (2κf + Υ1)∆k ≤κfcd 2 (1 −η)∥¯∇Lk∥−ϵg. Multiplying ∆k and rearranging the terms, we have shown (A.6), thus proved (13a). Next, we prove (13b). When α = 0, we have max{1, ∥¯Hk∥} ≤κB under Ak. Therefore it suffices to show ∥¯∇Lk∥≥ηκB∆k. Recalling κfcd ≤1, η < 1, we have (ηκB + κg)∆k ≤(4κf + κg + 2Υ1 + κB)∆k (21) ≤κfcd(1 −η)∥∇Lk∥−(κfcd(1 −η) + 2)ϵg ≤∥∇Lk∥−ϵg.",
        "metadata": {
            "author": "",
            "keywords": [
                "κfcd",
                "ηκB",
                "implies",
                "Multiplying",
                "terms",
                "shown",
                "proved",
                "prove",
                "rearranging",
                "max"
            ]
        }
    },
    {
        "id": "828bdb88-e24a-4c09-86f1-e7a9336da7ac",
        "title": "",
        "chunk_text": "Combining the above inequality with the relation ∥¯∇Lk∥≥∥∇Lk∥−κg∆k −ϵg, we obtain ∥¯∇Lk∥≥ ηκB∆k, thus (13b) holds. We complete the proof. ■ A.6 Proof of Lemma 4.9 Proof. To show the result, we consider two cases separately. Case A: max{τ + k , ∥∇Lk∥} = ∥∇Lk∥ and Case B: max{τ + k , ∥∇Lk∥} = τ + k . • Case A. When α = 1, ϑα = 2ϵf + ϵ3/2 g . Thus, to show (13a), we only need to show Aredk −2ϵf − ϵ3/2 g ≤ηPredk can be satisfied by (22).",
        "metadata": {
            "author": "",
            "keywords": [
                "proof",
                "ηκB",
                "holds",
                "Case",
                "Combining",
                "relation",
                "obtain",
                "show",
                "max",
                "inequality"
            ]
        }
    },
    {
        "id": "5bfbbe6d-57ed-4eb2-8a2d-0a2c975cc0b7",
        "title": "",
        "chunk_text": "By the algorithm design, we first consider the scenario in which the SOC step is not computed (i.e., xsk = xk + ∆xk). We have Aredk −2ϵf −ϵ3/2 g = ¯Lsk ¯µk −¯Lk ¯µk −2ϵf −ϵ3/2 g = ¯Lsk ¯µk −Lsk ¯µk + Lsk ¯µk −Lk ¯µk + Lk ¯µk −¯Lk ¯µk −2ϵf −ϵ3/2 g = ¯fsk −fsk + Lsk ¯µk −Lk ¯µk + fk −¯fk −2ϵf −ϵ3/2 g Lemma 4.4 ≤| ¯fsk −fsk| + |fk −¯fk| + Predk + ϵg∆k + Υ1∆2 k −2ϵf −ϵ3/2 g . (A.8) We observe that under Ak ∩Bk, (22) implies ∆k ≤∥¯∇Lk∥/∥¯Hk∥.",
        "metadata": {
            "author": "",
            "keywords": [
                "Lsk",
                "fsk",
                "SOC",
                "xsk",
                "design",
                "computed",
                "algorithm",
                "scenario",
                "step",
                "Predk"
            ]
        }
    },
    {
        "id": "3a2b1aa5-5f2f-42c0-bf4b-4aff6411552b",
        "title": "",
        "chunk_text": "Thus, Predk ≤−κfcd 2 ∥¯∇Lk∥min \u001a ∆k, ∥¯∇Lk∥ ∥¯Hk∥ \u001b = −κfcd 2 ∥¯∇Lk∥∆k. (A.9) Combining (A.8) and (A.9), and noticing that when α = 1 and Ck happens, |fk −¯fk| + | ¯fsk −fsk| ≤ 2ϵf + 2κf max{∆max, 1}∆2 k, we only need to show 2κf max{∆max, 1}∆2 k + ϵg∆k + Υ1∆2 k ≤κfcd 2 (1 −η)∥¯∇Lk∥∆k.",
        "metadata": {
            "author": "",
            "keywords": [
                "κfcd",
                "max",
                "Predk",
                "min",
                "fsk",
                "Combining",
                "show",
                "noticing"
            ]
        }
    },
    {
        "id": "bb82473a-9a8d-42dd-acc8-17e2ec3326cf",
        "title": "",
        "chunk_text": "(A.10) To this end, we note that (22) ⇒{(4κf + κg) max{∆max, 1} + 2Υ1}∆k ≤κfcd(1 −η)∥∇Lk∥−{κfcd(1 −η) + 2} ϵg ⇒{4κf max{∆max, 1} + κfcd(1 −η)κg max{∆max, 1} + 2Υ1}∆k ≤κfcd(1 −η)∥∇Lk∥−{κfcd(1 −η) + 2} ϵg, since κfcd(1 −η) < 1. Rearranging the terms, we have (2κf max{∆max, 1} + Υ1)∆k ≤κfcd 2 (1 −η)(∥∇Lk∥−κg max{∆max, 1}∆k −ϵg) −ϵg. (A.11) 36 Under Bk, we have ∥¯∇Lk∥≥∥∇Lk∥−κg max{∆max, 1}∆k −ϵg. This, together with (A.11), implies (2κf max{∆max, 1} + Υ1)∆k ≤κfcd 2 (1 −η)∥¯∇Lk∥−ϵg.",
        "metadata": {
            "author": "",
            "keywords": [
                "max",
                "κfcd",
                "end",
                "note",
                "Rearranging",
                "terms",
                "implies"
            ]
        }
    },
    {
        "id": "c26ee257-81d1-4b75-b806-ee7f3cf54bd0",
        "title": "",
        "chunk_text": "Multiplying ∆k and rearranging the terms, we have shown (A.10), thus proved (13a). Since (13a) holds, by the algorithm design, the SOC step will not be triggered in this iteration. Next, we show that (13b) holds. When α = 1, under Ak ∩Bk, we have max{1, ∥¯Hk∥} ≤κB. Therefore, it suffices to show ∥¯∇Lk∥≥ηκB∆k. Recalling κfcd ≤1, η < 1, we have (ηκB + κg max{∆max, 1})∆k ≤{(4κf + κg) max{∆max, 1} + 2Υ1 + κB}∆k (22) ≤κfcd(1 −η)∥∇Lk∥−{κfcd(1 −η) + 2}ϵg ≤∥∇Lk∥−ϵg.",
        "metadata": {
            "author": "",
            "keywords": [
                "max",
                "holds",
                "Multiplying",
                "κfcd",
                "terms",
                "shown",
                "proved",
                "ηκB",
                "rearranging",
                "SOC"
            ]
        }
    },
    {
        "id": "9215e54b-fd38-4fcf-9872-a7329f97d6d4",
        "title": "",
        "chunk_text": "(A.12) Since ∥¯∇Lk∥≥∥∇Lk∥−κg max{∆max, 1}∆k −ϵg under Bk, which combined with (A.12) leads to ∥¯∇Lk∥≥ηκB∆k, thus (13b) holds. • Case B. In this scenario, we show that (13a) holds by considering two subcases. Case B.1: ∥ck∥≥ r and Case B.2: ∥ck∥< r. • Case B.1. In this case, the SOC step will not be performed, so we need to show that Aredk − 2ϵf −ϵ3/2 g ≤ηPredk holds for xsk = xk +∆xk.",
        "metadata": {
            "author": "",
            "keywords": [
                "max",
                "Case",
                "ηκB",
                "holds",
                "leads",
                "combined",
                "show",
                "Aredk",
                "SOC",
                "scenario"
            ]
        }
    },
    {
        "id": "b1d256e2-9501-40aa-b2ad-9a9962c64c55",
        "title": "",
        "chunk_text": "Using the same analysis as in Case A, we have (A.8) and |fk −¯fk| + | ¯fsk −fsk| ≤2ϵf + 2κf max{∆max, 1}∆2 k under Ck with α = 1. Since Predk ≤−κfcd 2 ¯τ + k ∥ck∥∆k ≤−κfcd 2 ¯τ + k r∆k, it suffices to show 2κf max{∆max, 1}∆2 k + ϵg∆k + Υ1∆2 k ≤(1 −η)κfcd 2 ¯τ + k r∆k. (A.13) Now, we show that this can be achieved by ∆k satisfying (22).",
        "metadata": {
            "author": "",
            "keywords": [
                "max",
                "fsk",
                "κfcd",
                "Case",
                "analysis",
                "show",
                "Predk",
                "suffices",
                "satisfying",
                "achieved"
            ]
        }
    },
    {
        "id": "4aab9e26-ce87-4992-9ed8-832bdd2cf9a8",
        "title": "",
        "chunk_text": "We notice that τ + k (22) ≥ \u00124κf max{∆max, 1} + 2Υ1 + 2Υ2 (1 −η)κfcd min{1, r} + κH + η \u0013 ∆k + (2 + 2√mLG/√κ1,G)ϵg + 2ϵh (1 −η)κfcd min{1, r} ≥ \u00124κf max{∆max, 1} + 2Υ1 (1 −η)rκfcd + κH \u0013 ∆k + 2ϵg (1 −η)rκfcd + ϵH (by the def. of ϵH). Rearranging the terms, we have (2κf max{∆max, 1} + Υ1)∆k + ϵg ≤(1 −η)κfcd 2 r \u0000τ + k −κH∆k −ϵH \u0001 . Multiplying ∆k on both sides, using the relation ¯τ + k ≥τ + k −κH∆k−ϵH under Ak∩Bk (cf. Lemma 4.3), we can show that (A.13) holds. • Case B.2.",
        "metadata": {
            "author": "",
            "keywords": [
                "max",
                "rκfcd",
                "min",
                "κfcd",
                "mLG",
                "notice",
                "def.",
                "Multiplying",
                "Lemma",
                "Case"
            ]
        }
    },
    {
        "id": "cd48e4bc-40b7-4eb9-a5dd-90461aac5322",
        "title": "",
        "chunk_text": "In this case, if Aredk −2ϵf −ϵ3/2 g ≤ηPredk holds for xsk = xk + ∆xk, then there is nothing to prove. If not, the SOC step dk (cf. Section 2.1.3) will be performed. Thus, we only need to show that Aredk −2ϵf −ϵ3/2 g ≤ηPredk holds for xsk = xk + ∆xk +dk.",
        "metadata": {
            "author": "",
            "keywords": [
                "Aredk",
                "ηPredk",
                "xsk",
                "case",
                "prove",
                "holds",
                "Section",
                "SOC",
                "performed",
                "step"
            ]
        }
    },
    {
        "id": "921b3b82-9860-4d1f-ab9b-59a9bc34b93a",
        "title": "",
        "chunk_text": "Similar to the proof in Case B.1, but using the conclusion of Lemma 4.5, we have Aredk −2ϵf −ϵ3/2 g ≤| ¯fsk −fsk| + |fk −¯fk| + Predk + \u00121 2ϵh + √mLG 2√κ1,G ϵg \u0013 ∆2 k + Υ2∆3 k −2ϵf 37 ≤Predk + \u00121 2ϵh + √mLG 2√κ1,G ϵg \u0013 ∆2 k + (Υ2 + 2κf)∆3 k, where the last inequality is due to the definition of Ck with α = 1. By the algorithm design, we have Predk ≤−κfcd 2 ¯τ + k ∆2 k, thus we only need to show \u00121 2ϵh + √mLG 2√κ1,G ϵg \u0013 ∆2 k + (Υ2 + 2κf)∆3 k ≤(1 −η)κfcd 2 ¯τ + k ∆2 k.",
        "metadata": {
            "author": "",
            "keywords": [
                "Predk",
                "mLG",
                "fsk",
                "Case",
                "Lemma",
                "Aredk",
                "κfcd",
                "Similar",
                "proof",
                "conclusion"
            ]
        }
    },
    {
        "id": "0ece34c7-20c6-4e5e-bc98-f5e475b03850",
        "title": "",
        "chunk_text": "(A.14) To this end, we note that with the definition of ϵH, τ + k (22) ≥ \u00124κf max{∆max, 1} + 2Υ1 + 2Υ2 (1 −η)κfcd min{1, r} + κH + η \u0013 ∆k + (2 + 2√mLG/√κ1,G)ϵg + 2ϵh (1 −η)κfcd min{1, r} ≥ \u0012 4κf + 2Υ2 (1 −η)κfcd + κH \u0013 ∆k + ϵh + ϵg √mLG/√κ1,G (1 −η)κfcd + ϵH. Rearranging the terms and multiplying ∆2 k on both sides, we have (2κf + Υ2)∆3 k + \u00121 2ϵh + √mLG 2√κ1,G ϵg \u0013 ∆2 k ≤κfcd 2 (1 −η)(τ + k −κH∆k −ϵH)∆2 k. Since under Ak ∩Bk, we have ¯τ + k ≥τ + k −κH∆k −ϵH (cf. Lemma 4.3).",
        "metadata": {
            "author": "",
            "keywords": [
                "κfcd",
                "mLG",
                "max",
                "min",
                "end",
                "note",
                "definition",
                "Lemma",
                "Rearranging",
                "multiplying"
            ]
        }
    },
    {
        "id": "213aca36-51db-4b2a-b2dd-45a77869e8b4",
        "title": "",
        "chunk_text": "This relation, together with the above display, has proved (A.14). Combining Case B.1 and Case B.2, we have shown that (13a) holds in Case B. Next, we show that (13b) holds in Case B. By the algorithm design, it is sufficient to show ¯τ + k ≥η∆k. We note that (22) implies τ + k ≥(κH + η)∆k + ϵH. This, combined with the relation ¯τ + k ≥τ + k −κH∆k −ϵH under Ak ∩Bk, leads to ¯τ + k ≥η∆k. We thus complete the proof. ■ A.7 Proof of Lemma 4.10 Proof.",
        "metadata": {
            "author": "",
            "keywords": [
                "Case",
                "proof",
                "holds",
                "display",
                "proved",
                "relation",
                "show",
                "Lemma",
                "Combining",
                "design"
            ]
        }
    },
    {
        "id": "0cd9a0ed-62cc-46a3-b918-762836edfecd",
        "title": "",
        "chunk_text": "Since (13a) holds, we have ¯Lk+1 ¯µk −¯Lk ¯µk ≤ηPredk + ϑα. When α = 0, we have Predk ≤−κfcd 2 ∥¯∇Lk∥min \u001a ∆k, ∥¯∇Lk∥ ∥¯Hk∥ \u001b (13b) ≤−κfcd 2 η2∆2 k. When α = 1, we similarly have Predk ≤−κfcd 2 max \u001a ∥¯∇Lk∥min \u001a ∆k, ∥¯∇Lk∥ ∥¯Hk∥ \u001b , ¯τ + k ∆k (∆k + ∥ck∥) \u001b (13b) ≤− κfcdη3 2 max{∆max, 1}∆3 k. We complete the proof by combining the above two displays. ■ 38 A.8 Proof of Lemma 4.12 Proof. (a) We define the sequence {ϕk}k as ϕk = max \u001a logγ \u0012∆k b∆′ \u0013 , 0 \u001b and discuss the following cases.",
        "metadata": {
            "author": "",
            "keywords": [
                "max",
                "κfcd",
                "Predk",
                "proof",
                "min",
                "holds",
                "ηPredk",
                "Lemma",
                "similarly",
                "displays"
            ]
        }
    },
    {
        "id": "9919715c-0289-4a3b-8a79-a1952b3d80d4",
        "title": "",
        "chunk_text": "Case A: ΛkΘk = 1, Case B: Λk(1 −Θk) = 1, and Case C: Λk = 0. • Case A. When ΛkΘk = 1, we have min{∆k, ∆k+1} ≥b∆′ and ∆k+1 = min{γ∆k, ∆max}. Since ∆k+1 ≥∆k ≥b∆′, we have logγ \u0012∆k+1 b∆′ \u0013 ≥logγ \u0012∆k b∆′ \u0013 ≥0. Therefore, ϕk = logγ(∆k/b∆′) and ϕk+1 = logγ(∆k+1/b∆′). Since ∆k+1 ≤γ∆k, we further have logγ \u0012∆k b∆′ \u0013 + 1 ≥logγ \u0012∆k+1 b∆′ \u0013 , thus ϕk+1 −ϕk ≤1. • Case B. When Λk(1 −Θk) = 1, we have min{∆k, ∆k+1} ≥b∆′ and ∆k+1 = ∆k/γ.",
        "metadata": {
            "author": "",
            "keywords": [
                "Case",
                "logγ",
                "ΛkΘk",
                "min",
                "max"
            ]
        }
    },
    {
        "id": "41cea7b1-0b71-460d-bc4c-27a343789029",
        "title": "",
        "chunk_text": "Therefore, we must have ∆k ≥γ b∆′, which leads to logγ \u0012∆k b∆′ \u0013 ≥1 and logγ \u0012∆k+1 b∆′ \u0013 = logγ \u0012∆k b∆′ \u0013 −1 ≥0. Thus, ϕk = logγ(∆k/b∆′), ϕk+1 = logγ(∆k+1/b∆′) and ϕk+1 −ϕk = −1. • Case C. When Λk = 0, we have max{∆k, ∆k+1} ≤b∆′. Thus, ϕk = ϕk+1 = 0. Combining the above three cases, we sum ϕk+1 −ϕk over k = 0, · · · , T −1 and get ϕT −ϕ0 = T−1 X k=0 ϕk+1 −ϕk ≤ T−1 X k=0 ΛkΘk − T−1 X k=0 Λk(1 −Θk).",
        "metadata": {
            "author": "",
            "keywords": [
                "logγ",
                "leads",
                "Case",
                "cases",
                "max",
                "ΛkΘk",
                "Combining",
                "sum"
            ]
        }
    },
    {
        "id": "c2869815-311a-49ce-8ce1-74c166712efd",
        "title": "",
        "chunk_text": "(A.15) Noticing that ϕ0 = logγ(∆0/b∆′) (since ∆0 > b∆′) and ϕT ≥0, we rearrange the terms in (A.15) and complete the proof. (b) This is a direct corollary of (a), which can be proved by rearranging terms of (a). (c) We define the sequence {eϕk}k as eϕk = max ( logγ b∆′ ∆k ! , 0 ) and discuss three cases. Case A: (1 −Λk)Θk = 1, Case B: (1 −Λk)(1 −Θk) = 1, Case C: Λk = 1. • Case A. When (1−Λk)Θk = 1, we have max{∆k, ∆k+1} ≤b∆′ and ∆k+1 = min{γ∆k, ∆max}. Since ∆k ≤b∆′ ≤γ−1∆max, we have γ∆k ≤∆max.",
        "metadata": {
            "author": "",
            "keywords": [
                "Noticing",
                "max",
                "Case",
                "terms",
                "logγ",
                "proof",
                "rearrange",
                "complete",
                "eϕk",
                "cases"
            ]
        }
    },
    {
        "id": "ca9acb89-b238-456e-a5de-5ce4bad18f5c",
        "title": "",
        "chunk_text": "Thus ∆k+1 = γ∆k. In this case, to ensure ∆k+1 ≤b∆′, we must have ∆k ≤γ−1 b∆′. The above relations imply logγ b∆′ ∆k ! ≥1 and logγ b∆′ ∆k+1 ! = logγ b∆′ ∆k ! −1 ≥0. 39 Therefore, eϕk = logγ(b∆′/∆k), eϕk+1 = logγ(b∆′/∆k+1), and eϕk+1 −eϕk = −1. • Case B. When (1 −Λk)(1 −Θk) = 1, we have max{∆k, ∆k+1} ≤b∆′ and ∆k+1 = ∆k/γ. Thus, logγ b∆′ ∆k ! ≥0 and logγ b∆′ ∆k+1 ! = logγ b∆′ ∆k ! + 1. Therefore, eϕk = logγ(b∆′/∆k), eϕk+1 = logγ(b∆′/∆k+1), and eϕk+1 −eϕk = 1. • Case C.",
        "metadata": {
            "author": "",
            "keywords": [
                "logγ",
                "eϕk",
                "case",
                "ensure",
                "relations",
                "imply",
                "max"
            ]
        }
    },
    {
        "id": "53426f11-8c79-4b06-a48b-839498527463",
        "title": "",
        "chunk_text": "When Λk = 1, we have min{∆k+1, ∆k} ≥b∆′, thus eϕk+1 = eϕk = 0. Combining the above cases, we sum eϕk+1 −eϕk over k = 0, · · · , T −1 and get eϕT −eϕ0 = T−1 X k=0 eϕk+1 −eϕk ≤ T−1 X k=0 (1 −Λk)(1 −Θk) − T−1 X k=0 (1 −Λk)Θk. Noticing that eϕ0 = max{logγ(b∆′/∆0), 0} = 0 (since ∆0 = ∆max ≥b∆′) and eϕT ≥0, we rearrange the terms and complete the proof.",
        "metadata": {
            "author": "",
            "keywords": [
                "eϕk",
                "min",
                "eϕT",
                "max",
                "Combining",
                "cases",
                "sum",
                "logγ",
                "Noticing",
                "proof"
            ]
        }
    },
    {
        "id": "9a87d880-6f82-4034-aa54-22a22e4ee125",
        "title": "",
        "chunk_text": "(d) We note that T X k=0 (1 −Λk)Ik = T X k=0 (1 −Λk)IkΘk ≤ T X k=0 (1 −Λk)Θk ≤ T X k=0 (1 −Λk)(1 −Θk) = T X k=0 (1 −Λk)(1 −Θk)(1 −Ik) ≤ T X k=0 (1 −Λk)(1 −Ik). The first equality follows from Lemmas 4.8 and 4.9 that small and accurate iterations must be sufficient, the second inequality is by Lemma 4.12 (c), and the last equality is by the converse of Lemmas 4.8 and 4.9 that small and insufficient iterations must be inaccurate. ■ A.9 Proof of Lemma 4.14 Proof.",
        "metadata": {
            "author": "",
            "keywords": [
                "Lemmas",
                "Lemma",
                "Proof",
                "IkΘk",
                "small",
                "note",
                "iterations",
                "equality",
                "sufficient",
                "inaccurate"
            ]
        }
    },
    {
        "id": "0d648fdc-fed7-48b5-8591-43826c85ad6a",
        "title": "",
        "chunk_text": "It follows from P(Ik = 1 | Fk−1/2) ≥p that {PT−1 k=0 Ik −pT} is a submartingale. Moreover, T X k=0 Ik −p(T + 1) ! − T−1 X k=0 Ik −pT ! = |IT −p| ≤max{|1 −p|, |0 −p|} = p. The last equality follows from p > 1/2. By the Azuma-Hoeffding inequality, for any T ≥1 and c ≥0, we have P \"T−1 X k=0 Ik −pT ≤−c # ≤exp \u001a −c2 2Tp2 \u001b . Letting c = (p −bp)T, we complete the proof. ■ 40 A.10 Proof of Lemma 4.15 Proof.",
        "metadata": {
            "author": "",
            "keywords": [
                "proof",
                "submartingale",
                "Letting",
                "Lemma",
                "max",
                "exp",
                "equality",
                "inequality",
                "Azuma-Hoeffding",
                "complete"
            ]
        }
    },
    {
        "id": "3c6cbbc7-f36c-4a53-9c0e-f0750b64d386",
        "title": "",
        "chunk_text": "It suffices to show that Tϵ > T −1 and PT−1 k=0 Ik ≥pT imply PT−1 k=0 ΘkIkΛk ≥ \u0000p −1 2 \u0001 T − 1 2 logγ \u0010 ∆0 b∆ \u0011 −1.",
        "metadata": {
            "author": "",
            "keywords": [
                "ΘkIkΛk",
                "logγ",
                "suffices",
                "show",
                "imply"
            ]
        }
    },
    {
        "id": "eea138d3-ad1c-454c-ad35-73a6c169793b",
        "title": "",
        "chunk_text": "For k = 0, 1, · · · , T −1, we define the following sets and use | · | to denote their car- dinality: • AL = {k : IkΛk = 1} as the set of iterations that are large and accurate; • IL = {k : (1 −Ik)Λk = 1} as the set of iterations that are large and inaccurate; • AS = {k : Ik(1 −Λk) = 1} as the set of iterations that are small and accurate; • IS = {k : (1 −Ik)(1 −Λk) = 1} as the set of iterations that are small and inaccurate.",
        "metadata": {
            "author": "",
            "keywords": [
                "iterations",
                "set",
                "accurate",
                "inaccurate",
                "large",
                "small",
                "dinality",
                "IkΛk",
                "sets",
                "car"
            ]
        }
    },
    {
        "id": "813fa7ce-f7bd-4721-af79-6273fb420efa",
        "title": "",
        "chunk_text": "This classification is valid, noticing that |IS| + |AS| + |IL| + |AL| = T. It follows from Lemma 4.12 (b) and the observation PT−1 k=0 Λk = |IL| + |AL| that T−1 X k=0 ΛkΘk ≥1 2(|IL| + |AL|) −1 2 logγ \u0012∆0 b∆′ \u0013 . (A.16) Since PT−1 k=0 Ik ≥pT, by the fact that an iteration is either large or small, we have |IL| + |IS| = T−1 X k=0 (1 −Ik) = T − T−1 X k=0 Ik ≤T −pT. (A.17) Moreover, it follows from Lemma 4.12 (d) that |AS| = T−1 X k=0 (1 −Λk)Ik ≤ T−1 X k=0 (1 −Λk)(1 −Ik) = |IS|.",
        "metadata": {
            "author": "",
            "keywords": [
                "Lemma",
                "valid",
                "noticing",
                "classification",
                "ΛkΘk",
                "logγ",
                "small",
                "observation",
                "fact",
                "iteration"
            ]
        }
    },
    {
        "id": "fa947388-7bce-4ec9-93c3-1a85281e8822",
        "title": "",
        "chunk_text": "(A.18) Combining the above displays, we have T−1 X k=0 ΛkΘkIk = T−1 X k=0 ΛkΘk − T−1 X k=0 ΛkΘk(1 −Ik) (A.16) ≥ 1 2(|IL| + |AL|) −1 2 logγ \u0012∆0 b∆′ \u0013 − T−1 X k=0 Λk(1 −Ik) = 1 2(|IL| + |AL|) −1 2 logγ \u0012∆0 b∆′ \u0013 −|IL| = 1 2(T −|IS| −|AS|) −1 2 logγ \u0012∆0 b∆′ \u0013 −|IL| (A.17) ≥ 1 2(T −|IS| −|AS|) −1 2 logγ \u0012∆0 b∆′ \u0013 + |IS| −T + pT = 1 2(|IS| −|AS|) + \u0012 p −1 2 \u0013 T −1 2 logγ \u0012∆0 b∆′ \u0013 (A.18) ≥ \u0012 p −1 2 \u0013 T −1 2 logγ \u0012∆0 b∆′ \u0013 ≥ \u0012 p −1 2 \u0013 T −1 2 logγ \u0012∆0 b∆ \u0013 −1.",
        "metadata": {
            "author": "",
            "keywords": [
                "logγ",
                "ΛkΘk",
                "Combining",
                "ΛkΘkIk",
                "displays"
            ]
        }
    },
    {
        "id": "41ab41a0-fe5e-48cf-a6d8-ff56326b4efd",
        "title": "",
        "chunk_text": "In the last inequality, we use the relation γ−2 b∆≤b∆′, which implies ∆0/b∆′ ≤∆0/(γ−2 b∆) and thus logγ \u0012∆0 b∆′ \u0013 ≤logγ \u0012 ∆0 γ−2 b∆ \u0013 = logγ \u0012∆0 b∆ \u0013 + 2. We complete the proof. ■ 41 B Proofs of Section 4.2 B.1 Proof of Theorem 4.16 Proof. Since P{Tϵ ≤T −1} = 1 −P{Tϵ > T −1}, we will bound P{Tϵ > T −1} in the proof. By the law of total probability, we have P{Tϵ > T −1} = P ( Tϵ > T −1, T−1 X k=0 (ek + esk) > (2ϵf + 2s)T ) | {z } A + P ( Tϵ > T −1, T−1 X k=0 (ek + esk) ≤(2ϵf + 2s)T ) | {z } B .",
        "metadata": {
            "author": "",
            "keywords": [
                "logγ",
                "proof",
                "inequality",
                "implies",
                "relation",
                "Proofs",
                "esk",
                "Section",
                "Theorem",
                "complete"
            ]
        }
    },
    {
        "id": "ebc3318e-05dd-4a47-b0d8-ab0897830e45",
        "title": "",
        "chunk_text": "(B.1) In what follows, we present lemmas that bound P(A) and P(B), separately. The proofs of these lemmas are presented in Appendices B.2, B.3 and B.4. Lemma B.1 (Burkholder-type inequality). Let Xk, k = 0, 1, · · · , T −1, be random variables with E[Xk | X0:k−1] = 0 and E[|Xk|p] < ∞for any k ≥0 and some p ∈(1, 2], then E \" T−1 X k=0 Xk p# ≤22−p T−1 X k=0 E[|Xk|p]. (B.2) Lemma B.2.",
        "metadata": {
            "author": "",
            "keywords": [
                "separately",
                "lemmas",
                "Lemma",
                "present",
                "bound",
                "Appendices",
                "Burkholder-type",
                "proofs",
                "presented",
                "inequality"
            ]
        }
    },
    {
        "id": "0e094c1d-4f8d-47fc-87d4-d8255c3fa159",
        "title": "",
        "chunk_text": "Under conditions of the probabilistic heavy-tailed zeroth-order oracle, for any T ≥1, s ≥0, and δ specified in (18), we have P(A) ≤2 exp  −2(ϵf −eϵf + s)2 (3 + δ)2e1+δΥ 2 1+δ f · T  + 32+δΥf (ϵf −eϵf + s)1+δ · T −δ. Lemma B.3. Under Assumptions 4.1, 4.7, when bp satisfies (24) and T satisfies (25), we have P(B) ≤exp \u001a −(p −bp)2 2p2 T \u001b . (B.3) Combining (B.1) with the conclusions of Lemmas B.2 and B.3, we complete the proof. ■ B.2 Proof of Lemma B.1 Proof.",
        "metadata": {
            "author": "",
            "keywords": [
                "eϵf",
                "exp",
                "Lemma",
                "δΥf",
                "proof",
                "oracle",
                "Lemmas",
                "conditions",
                "probabilistic",
                "heavy-tailed"
            ]
        }
    },
    {
        "id": "fc7d2d76-6b5a-47c0-8d68-f138de77f68a",
        "title": "",
        "chunk_text": "To prove the result, we fist show the following inequality: |a + b|p ≤|a|p + p · sgn(a) · |a|p−1b + 22−p|b|p. (B.4) When a = 0, the inequality holds trivially as 22−p ≥1 for p ∈(1, 2]. Moreover, when p = 2, the in- equality also holds trivially by the observation that sgn(a)|a| = a. In what follows, we consider a ̸= 0 and p ∈(1, 2). We divide |a|p on both sides and get 1 + b a p ≤1 + p b a + 22−p b a p , 42 where we use the observation that sgn(a)/|a| = 1/a.",
        "metadata": {
            "author": "",
            "keywords": [
                "sgn",
                "inequality",
                "result",
                "holds",
                "trivially",
                "prove",
                "fist",
                "show",
                "observation",
                "equality"
            ]
        }
    },
    {
        "id": "756261d3-44f2-4de2-a798-08220da1bd02",
        "title": "",
        "chunk_text": "Denoting x = b/a, it suffices to show that f(x) = 1 + px + 22−p |x|p −|1 + x|p ≥0. To this end, we consider three cases. Case A: b/a > 0, Case B: −1 < b/a ≤0, and Case C: b/a ≤−1. • Case A: b/a > 0: In this case f(x) = 1 + px + 22−pxp −(1 + x)p with f(0) = 0, f′(0) = 0 and f′′(x) = p(p −1) \"\u00122 x \u00132−p − \u0012 1 1 + x \u00132−p# . Since 1 + x > x/2 for all x > 0, when p < 2, we have \u00122 x \u00132−p > \u0012 1 1 + x \u00132−p =⇒f′′(x) > 0.",
        "metadata": {
            "author": "",
            "keywords": [
                "Case",
                "Denoting",
                "suffices",
                "show",
                "cases",
                "end",
                "pxp"
            ]
        }
    },
    {
        "id": "f25e2024-9ad8-4626-bdd7-2622f000f8e4",
        "title": "",
        "chunk_text": "Therefore, for all x > 0, we have f′(x) > 0, which then implies f(x) > 0, and thus (B.4) holds. • Case B: −1 < b/a ≤0: We substitute y = 1 + x so that 0 ≤y ≤1. Then, f(x) = f(y −1) = 22−p(1 −y)p + p(y −1) + 1 −yp ≥(1 −y)p + p(y −1) + 1 −yp := g(y). Then, we have g(1) = 0, g′(0) = g′(1) = 0, and g′′(y) = p(p −1) \"\u0012 1 1 −y \u00132−p − \u00121 y \u00132−p# . Note that g′′(y) has exactly one root at y = 1/2. Moreover, g′′(y) < 0 for y < 1/2 and g′′(y) > 0 for y > 1/2, so g′(y) has a minimum at y = 1 2.",
        "metadata": {
            "author": "",
            "keywords": [
                "holds",
                "implies",
                "Case",
                "Note",
                "substitute",
                "root",
                "minimum"
            ]
        }
    },
    {
        "id": "b2a429c9-2e66-4c5d-bbaf-4c115f3069c5",
        "title": "",
        "chunk_text": "Since g′(0) = g′(1) = 0, we conclude that g′(y) < 0 for all 0 < y < 1. As g(y) is monotonically decreasing on (0, 1) and g(1) = 0, we have g(y) ≥0 for all 0 ≤y ≤1. Therefore, f(x) ≥0. • Case C: b/a ≤−1: We substitute z = −1 −x so that z ≥0. Then f(x) = f(−z −1) =: h(z) and h(z) = 22−p(1 + z)p −p(1 + z) + 1 −zp. Then, we have h(0) ≥0, h′(1) = 0, and h′(z) = p(p −1) \"\u0012 2 1 + z \u00132−p − \u00121 z \u00132−p# . Note that h′′(z) = 0 only at z = 1.",
        "metadata": {
            "author": "",
            "keywords": [
                "conclude",
                "Case",
                "monotonically",
                "decreasing",
                "Note",
                "substitute"
            ]
        }
    },
    {
        "id": "1801c3d0-8b0d-4c3f-b9ac-e0668add87ec",
        "title": "",
        "chunk_text": "Moreover, h′′(z) < 0 for z < 1 and h′′(z) > 0 for z > 1, so h′(z) has a minimum at z = 1. Since h′(1) = 0, we know h′(z) ≥0 for all z ≥0, and hence h(z) ≥0 for all z ≥0. Combining the above three cases, we prove (B.4). Next, we prove (B.2) by induction. The inequality holds trivially for T = 1, as 22−p ≥1 for p ∈ (1, 2]. Suppose the inequality holds for T = n, we now consider T = n + 1.",
        "metadata": {
            "author": "",
            "keywords": [
                "prove",
                "minimum",
                "inequality",
                "holds",
                "Combining",
                "cases",
                "induction",
                "Suppose",
                "trivially"
            ]
        }
    },
    {
        "id": "b734c79c-cc73-4bf7-8626-60ab759e467a",
        "title": "",
        "chunk_text": "Applying (B.4) with a = Pn−1 k=0 Xk and b = Xn, we have n X k=0 Xk p ≤ n−1 X k=0 Xk p + p · sgn( n−1 X k=0 Xk) · n−1 X k=0 Xk p−1 · Xn + 22−p|Xn|p. 43 Taking expectation on both sides, we have E \" n X k=0 Xk p# ≤E \" n−1 X k=0 Xk p# + pE  sgn( n−1 X k=0 Xk) n−1 X k=0 Xk p−1 · Xn  + 22−pE [|Xn|p] . Using E[Xn | X0:n−1] = 0, we get E \" n X k=0 Xk p# ≤E \" n−1 X k=0 Xk p# + 22−pE [|Xn|p] . We complete the proof by induction. ■ B.3 Proof of Lemma B.2 Proof.",
        "metadata": {
            "author": "",
            "keywords": [
                "sgn",
                "proof",
                "Applying",
                "Taking",
                "Lemma",
                "sides",
                "induction",
                "expectation",
                "complete"
            ]
        }
    },
    {
        "id": "6c90be3f-9ff2-4471-8808-c2dff1bc38c1",
        "title": "",
        "chunk_text": "We observe that P(A) ≤P (T−1 X k=0 (ek + esk) > (2ϵf + 2s)T ) = P (T−1 X k=0 (ek + esk −E[ek | Fk−1] −E[esk | Fk−1/2]) > (2ϵf + 2s)T − T−1 X k=0 (E[ek | Fk−1] + E[esk | Fk−1/2]) ) ≤P (T−1 X k=0 (ek −E[ek | Fk−1]) > (ϵf + s)T − T−1 X k=0 E[ek | Fk−1] ) + P (T−1 X k=0 (esk −E[esk | Fk−1/2]) > (ϵf + s)T − T−1 X k=0 E[esk | Fk−1/2] ) (by union bound) (17) ≤P (T−1 X k=0 (ek −E[ek | Fk−1]) > (ϵf −eϵf + s)T ) + P (T−1 X k=0 (esk −E[esk | Fk−1/2]) > (ϵf −eϵf + s)T ) .",
        "metadata": {
            "author": "",
            "keywords": [
                "esk",
                "eϵf",
                "bound",
                "observe",
                "union"
            ]
        }
    },
    {
        "id": "96b17321-6067-4306-824c-6750f9e98b27",
        "title": "",
        "chunk_text": "(B.5) Next we bound (B.5) by discussing two cases. Case A: δ ∈(0, 1] and Case B: δ ∈(1, ∞). • Case A: When δ ∈(0, 1].",
        "metadata": {
            "author": "",
            "keywords": [
                "Case",
                "bound",
                "discussing",
                "cases"
            ]
        }
    },
    {
        "id": "fa452316-d936-4f97-93aa-5b3e0a0631be",
        "title": "",
        "chunk_text": "Since ek −E[ek | Fk−1] is a martingale difference that satisfies (18), it follows from the Markov’s inequality and the conclusion of Lemma B.1 that P (T−1 X k=0 (ek −E[ek | Fk−1]) > (ϵf −eϵf + s)T ) ≤ E \u0014 PT−1 k=0 (ek −E[ek | Fk−1]) 1+δ\u0015 (ϵf −eϵf + s)1+δT 1+δ ≤ 21−δ PT−1 k=0 E h |ek −E[ek | Fk−1]|1+δi (ϵf −eϵf + s)1+δT 1+δ ≤ 21−δTΥf (ϵf −eϵf + s)1+δT 1+δ = 21−δΥf (ϵf −eϵf + s)1+δ · T −δ.",
        "metadata": {
            "author": "",
            "keywords": [
                "eϵf",
                "Lemma",
                "Markov",
                "δTΥf",
                "δΥf",
                "satisfies",
                "martingale",
                "difference",
                "inequality",
                "conclusion"
            ]
        }
    },
    {
        "id": "b70864a5-f8cc-4f51-94b7-c8ff41bd9b89",
        "title": "",
        "chunk_text": "44 We can similarly show P (T−1 X k=0 (esk −E[esk | Fk−1/2]) > (ϵf −eϵf + s)T ) ≤ 21−δΥf (ϵf −eϵf + s)1+δ · T −δ. Therefore, for δ ∈(0, 1] we have P(A) ≤ 22−δΥf (ϵf −eϵf + s)1+δ · T −δ. • Case B: When δ ∈(1, ∞).",
        "metadata": {
            "author": "",
            "keywords": [
                "eϵf",
                "esk",
                "δΥf",
                "similarly",
                "show",
                "Case"
            ]
        }
    },
    {
        "id": "561d7034-fcf2-45d3-9f98-8025de8c2c12",
        "title": "",
        "chunk_text": "We apply the martingale Fuk–Nagaev inequality (Fan et al., 2017, Corollary 2.5) and get P (T−1 X k=0 (ek −E[ek | Fk−1]) > (ϵf −eϵf + s)T ) ≤exp \u0012 −(ϵf −eϵf + s)2T 2 2V \u0013 + C (ϵf −eϵf + s)1+δT 1+δ , (B.6) where V = 1 4(3 + δ)2e1+δ T−1 X k=0 E h (ek −E [ek | Fk−1])2 | Fk−1 i , C = \u0012 1 + 2 1 + δ \u00131+δ T−1 X k=0 E h |ek −E [ek | Fk−1]|1+δ | Fk−1 i . By the H¨older’s inequality and (18), we have E h (ek −E [ek | Fk−1])2 | Fk−1 i ≤Υ 2 1+δ f .",
        "metadata": {
            "author": "",
            "keywords": [
                "eϵf",
                "inequality",
                "Corollary",
                "Fuk",
                "Nagaev",
                "Fan",
                "exp",
                "older",
                "apply",
                "martingale"
            ]
        }
    },
    {
        "id": "68ecc35a-b08f-4dcb-b222-3dd9c52b228e",
        "title": "",
        "chunk_text": "Thus V ≤1 4(3 + δ)2e1+δΥ 2 1+δ f T and C ≤ \u0012 1 + 2 1 + δ \u00131+δ ΥfT. (B.7) Combining (B.6) and (B.7), we have P (T−1 X k=0 (ek −E[ek | Fk−1]) > (ϵf −eϵf + s)T ) ≤exp  −2(ϵf −eϵf + s)2 (3 + δ)2e1+δΥ 2 1+δ f T   + Υf \u0012 3 + δ (1 + δ)(ϵf −eϵf + s) \u00131+δ · T −δ. (B.8) By similar argument, we have P (T−1 X k=0 (esk −E[esk | Fk−1/2]) > (ϵf −eϵf + s)T ) ≤exp  −2(ϵf −eϵf + s)2 (3 + δ)2e1+δΥ 2 1+δ f T   + Υf \u0012 3 + δ (1 + δ)(ϵf −eϵf + s) \u00131+δ · T −δ.",
        "metadata": {
            "author": "",
            "keywords": [
                "eϵf",
                "exp",
                "ΥfT",
                "Combining",
                "esk",
                "argument",
                "similar"
            ]
        }
    },
    {
        "id": "852b067e-0dd5-4674-bf96-7d7889bb92cb",
        "title": "",
        "chunk_text": "(B.9) 45 Combining (B.8) and (B.9), we have P(A) ≤2 exp  −2(ϵf −eϵf + s)2 (3 + δ)2e1+δΥ 2 1+δ f T  + 2Υf \u0012 3 + δ (1 + δ)(ϵf −eϵf + s) \u00131+δ · T −δ. Combining the conclusions in Case A. and Case B., we have P(A) ≤2 exp  −2(ϵf −eϵf + s)2 (3 + δ)2e1+δΥ 2 1+δ f T  + 2Υf · max \u001a\u0010 3+δ 1+δ \u00111+δ , 21−δ \u001b (ϵf −eϵf + s)1+δ · T −δ ≤2 exp  −2(ϵf −eϵf + s)2 (3 + δ)2e1+δΥ 2 1+δ f T  + 32+δΥf (ϵf −eϵf + s)1+δ · T −δ, as (3 + δ)/(1 + δ) < 3 for δ > 0. We thus complete the proof.",
        "metadata": {
            "author": "",
            "keywords": [
                "eϵf",
                "exp",
                "Combining",
                "Case",
                "max",
                "δΥf",
                "conclusions",
                "proof",
                "complete"
            ]
        }
    },
    {
        "id": "7fd10b9e-460c-4cc1-90aa-58ac4fbb23b6",
        "title": "",
        "chunk_text": "■ B.4 Proof of Lemma B.3 Proof. Denoting d = logγ(∆0/b∆) + 2, by the law of total probability, we have P(B) = P ( Tϵ > T −1, T−1 X k=0 (ϑα + ek + esk) ≤(2ϵf + ϑα + 2s)T, T−1 X k=0 ΛkIkΘk ≥ \u0012 bp −1 2 \u0013 T −d 2 ) | {z } B1 + P ( Tϵ > T −1, T−1 X k=0 (ϑα + ek + esk) ≤(2ϵf + ϑα + 2s)T, T−1 X k=0 ΛkIkΘk < \u0012 bp −1 2 \u0013 T −d 2 ) | {z } B2 . To prove P(B1) = 0, we will show that Tϵ > T −1, PT−1 k=0 (ϑα +ek +esk) ≤(2ϵf +ϑα +2s)T together imply PT−1 k=0 ΛkIkΘk < (bp−1/2)T −d/2.",
        "metadata": {
            "author": "",
            "keywords": [
                "Proof",
                "Lemma",
                "esk",
                "ΛkIkΘk",
                "logγ",
                "Denoting",
                "probability",
                "law",
                "total",
                "prove"
            ]
        }
    },
    {
        "id": "7b6ffe01-00c7-495c-ad85-8d5f763b1c7f",
        "title": "",
        "chunk_text": "Suppose, on the contrary, PT−1 k=0 ΛkIkΘk ≥(bp−1/2)T −d/2 holds. Then unifying α = 0 and α = 1, we discuss the following two cases. Case A: ΛkIkΘk = 1 and Case B: ΛkIkΘk = 0. • Case A: ΛkIkΘk = 1. We have Lk+1 ¯µk −Lk ¯µk = Lk+1 ¯µk −¯Lk+1 ¯µk + ¯Lk+1 ¯µk −¯Lk ¯µk + ¯Lk ¯µk −Lk ¯µk ≤|fk+1 −¯fk+1| + | ¯fk −fk| + ¯Lk+1 ¯µk −¯Lk ¯µk ≤−hα(∆k) + ek + esk + ϑα (by Lemma 4.10) ≤−hα(b∆′) + ek + esk + ϑα.",
        "metadata": {
            "author": "",
            "keywords": [
                "ΛkIkΘk",
                "Case",
                "Suppose",
                "holds",
                "contrary",
                "esk",
                "cases",
                "Lemma",
                "unifying",
                "discuss"
            ]
        }
    },
    {
        "id": "295d908a-6e8b-4d39-a389-c11a6f60c2ed",
        "title": "",
        "chunk_text": "(since ∆k ≥b∆′ when Λk = 1) Dividing ¯µk on both sides and noticing that ¯µ0 ≤¯µk ≤¯µT−1 and γ−2 b∆< b∆′, we have 1 ¯µk (Lk+1 ¯µk −finf) −1 ¯µk (Lk ¯µk −finf) ≤−1 ¯µk hα(b∆′) + 1 ¯µk (ϑα + ek + esk) < − 1 ¯µT−1 hα(γ−2 b∆) + 1 ¯µ0 (ϑα + ek + esk). 46 • Case B: ΛkIkΘk = 0. We consider two subcases. Case B.1: (13a) holds in the k-th iteration; and Case B.2: (13a) does not hold the k-th iteration.",
        "metadata": {
            "author": "",
            "keywords": [
                "finf",
                "esk",
                "Dividing",
                "Case",
                "sides",
                "noticing",
                "iteration",
                "k-th",
                "ΛkIkΘk",
                "subcases"
            ]
        }
    },
    {
        "id": "7eb9b39c-86c7-47e2-afe2-c4840f28b474",
        "title": "",
        "chunk_text": "• Case B.1: In this case, we have Lk+1 ¯µk −Lk ¯µk = Lk+1 ¯µk −¯Lk+1 ¯µk + ¯Lk+1 ¯µk −¯Lk ¯µk + ¯Lk ¯µk −Lk ¯µk ≤|fk+1 −¯fk+1| + | ¯fk −fk| + ηPredk + ϑα (by(13a)) ≤ek + esk + ϑα. The last inequality is because Predk < 0 but we can no longer guarantee (13b). • Case B.2: In this case, since xk+1 = xk, we have Lk+1 ¯µk −Lk ¯µk = 0. We combine Case B.1 and Case B.2 and divide ¯µk on both sides. Since ¯µ0 ≤¯µk, we have 1 ¯µk (Lk+1 ¯µk −finf) −1 ¯µk (Lk ¯µk −finf) ≤1 ¯µ0 (ϑα + ek + esk).",
        "metadata": {
            "author": "",
            "keywords": [
                "Case",
                "ηPredk",
                "esk",
                "Predk",
                "finf",
                "guarantee",
                "inequality",
                "longer",
                "divide",
                "sides"
            ]
        }
    },
    {
        "id": "aacc46af-e7de-45f9-9b2f-697eb4449122",
        "title": "",
        "chunk_text": "(B.10) Using the results in (B.10) , we sum over k = 0, 1, · · · , T −1. Since ¯µk ≤¯µk+1, we have 1 ¯µT−1 (LT ¯µT −1 −finf) −1 ¯µ0 (L0 ¯µ0 −finf) ≤−hα(γ−2 b∆) ¯µT−1 T−1 X k=0 ΛkIkΘk + T−1 X k=0 ϑα + ek + esk ¯µ0 . Rearranging the terms, using the relations PT−1 k=0 (ϑα+ek+esk) ≤(2ϵf+ϑα+2s)T and PT−1 k=0 ΛkIkΘk ≥ (bp −1/2)T −d/2, we have 1 ¯µT−1 (LT ¯µT −1 −finf) ≤1 ¯µ0 (L0 ¯µ0 −finf) −hα(γ−2 b∆) ¯µT−1 \u0012 bp −1 2 \u0013 T + 2ϵf + ϑα + 2s ¯µ0 T + hα(γ−2 b∆)d 2¯µT−1 .",
        "metadata": {
            "author": "",
            "keywords": [
                "finf",
                "ΛkIkΘk",
                "esk",
                "results",
                "sum",
                "Rearranging",
                "terms",
                "relations"
            ]
        }
    },
    {
        "id": "8c8b2a73-a51d-407c-a06c-eddc2c6f41fb",
        "title": "",
        "chunk_text": "(B.11) However, the conditions of bp and T imply that the right-hand side of (B.11) is negative. This yields a contradiction, since the left-hand side is non-negative. We thus proved P(B1) = 0. Next, we bound P(B2). We observe that P(B2) ≤P ( Tϵ > T −1, T−1 X k=0 ΛkIkΘk < \u0012 bp −1 2 \u0013 T −d 2 ) = P ( Tϵ > T −1, T−1 X k=0 Ik < bpT, T−1 X k=0 ΛkIkΘk < \u0012 bp −1 2 \u0013 T −d 2 ) | {z } B21 + P ( Tϵ > T −1, T−1 X k=0 Ik ≥bpT, T−1 X k=0 ΛkIkΘk < \u0012 bp −1 2 \u0013 T −d 2 ) | {z } B22 .",
        "metadata": {
            "author": "",
            "keywords": [
                "ΛkIkΘk",
                "side",
                "negative",
                "conditions",
                "imply",
                "right-hand",
                "bpT",
                "contradiction",
                "non-negative",
                "yields"
            ]
        }
    },
    {
        "id": "a3d2b27e-f46f-4560-8819-d473cf46ce9e",
        "title": "",
        "chunk_text": "For P(B21), we have P(B21) ≤P (T−1 X k=0 Ik < bpT ) Lemma 4.14 ≤ exp \u001a −(p −bp)2 2p2 T \u001b . 47 For P(B22), if follows from Lemma 4.15 that P(B22) = 0. Combining the above results, we have P(B) ≤exp \u001a −(p −bp)2 2p2 T \u001b . We thus complete the proof. ■ C Proofs of Section 4.3 C.1 Proof of Theorem 4.20 Proof. Same as the proof of Theorem 4.16 in Appendix B, we also bound P{Tϵ > T −1} via (B.1) and analyze P(A) and P(B) separately.",
        "metadata": {
            "author": "",
            "keywords": [
                "Lemma",
                "proof",
                "exp",
                "Theorem",
                "bpT",
                "Proofs",
                "Section",
                "Appendix",
                "Combining",
                "results"
            ]
        }
    },
    {
        "id": "da169e85-08e7-48ff-b037-3f09917380fa",
        "title": "",
        "chunk_text": "To bound P(A), we observe that P(A) ≤P (T−1 X k=0 (ek + esk) ≥(2ϵf + 2s)T ) = P (T−1 X k=0 (ek + esk −E[ek | Fk−1] −E[esk | Fk−1/2]) ≥(2ϵf + 2s)T − T−1 X k=0 (E[ek | Fk−1] + E[esk | Fk−1/2]) ) = P (T−1 X k=0 λ(ek + esk −E[ek | Fk−1] −E[esk | Fk−1/2]) ≥λ(2ϵf + 2s)T − T−1 X k=0 λ(E[ek | Fk−1] + E[esk | Fk−1/2]) ) for any λ ∈ \u0014 0, 1 b \u0015 (17) ≤P (T−1 X k=0 λ(ek + esk −E[ek | Fk−1] −E[esk | Fk−1/2]) ≥λ(2ϵf −2eϵf + 2s)T ) ≤E h e PT −1 k=0 λ(ek+esk−E[ek|Fk−1]−E[esk|Fk−1/2])i · e−λ(2ϵf−2eϵf+2s)T (by Markov inequality).",
        "metadata": {
            "author": "",
            "keywords": [
                "esk",
                "Markov",
                "inequality",
                "bound",
                "observe"
            ]
        }
    },
    {
        "id": "d599b5b1-67af-4e85-8227-e81af28b358f",
        "title": "",
        "chunk_text": "Next, we prove E h e PT −1 k=0 λ(ek+esk−E[ek|Fk−1]−E[esk|Fk−1/2])i ≤e (2v)2λ2 2 T . To show this, we first note that E[eλ(ek+esk−E[ek|Fk−1]−E[esk|Fk−1/2]) | Fk−1] ≤ \u0010 E[e2λ(ek−E[ek|Fk−1]) | Fk−1] · E[e2λ(esk−E[esk|Fk−1/2]) | Fk−1] \u00111/2 (by H¨older’s inequality) ≤ \u0010 E[e2λ(ek−E[ek|Fk−1]) | Fk−1] · E[E[e2λ(esk−E[esk|Fk−1/2]) | Fk−1/2] | Fk−1] \u00111/2 (by Tower’s property) (19) ≤ \u0010 e2λ2v2 · e2λ2v2\u00111/2 = e λ2(2v)2 2 for all λ ∈ \u0014 0, 1 2b \u0015 .",
        "metadata": {
            "author": "",
            "keywords": [
                "esk",
                "Tower",
                "older",
                "inequality",
                "property",
                "prove",
                "show",
                "note"
            ]
        }
    },
    {
        "id": "da38944c-c001-4a32-bd86-aed009a89ded",
        "title": "",
        "chunk_text": "(C.1) 48 Thus, given Fk−1, ek + esk follows a sub-exponential distribution with parameters (2v, 2b).",
        "metadata": {
            "author": "",
            "keywords": [
                "esk",
                "parameters",
                "sub-exponential",
                "distribution"
            ]
        }
    },
    {
        "id": "8514c498-ea5f-4a2c-abd9-ba25bf49589c",
        "title": "",
        "chunk_text": "Since E h e PT −1 k=0 λ(ek+esk−E[ek|Fk−1]−E[esk|Fk−1/2])i = E h E h e PT −1 k=0 λ(ek+esk−E[ek|Fk−1]−E[esk|Fk−1/2]) | FT−2 ii = E h e PT −2 k=0 λ(ek+esk−E[ek|Fk−1]−E[esk|Fk−1/2]) · E h eλ(eT −1+esT −1−E[eT −1|FT −2]−E[esT −1|FT −3/2]) | FT−2 ii (C.1) ≤E h e PT −2 k=0 λ(ek+esk−E[ek|Fk−1]−E[esk|Fk−1/2])i · e λ2(2v)2 2 , we can inductively show E h e PT −1 k=0 λ(ek+esk−E[ek|Fk−1]−E[esk|Fk−1/2])i ≤e (2v)2λ2 2 T for any λ ∈ \u0014 0, 1 2b \u0015 . Therefore, we have P(A) ≤exp \b (2v2λ2)T −2λ(ϵf −eϵf + s)T .",
        "metadata": {
            "author": "",
            "keywords": [
                "esk",
                "esT",
                "exp",
                "eϵf",
                "inductively",
                "show"
            ]
        }
    },
    {
        "id": "73123eb0-1668-49b8-9852-f971ce98d3fd",
        "title": "",
        "chunk_text": "To find the optimal λ to minimize the right-hand side, we define l(λ) = (2v2λ2)T −2λ(ϵf −eϵf + s)T. Since el(λ) is monotonically increasing with respect to l(λ), we only need to minimize l(λ), which is achieved by λ∗= (ϵf −eϵf +s)/(2v2). Recalling that λ ∈[0, 1/(2b)], when (ϵf −eϵf +s)/(2v2) ≤1/(2b), we have min λ el(λ) = el(λ∗) = e− (ϵf −eϵf +s)2 2v2 T , otherwise min λ el(λ) = el(1/(2b)) ≤e− ϵf −eϵf +s 2b T .",
        "metadata": {
            "author": "",
            "keywords": [
                "eϵf",
                "minimize",
                "side",
                "find",
                "optimal",
                "right-hand",
                "define",
                "min",
                "monotonically",
                "increasing"
            ]
        }
    },
    {
        "id": "75360b7a-2c08-4473-ac6f-4256e5521a24",
        "title": "",
        "chunk_text": "Combining the above two displays, we have P(A) ≤exp \u001a −1 2 min \u0012(ϵf −eϵf + s)2 v2 , ϵf −eϵf + s b \u0013 T \u001b . (C.2) Note that the proof of Lemma B.3 is independent to the probabilistic heavy-tailed zeroth-order oracle design, thus we can bound P(B) in the same approach as in Lemma B.3 and obtain (B.3). Combining (B.3) and (C.2), we complete the proof. ■ C.2 Proof of Lemma 4.23 Proof.",
        "metadata": {
            "author": "",
            "keywords": [
                "eϵf",
                "Lemma",
                "proof",
                "Note",
                "exp",
                "min",
                "Combining",
                "displays",
                "design",
                "obtain"
            ]
        }
    },
    {
        "id": "bb5b4b40-74fb-44e6-918b-8acc90cb58dc",
        "title": "",
        "chunk_text": "It follows from the definition of Ck and the union bound that P(Ck | Fk−1/2) ≥1 −P(ek ≥ϵf + κf∆α+2 k | Fk−1) −P(esk ≥ϵf + κf∆α+2 k | Fk−1/2), where we also use the fact that P(ek ≥ϵf + κf∆α+2 k | Fk−1) = P(ek ≥ϵf + κf∆α+2 k | Fk−1/2). We consider the probability P(ek ≥ϵf + κf∆α+2 k | Fk−1) first.",
        "metadata": {
            "author": "",
            "keywords": [
                "esk",
                "definition",
                "union",
                "bound",
                "fact",
                "probability"
            ]
        }
    },
    {
        "id": "9eb6b873-0241-4f27-8449-e6e9083bd1a9",
        "title": "",
        "chunk_text": "We have P(ek ≥ϵf + κf∆α+2 k | Fk−1) = P(ek −E[ek | Fk−1] ≥ϵf + κf∆α+2 k −E[ek | Fk−1] | Fk−1) = P(eλ(ek−E[ek|Fk−1]) ≥eλ(ϵf+κf∆α+2 k −E[ek|Fk−1]) | Fk−1) 49 (17) ≤P(eλ(ek−E[ek|Fk−1]) ≥eλ(ϵf−eϵf) | Fk−1) ≤E[eλ(ek−E[ek|Fk−1]) | Fk−1] · e−λ(ϵf−eϵf) (by Markov’s inequality) (19) ≤e λ2v2 2 −λ(ϵf−eϵf), where the second equality is due to the monotonicity of ex and holds for any λ ∈[0, 1/b].",
        "metadata": {
            "author": "",
            "keywords": [
                "eϵf",
                "Markov",
                "inequality",
                "equality",
                "due",
                "monotonicity",
                "holds"
            ]
        }
    },
    {
        "id": "10453499-c3fa-482d-aa86-e6e94c7f2804",
        "title": "",
        "chunk_text": "We apply the same approach as in Appendix C.1 to find the optimal λ that minimizes the right-hand side and get P(ek ≥ϵf + κf∆α+2 k | Fk−1) ≤exp \u001a −1 2 min \u0012(ϵf −eϵf)2 v2 , ϵf −eϵf b \u0013\u001b . (C.3) Applying the same analysis to P(esk ≥ϵf + κf∆α+2 k | Fk−1/2), we can still get (C.3). Combining them together, we show (32) and complete the proof. ■ 50",
        "metadata": {
            "author": "",
            "keywords": [
                "eϵf",
                "Appendix",
                "Applying",
                "exp",
                "min",
                "esk",
                "apply",
                "approach",
                "find",
                "optimal"
            ]
        }
    }
]