[
    {
        "id": "66ede82d-e142-4067-84a3-0a7d1cbde31b",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning Chu Myaet Thwala, Minh N. H. Nguyenb, Ye Lin Tuna, Seong Tae Kima, My T.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "OnDev-LCT",
                "Thwala",
                "Minh",
                "Lightweight",
                "Convolutional",
                "Transformers",
                "Federated",
                "Learning",
                "Chu",
                "Myaet"
            ]
        }
    },
    {
        "id": "951429fb-4088-4826-bd81-54ec88e78299",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "Thaic, Choong Seon Honga,∗ aDepartment of Computer Science and Engineering, Kyung Hee University, Yongin-si, Gyeonggi-do 17104, South Korea bVietnam - Korea University of Information and Communication Technology, Danang, Vietnam cDepartment of Computer and Information Science and Engineering, University of Florida, Gainesville, Florida 32611, USA Abstract Federated learning (FL) has emerged as a promising approach to collaboratively train machine learning models across multiple edge devices while preserving privacy.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "Engineering",
                "University",
                "Florida",
                "Computer",
                "Science",
                "Yongin-si",
                "Gyeonggi-do",
                "Danang",
                "Gainesville",
                "Korea"
            ]
        }
    },
    {
        "id": "42a39394-8932-4879-9d90-c128a5c1eefc",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "The success of FL hinges on the efficiency of participating models and their ability to handle the unique challenges of distributed learning. While several variants of Vision Transformer (ViT) have shown great potential as alternatives to modern convolutional neural networks (CNNs) for centralized training, the unprecedented size and higher compu- tational demands hinder their deployment on resource-constrained edge devices, challenging their widespread application in FL.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "learning",
                "success",
                "hinges",
                "efficiency",
                "participating",
                "models",
                "ability",
                "handle",
                "unique",
                "challenges"
            ]
        }
    },
    {
        "id": "d7eb085b-7c63-4c24-aad8-0609d23bcd17",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "Since client devices in FL typically have limited computing resources and communication bandwidth, models intended for such devices must strike a balance between model size, computational efficiency, and the ability to adapt to the diverse and non-IID data distributions encountered in FL. To address these challenges, we propose OnDev-LCT: Lightweight Convolutional Transformers for On-Device vision tasks with limited training data and resources.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "devices",
                "bandwidth",
                "size",
                "computational",
                "efficiency",
                "limited",
                "resources",
                "data",
                "client",
                "typically"
            ]
        }
    },
    {
        "id": "1755e1cb-8035-487c-bfa4-37fe1d281609",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "Our models incorporate image-specific inductive biases through the LCT tokenizer by leveraging efficient depthwise separable convolutions in residual linear bottleneck blocks to extract local features, while the multi-head self-attention (MHSA) mechanism in the LCT encoder implicitly facilitates capturing global repre- sentations of images.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "MHSA",
                "LCT",
                "features",
                "self-attention",
                "mechanism",
                "repre",
                "sentations",
                "images",
                "models",
                "incorporate"
            ]
        }
    },
    {
        "id": "cbcb1915-d033-456c-8d7d-e6cdaf8f7918",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "Extensive experiments on benchmark image datasets indicate that our models outperform existing lightweight vision models while having fewer parameters and lower computational demands, making them suitable for FL scenarios with data heterogeneity and communication bottlenecks. Keywords: convolutional transformer, lightweight, computer vision, federated learning, data heterogeneity 1.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "Extensive",
                "demands",
                "making",
                "bottlenecks",
                "models",
                "data",
                "heterogeneity",
                "Keywords",
                "lightweight",
                "vision"
            ]
        }
    },
    {
        "id": "24e916a1-9d9c-4fc4-afee-61b609db9ce3",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "Introduction Recent trends in deep learning enable artificial intelligence to make significant strides, even surpassing humans in many tasks. When it comes to image understanding, convolutional neural networks (CNNs) have become the de facto among nu- merous types of deep neural architectures.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "Recent",
                "Introduction",
                "strides",
                "tasks",
                "CNNs",
                "deep",
                "trends",
                "learning",
                "enable",
                "artificial"
            ]
        }
    },
    {
        "id": "f04f8b8a-f5b3-4609-b9ef-2e40b90f734f",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "Along with the tremendous size of publicly available image datasets, i.e., Im- ageNet (Deng et al., 2009) and Microsoft COCO (Lin et al., 2014), deep CNNs like ResNet (He et al., 2016) and its variants have been dominating the field of computer vision for decades.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "ageNet",
                "Deng",
                "COCO",
                "Lin",
                "Microsoft",
                "datasets",
                "deep",
                "ResNet",
                "decades",
                "tremendous"
            ]
        }
    },
    {
        "id": "89357676-ed70-4419-a765-19a761ba4cd3",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "Evidently, lightweight CNNs, such as MobileNets (Howard et al., 2017; Sandler et al., 2018; Howard et al., 2019) and their extensions (Han et al., 2020; Li et al., 2021b), have achieved state-of-the-art performance for many on-device vision tasks since image-specific inductive biases (Lenc and Vedaldi, 2015; Mitchell et al., 2017) allow them to learn visual representations and recognize complex patterns with fewer parameters.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "Howard",
                "Sandler",
                "Han",
                "Lenc",
                "Vedaldi",
                "Mitchell",
                "Evidently",
                "lightweight",
                "CNNs",
                "MobileNets"
            ]
        }
    },
    {
        "id": "9eac955e-05fb-4784-b561-350a0f9e70da",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "CNNs, however, are spatially local (Zuo et al., 2015), making them ∗Corresponding author Email addresses: chumyaet@khu.ac.kr (Chu Myaet Thwal), nhnminh@vku.udn.vn (Minh N. H. Nguyen), yelintun@khu.ac.kr (Ye Lin Tun), st.kim@khu.ac.kr (Seong Tae Kim), mythai@cise.ufl.edu (My T.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "CNNs",
                "Zuo",
                "Chu",
                "Thwal",
                "Minh",
                "Email",
                "Myaet",
                "local",
                "making",
                "addresses"
            ]
        }
    },
    {
        "id": "27de99c5-b3fa-46b1-bf8c-f36b8355dfc2",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "Thai), cshong@khu.ac.kr (Choong Seon Hong) 0 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1 MACs (×109) 60 65 70 75 80 85 90 Accuracy (%) 0.21M 0.31M 0.51M 0.91M 0.28M 0.48M 0.42M 0.45M 1.21M 1.23M 0.27M 0.47M 0.67M 0.21M 0.72M OnDev-LCT CCT ViT ResNet MobileNetv2 Figure 1: Performance comparison on CIFAR-10 dataset in the central- ized scenario. The comparison is performed among small model variants, i.e., #Params < 1.3M, while constraining the computational budget within 0.1 × 109 MACs.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "Accuracy",
                "Figure",
                "MACs",
                "Choong",
                "Hong",
                "Performance",
                "Thai",
                "Seon",
                "CCT",
                "OnDev-LCT"
            ]
        }
    },
    {
        "id": "71cbffb1-7dcd-4d7c-bfd7-ee5671274842",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "Each bubble’s area is proportional to the model size. hard to capture global representations, i.e., contextual depen- dencies between different image regions. Parallel to the widespread use of CNNs in computer vision, transformer architectures (Vaswani et al., 2017), which exploit the multi-head self-attention mechanism, have emerged to be- come an integral part of natural language processing (NLP).",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "size",
                "bubble",
                "area",
                "proportional",
                "model",
                "NLP",
                "Vaswani",
                "hard",
                "representations",
                "contextual"
            ]
        }
    },
    {
        "id": "fe425204-f8dc-49b7-a1a9-c6f2c3035e00",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "In response to the dominant success of transformers in large- scale language modeling, attention-based architectures have re- Preprint submitted to Neural Networks January 23, 2024 arXiv:2401.11652v1 [cs.CV] 22 Jan 2024 cently piqued a growing interest in computer vision.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "Jan",
                "arXiv",
                "cs.CV",
                "Preprint",
                "January",
                "Neural",
                "Networks",
                "large",
                "scale",
                "modeling"
            ]
        }
    },
    {
        "id": "7b748569-cb09-4723-af5e-118fa7cfd3fb",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "Attention- embedded CNN models (Wang et al., 2017; Hu et al., 2018; Bello et al., 2019), transformer-based networks (Parmar et al., 2018; Dosovitskiy et al., 2021; Touvron et al., 2021), and most recently, hybrid architectures of convolutional transform- ers (Wu et al., 2021a; Hassani et al., 2021; Mehta and Rastegari, 2022; Graham et al., 2021; Chen et al., 2022b) have shown their potential in a wide range of vision tasks.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "Wang",
                "Bello",
                "Parmar",
                "Dosovitskiy",
                "Touvron",
                "Hassani",
                "Mehta",
                "Rastegari",
                "Graham",
                "Chen"
            ]
        }
    },
    {
        "id": "75539c8c-08b6-4841-889a-ebb64eab235d",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "Vision Transformer (ViT) (Dosovitskiy et al., 2021), adapting the encoder design of NLP transformer (Vaswani et al., 2017) with minimum mod- ifications, shows its superior performance over cutting-edge CNNs in several benchmark tasks. Hence, ViT becomes more prevalent in computer vision for its ability to learn global rep- resentations through attention mechanisms.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "Transformer",
                "ifications",
                "Dosovitskiy",
                "Vaswani",
                "NLP",
                "ViT",
                "adapting",
                "mod",
                "shows",
                "tasks"
            ]
        }
    },
    {
        "id": "148bae2e-f9d0-4553-ac19-39b8ede8adba",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "Conversely, the general trend towards improving the perfor- mance of transformers is to increase the number of parame- ters, which demands a vast amount of computational resources. Thus, the unprecedented size of ViT hinders its applications on resource-constrained devices.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "Conversely",
                "ters",
                "perfor",
                "mance",
                "parame",
                "resources",
                "general",
                "trend",
                "improving",
                "transformers"
            ]
        }
    },
    {
        "id": "bbeeaed7-e967-4769-8590-05a8e0c62547",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "Moreover, ViT requires pre- training on large-scale image datasets, i.e., ImageNet-21K or JFT-300M, to learn generalizable visual representations since it lacks some inductive biases inherent in CNNs (Dosovitskiy et al., 2021), such as translation equivariance and locality.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "Dosovitskiy",
                "ViT",
                "pre",
                "training",
                "datasets",
                "CNNs",
                "locality",
                "requires",
                "large-scale",
                "image"
            ]
        }
    },
    {
        "id": "f374cace-f2ac-4f0e-bf0b-e3ac1cfa51d4",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "Re- cent studies focus on combining the strengths of CNNs with transformers and have become acknowledged for their high- performing hybrid architectures of convolutional transform- ers (Wu et al., 2021a; Hassani et al., 2021; Mehta and Raste- gari, 2022; Graham et al., 2021; Chen et al., 2022b).",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "Hassani",
                "Mehta",
                "Raste",
                "Graham",
                "Chen",
                "ers",
                "gari",
                "cent",
                "high",
                "performing"
            ]
        }
    },
    {
        "id": "cbccf9d6-2558-4506-8f11-c971f4dbe2cb",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "However, these hybrid models still have a huge number of parameters or require extensive data augmentation techniques, demanding higher computational resources for training to perform at the same level as CNNs in low-data regimes. On the other hand, for many domains including science and medical research (Varoquaux and Cheplygina, 2022; Ghassemi et al., 2020), it is costly to get a large quantity of annotated data with the size of ImageNet (Deng et al., 2009) for training.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "techniques",
                "demanding",
                "regimes",
                "training",
                "data",
                "hybrid",
                "models",
                "huge",
                "number",
                "parameters"
            ]
        }
    },
    {
        "id": "079cadf4-e031-4847-9b41-2054b2f77f12",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "Due to the privacy-sensitive nature of user data in such domains, col- lecting and centralizing the training data also turn out to be chal- lenging. Federated learning (FL) is a distributed model training strategy that guarantees user privacy without the need for data collection. FL enables multiple client devices to collaboratively train their local models without exposing the sensitive data of each client.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "col",
                "lenging",
                "data",
                "Due",
                "domains",
                "lecting",
                "chal",
                "user",
                "training",
                "privacy-sensitive"
            ]
        }
    },
    {
        "id": "9570f1b7-6932-41de-a04c-8642c8ea9dbd",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "The vanilla FL algorithm called federated averag- ing (FedAvg) (McMahan et al., 2017) learns a shared global model by iteratively averaging the local model updates of par- ticipating clients. A successful FL implementation can yield promising results for on-device applications, especially in cer- tain domains where data privacy and confidentiality are crucial.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "ing",
                "FedAvg",
                "averag",
                "McMahan",
                "learns",
                "par",
                "ticipating",
                "clients",
                "model",
                "vanilla"
            ]
        }
    },
    {
        "id": "ada7c830-386f-4a43-808e-1b5f0279b76a",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "Nonetheless, FL introduces several core challenges, including communication bottlenecks and data heterogeneity (Li et al., 2020a), which need to be considered before it can be applied in real-world distributed systems. Moreover, since client devices can only offer a limited amount of computing resources, models intended for those devices should be lightweight and capable of on-device applications.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "Nonetheless",
                "challenges",
                "including",
                "heterogeneity",
                "systems",
                "introduces",
                "core",
                "communication",
                "bottlenecks",
                "data"
            ]
        }
    },
    {
        "id": "b8586365-f297-4e1d-8e3f-ccf6ef01ddaf",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "As a result, it demands an open research question: “Whether a powerful lightweight vision transformer can be efficiently designed for resource-constrained devices?”. In light of this demand, we propose OnDev-LCT: On-Device Lightweight Convolutional Transformers for vision tasks, in- spired by the idea of employing an early convolutional stem in transformers. We introduce image-specific inductive priors to our models by incorporating a convolutional tokenizer be- fore the transformer encoder.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "lightweight",
                "Convolutional",
                "result",
                "question",
                "devices",
                "vision",
                "open",
                "research",
                "powerful",
                "efficiently"
            ]
        }
    },
    {
        "id": "e6e84003-bb57-44cf-9aa0-0b6daf54949f",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "Specifically, we leverage effi- cient depthwise separable convolutions in our LCT tokenizer for extracting local features from images. The multi-head self- attention (MHSA) mechanism in our LCT encoder implicitly enables our models to learn global representations of images. Hence, benefiting from both convolutions and attention mech- anisms, we design our OnDev-LCTs for on-device vision tasks with limited training data and resources.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "LCT",
                "Specifically",
                "images",
                "effi",
                "cient",
                "MHSA",
                "leverage",
                "depthwise",
                "separable",
                "tokenizer"
            ]
        }
    },
    {
        "id": "74648137-ad7c-496e-b276-82b60e423069",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "We analyze the per- formance of our models on various image classification bench- marks in comparison with popular lightweight vision models in the centralized scenario. We further explore our OnDev- LCT models in FL scenarios to empower client devices for vi- sion tasks while coping with data heterogeneity, resource con- straints, and user privacy (Li et al., 2020a).",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "models",
                "formance",
                "bench",
                "marks",
                "analyze",
                "image",
                "classification",
                "comparison",
                "popular",
                "lightweight"
            ]
        }
    },
    {
        "id": "21197fef-18fa-4d53-a4af-57dcf52bdf0e",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "Since data aug- mentations during local training can incur additional computa- tion overhead for resource-constrained FL devices, we compare our models with the baselines in FL scenarios without applying data augmentation techniques. Our empirical analysis indicates that OnDev-LCTs significantly outperform the other baseline models in both centralized and FL scenarios under various data heterogeneity settings.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "aug",
                "mentations",
                "computa",
                "tion",
                "devices",
                "techniques",
                "data",
                "scenarios",
                "models",
                "local"
            ]
        }
    },
    {
        "id": "0f0bda48-ffe5-4c52-b64b-a39d2a7093f7",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "Additionally, we provide detailed dis- cussions based on our observations from various performance analyses to demonstrate the efficiency and effectiveness of our OnDev-LCTs. Figure 1 depicts the image classification per- formance on the CIFAR-10 (Krizhevsky et al., 2009) dataset for different model families with variants of comparable sizes, i.e., #Params < 1.3M (millions) and multiply-accumulate or MACs < 0.1G (billions), in the centralized scenario.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "Additionally",
                "dis",
                "cussions",
                "OnDev-LCTs",
                "Params",
                "provide",
                "detailed",
                "based",
                "observations",
                "performance"
            ]
        }
    },
    {
        "id": "b34f69b6-9a56-4df5-9c36-6ead677acf34",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "Compared to the other baselines, we can observe that our OnDev-LCTs can achieve better accuracy with fewer parameters and lower computational demands. 2. Related Works In this section, we highlight the evolution of prior research efforts in computer vision. Then, we give an overview of how FL can be applied for on-device vision applications. Convolutional neural networks.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "Compared",
                "baselines",
                "demands",
                "Related",
                "observe",
                "OnDev-LCTs",
                "achieve",
                "accuracy",
                "fewer",
                "parameters"
            ]
        }
    },
    {
        "id": "127ee610-7e1d-41c7-83dd-d030874cd253",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "Since AlexNet’s (Krizhevsky et al., 2012) record-breaking achievement on ImageNet (Deng et al., 2009), CNN architectures have gained a great deal of traction in computer vision. As the general trend towards better accuracy is to develop deeper and wider networks (Li et al., 2021c), deep CNNs (He et al., 2016; Simonyan and Zisser- man, 2014; Szegedy et al., 2015) that can extract complex fea- tures from images have become popular for vision applications.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "Krizhevsky",
                "Deng",
                "record-breaking",
                "ImageNet",
                "vision",
                "AlexNet",
                "achievement",
                "architectures",
                "gained",
                "great"
            ]
        }
    },
    {
        "id": "94b6e211-a8d6-42bb-82a7-c758c8e14f02",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "The residual attention networks proposed by Wang et al. (2017) build attention-aware feature maps to provide different repre- sentations of focused image patches as they embed the power of attention mechanisms in CNNs for better classification. How- ever, these improvements in accuracy do not always result in more efficient networks in terms of computation, speed, and 2 size (Khan et al., 2020).",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "Wang",
                "attention",
                "residual",
                "proposed",
                "networks",
                "Khan",
                "speed",
                "size",
                "build",
                "repre"
            ]
        }
    },
    {
        "id": "9517b4e3-364b-4c5c-8c85-c53802eb7fe6",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "Real-world applications, such as med- ical imaging (Varoquaux and Cheplygina, 2022), self-driving cars (Parekh et al., 2022), robotics (Roy et al., 2021; Neuman et al., 2022), and augmented reality (AR) (Mutis and Ambekar, 2020) require edge devices to run vision tasks promptly un- der limited resources.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "Varoquaux",
                "Cheplygina",
                "Parekh",
                "Roy",
                "Neuman",
                "Mutis",
                "Ambekar",
                "robotics",
                "Real-world",
                "applications"
            ]
        }
    },
    {
        "id": "46fddd14-1073-48ce-8789-d8b887787aee",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "In response to these challenges, several lightweight CNNs (Howard et al., 2017; Sandler et al., 2018; Howard et al., 2019; Han et al., 2020; Li et al., 2021b; Hu et al., 2018; Iandola et al., 2016; Park et al., 2020) have been devel- oped as their flexible and simple-to-train nature may easily re- place deep CNN backbones while minimizing the network size and improving latency.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "Howard",
                "Sandler",
                "Han",
                "Iandola",
                "Park",
                "CNNs",
                "CNN",
                "challenges",
                "devel",
                "oped"
            ]
        }
    },
    {
        "id": "d2e9f4d9-7658-42a0-af3a-0fbbde803e38",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "Yet, CNNs have one major drawback: as they are spatially local, it makes them hard to extract com- plex global interactions among the spatial features of images on a broader range (Zuo et al., 2015; LeCun et al., 2015). Transformers in vision. Dosovitskiy et al. (2021) introduced Vision Transformer (ViT) as the first notable example of using a pure transformer backbone for computer vision tasks.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "Zuo",
                "CNNs",
                "drawback",
                "local",
                "plex",
                "range",
                "LeCun",
                "vision",
                "major",
                "spatially"
            ]
        }
    },
    {
        "id": "4b45fb6c-a837-4aa2-94d8-d72ccf295a2e",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "How- ever, lacking some inductive priors makes it difficult for ViT to enhance its generalization ability for out-of-distribution data samples when training data is insufficient. Hence, transform- ers are data-hungry, demanding extremely large pre-training datasets, such as ImageNet-21K or JFT-300M, to compete with their convolutional counterparts (Khan et al., 2021). This has led to an explosion in model and dataset sizes, limiting the use of ViT among practitioners in low-data regimes.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "data",
                "lacking",
                "insufficient",
                "Khan",
                "inductive",
                "priors",
                "makes",
                "difficult",
                "enhance",
                "generalization"
            ]
        }
    },
    {
        "id": "b838d7c3-295a-4e0f-a76d-a3f35bebff52",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "In the con- text of lightweight and efficient vision transformers, parallel lines of research have emerged, exploring different strategies to accomplish high-performance on-device vision tasks.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "con",
                "text",
                "transformers",
                "parallel",
                "emerged",
                "exploring",
                "tasks",
                "vision",
                "lightweight",
                "efficient"
            ]
        }
    },
    {
        "id": "4ecf3c73-9e88-446f-bb48-055ad84f33d1",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "Re- searchers have delved into techniques such as knowledge dis- tillation (Touvron et al., 2021), quantization (Li et al., 2022; Lin et al., 2022), pruning (Zhu et al., 2021; Fang et al., 2023), and architectural modifications (Mehta and Rastegari, 2022; Graham et al., 2021) to enhance the efficiency of transformers on vision tasks.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "Touvron",
                "Lin",
                "Zhu",
                "Fang",
                "Mehta",
                "Rastegari",
                "Graham",
                "tillation",
                "quantization",
                "pruning"
            ]
        }
    },
    {
        "id": "3c159e71-cb1c-4494-ba82-0f7ffbac4e76",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "In an effort to improve upon ViT, the Data- efficient image Transformer (DeiT) (Touvron et al., 2021) in- troduced a transformer-specific distillation strategy that does not require a large quantity of training data. For a given pa- rameter constraint, DeiT still requires huge computational re- sources, with its performance slightly below that of Efficient- Nets (Tan and Le, 2019) with equivalent sizes.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "Data",
                "Transformer",
                "Touvron",
                "DeiT",
                "efficient",
                "ViT",
                "troduced",
                "Nets",
                "effort",
                "improve"
            ]
        }
    },
    {
        "id": "8fa50869-3398-45fe-9b18-ca9810dcbb8c",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "In addition, extensive data augmentation techniques and powerful teacher models are prerequisites for the best performance of DeiT mod- els. Nonetheless, models for real-world vision applications should be lightweight and efficient so that they can be deftly run on resource-constrained devices (Menghani, 2023).",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "els",
                "Menghani",
                "addition",
                "extensive",
                "mod",
                "Nonetheless",
                "models",
                "data",
                "augmentation",
                "techniques"
            ]
        }
    },
    {
        "id": "13f07ab8-fbed-4fdd-994a-1719e0e199be",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "As one of the most powerful model compression approaches, quantization has gained considerable attention due to its po- tential in reducing the computation cost and memory over- head while striving to maintain the performance of vision trans- formers. Quantization involves converting the full-precision or floating-point values (i.e., 32 bits) of weights and activations to integers with lower bit-width, thus accelerating the process- ing of vision transformers in resource-constrained settings.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "quantization",
                "vision",
                "approaches",
                "tential",
                "head",
                "trans",
                "powerful",
                "model",
                "compression",
                "gained"
            ]
        }
    },
    {
        "id": "89cfa554-fb27-46d1-b759-699e53c52130",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "To this point, several works have been studied for adapting vi- sion transformers to two distinct families of quantization meth- ods: quantization-aware training (QAT) (Li et al., 2022; Liu et al., 2023) and post-training quantization (PTQ) (Liu et al., 2021; Lin et al., 2022; Yuan et al., 2022; Ding et al., 2022), depending on the trade-off between achieving better accuracy and simpler deployment.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "Liu",
                "QAT",
                "PTQ",
                "Lin",
                "Yuan",
                "Ding",
                "quantization",
                "ods",
                "point",
                "sion"
            ]
        }
    },
    {
        "id": "b41cc760-9a9e-43b9-ba77-866fd41ea894",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "With an intuitive idea of compress- ing the model without significant performance sacrifice, prun- ing (Blalock et al., 2020; Frankle et al., 2021) has also proved to be highly effective and practical for accelerating vision trans- formers to varying degrees.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "prun",
                "Blalock",
                "Frankle",
                "ing",
                "compress",
                "sacrifice",
                "trans",
                "degrees",
                "intuitive",
                "idea"
            ]
        }
    },
    {
        "id": "21cc263c-1acd-4419-abfd-cd0934e6ff60",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "Researchers have explored var- ious pruning techniques aimed at removing specific compo- nents of vision transformers, such as attention heads or indi- vidual weights, to create more streamlined variants suitable for resource-constrained devices.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "Researchers",
                "var",
                "ious",
                "compo",
                "nents",
                "transformers",
                "indi",
                "vidual",
                "weights",
                "devices"
            ]
        }
    },
    {
        "id": "609988ed-39b6-4ae3-858f-b186ed1c737c",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "For instance, unstructured pruning (Dong et al., 2017; LeCun et al., 1989; Hassibi et al., 1993; Sanh et al., 2020) targets individual weights, providing more flexibility in model compression, but it often requires special hardware accelerators or software for model accelera- tion (Han et al., 2016).",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "Dong",
                "Hassibi",
                "Sanh",
                "Han",
                "tion",
                "model",
                "instance",
                "unstructured",
                "pruning",
                "LeCun"
            ]
        }
    },
    {
        "id": "19e2a9da-7e2c-4498-adc4-281420473b86",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "Conversely, structured pruning (Zhu et al., 2021; Yu and Xiang, 2023; Yu et al., 2022; Yang et al., 2023; Fang et al., 2023) has gained great interest due to its hardware-friendly nature and ability to reduce inference over- head by removing entire components of vision transformers, such as multi-head self-attention layers, token embeddings, or neurons.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "Zhu",
                "Xiang",
                "Yang",
                "Fang",
                "Conversely",
                "structured",
                "pruning",
                "head",
                "transformers",
                "layers"
            ]
        }
    },
    {
        "id": "e44e195b-1de2-4fd2-ba65-ae44148755d5",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "Nonetheless, the choice of the most suitable pruning technique for a particular task depends on several factors in- cluding the target model size, the desired performance level, and the available computational resources, in order to strike a balance between reducing the model size and retaining essential representations and prediction capabilities. Convolutional transformers.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "model",
                "size",
                "Nonetheless",
                "cluding",
                "level",
                "resources",
                "capabilities",
                "choice",
                "suitable",
                "pruning"
            ]
        }
    },
    {
        "id": "664851e5-5e99-4c49-ae06-088e46266c8c",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "In the context of architec- tural modifications, the Convolutional vision Transformer (CvT) (Wu et al., 2021a) introduced convolutions into ViT for improving its performance and efficiency on vision tasks. A convolutional token embedding is added prior to each hierar- chical stage of CvT transformers, along with a convolutional projection layer, which replaces the position-wise projection of multi-head self-attention in the transformer encoders. Hassani et al.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "Convolutional",
                "vision",
                "architec",
                "tural",
                "modifications",
                "introduced",
                "tasks",
                "Transformer",
                "CvT",
                "context"
            ]
        }
    },
    {
        "id": "2df6f775-4c98-41c5-b629-a9ceb387d30e",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "(2021) dispelled the myth of “data-hungry vision trans- formers” by presenting ViT-Lite with smaller patch sizing for patch-based image tokenization of the original ViT. The same authors introduced a sequential pooling method (SeqPool) in Compact Vision Transformer (CVT) (Hassani et al., 2021) to eliminate the need for a classification token from vision trans- formers.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "trans",
                "vision",
                "dispelled",
                "data-hungry",
                "ViT",
                "CVT",
                "myth",
                "presenting",
                "ViT-Lite",
                "smaller"
            ]
        }
    },
    {
        "id": "3fe3d2bc-1d85-4f8b-874a-3a4c7849be37",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "Coupling the strengths of CNNs and ViT, Compact Convolutional Transformer (CCT) (Hassani et al., 2021) and MobileViT (Mehta and Rastegari, 2022) have been proposed as lightweight hybrid models to improve the flexibility of trans- formers in vision tasks. CCT replaced the patch-based tok- enization of ViT with convolutional tokenization that preserves the local spatial relationships between convolutional patches.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "CCT",
                "Compact",
                "Transformer",
                "Hassani",
                "Mehta",
                "Rastegari",
                "Convolutional",
                "Coupling",
                "MobileViT",
                "trans"
            ]
        }
    },
    {
        "id": "4275ffa6-f37d-47d0-af62-8e80cbb691ad",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "MobileViT introduced a lightweight ViT model by replacing the local processing in convolutions with transformers to learn global representations of images. Thus, in addition to the as- pect of ViT in capturing long-range feature dependencies pro- vided by the attention mechanism, convolution and pooling op- erations of CNNs come up with the translation equivariance and 3 4 3 3 2 1 1 Client 1 Server Client k 1. Server distributes a global model. 2. Client trains a local model. 3.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "Client",
                "Server",
                "MobileViT",
                "images",
                "ViT",
                "model",
                "introduced",
                "lightweight",
                "replacing",
                "processing"
            ]
        }
    },
    {
        "id": "fd5c01a4-2849-44bc-bb2d-0a97abe591dd",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "Client sends model updates back to server. 4. Server aggregates model updates into a new global model. 2 Global model On-Device Local model On-Device Local model Figure 2: The vanilla federated averaging (FedAvg) framework. A typical four-step procedure is iterated until convergence. locality, allowing these hybrid models to work better on smaller datasets. To this end, these models desire high-level data aug- mentation techniques to justify their optimal performance in vi- sion tasks.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "model",
                "server",
                "Local",
                "updates",
                "global",
                "Client",
                "sends",
                "back",
                "models",
                "On-Device"
            ]
        }
    },
    {
        "id": "6f2055e6-10b1-4cb4-ab50-718c1b78d76e",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "Likewise, large computational resources are still es- sential for them to beat mobile-friendly CNN models. Recently, Graham et al. (2021) proposed a hybrid neural net- work named LeViT for faster inference of image classification by designing a multi-stage transformer in ConvNet’s clothing. Convolutional blocks of LeViT are built as pyramids for de- creasing the resolution of feature maps with an increasing num- ber of channels.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "CNN",
                "Likewise",
                "large",
                "sential",
                "models",
                "computational",
                "resources",
                "beat",
                "mobile-friendly",
                "Graham"
            ]
        }
    },
    {
        "id": "eba06e71-15cd-4d36-9efe-64ee9a4c4a31",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "The authors replaced the ViT’s positional embedding with a learned, per-head translation-invariant at- tention bias and redesigned attention-MLP blocks to improve the network capacity. LeViT models are trained following the DeiT’s distillation-driven training with two classification heads; thus, they require a powerful pre-trained teacher model, i.e., RegNetY-16GF, and strong computational resources, i.e., 32 GPUs, to perform 1000 training epochs in 3 to 5 days.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "learned",
                "per-head",
                "tention",
                "capacity",
                "authors",
                "replaced",
                "ViT",
                "positional",
                "embedding",
                "translation-invariant"
            ]
        }
    },
    {
        "id": "2856e335-b29b-4bf1-a1ed-a2bc22555ffb",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "Mobile-Former proposed by Chen et al. (2022b) parallelizes MobileNet (Howard et al., 2019) and transformer with a bidi- rectional cross-attention bridge in between. In contrast to the computationally efficient parallel design, Mobile-Former intro- duces additional parameters due to its parameter-heavy classi- fication head.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "Chen",
                "Mobile-Former",
                "Howard",
                "proposed",
                "parallelizes",
                "MobileNet",
                "bidi",
                "rectional",
                "design",
                "intro"
            ]
        }
    },
    {
        "id": "ed49c39f-f126-44ee-b896-73ffec642182",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "The EdgeNeXt (Maaz et al., 2022), with its in- novative split depthwise transpose attention (SDTA) encoder, demonstrates remarkable potential for mobile devices through its stage-wise design of hybrid architecture. EdgeNeXt variants surpass previous hybrid models in both accuracy and model size while significantly reducing the computational overhead.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "SDTA",
                "encoder",
                "Maaz",
                "novative",
                "attention",
                "demonstrates",
                "architecture",
                "EdgeNeXt",
                "hybrid",
                "split"
            ]
        }
    },
    {
        "id": "2a2f4e10-9af2-4909-94d3-e4f39a9f8398",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "Mehta and Rastegari (2023) proposed MobileViTv2, an im- proved version of the original MobileViT, to alleviate the main efficiency bottleneck caused by the multi-headed self-attention. The authors introduced a separable self-attention method with linear complexity, making MobileViTv2 suitable for deploy- ment on resource-constrained devices.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "Rastegari",
                "proposed",
                "Mehta",
                "proved",
                "MobileViT",
                "self-attention",
                "version",
                "original",
                "alleviate",
                "main"
            ]
        }
    },
    {
        "id": "9668c4a4-b6d4-4190-b960-b80864e75f1b",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "Jeevan and Sethi (2022) adapted linear attention mechanisms to Vision Transformer (ViT) (Dosovitskiy et al., 2021), resulting in the creation of Vision X-formers (ViXs), where X ∈{Performer (P), Lin- former (L), Nystr¨omformer (N)} (Choromanski et al., 2020; Wang et al., 2020c; Xiong et al., 2021). This adaptation led to a remarkable reduction of up to seven times in GPU memory requirements.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "Vision",
                "Performer",
                "Lin",
                "Nystr",
                "Sethi",
                "Transformer",
                "Dosovitskiy",
                "X-formers",
                "Choromanski",
                "Wang"
            ]
        }
    },
    {
        "id": "b0de6fd8-aa42-4591-b9d5-532a5802d0a4",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "Additionally, the authors introduced inductive priors by replacing the initial linear embedding layer with con- volutional layers in ViXs, resulting in a substantial improve- ment in classification accuracy without increasing the model size. Drawing our inspiration from the key concepts presented in those prior works, we aim to develop high-performing, lightweight convolutional transformers for vision applications on devices with limited resources in low-data regimes. Federated learning.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "Additionally",
                "con",
                "volutional",
                "ViXs",
                "resulting",
                "improve",
                "ment",
                "size",
                "layer",
                "layers"
            ]
        }
    },
    {
        "id": "75de5ba8-228b-415f-a9b8-e57e62d4f621",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "Federated averaging (FedAvg) (McMahan et al., 2017) is the earliest and most widely used FL baseline that executes an iterative model averaging process via a typ- ical four-step procedure, as illustrated in Figure 2. However, FedAvg suffers from performance deterioration when dealing with distribution shifts in training data across multiple clients.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "Figure",
                "FedAvg",
                "Federated",
                "McMahan",
                "typ",
                "ical",
                "procedure",
                "averaging",
                "earliest",
                "widely"
            ]
        }
    },
    {
        "id": "43a11a34-c74d-44b2-81a4-6d2a7e857df1",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "Various works have been proposed in an attempt to enhance the performance of FedAvg for tackling the data heterogeneity by focusing on either two aspects: improvements in local training, i.e., Step 2 of Figure 2 (Li et al., 2020b; Karimireddy et al., 2020; Li et al., 2021a) or improvements in server aggregation, i.e., Step 4 of Figure 2 (Wang et al., 2020a; Hsu et al., 2019; Wang et al., 2020b).",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "Step",
                "Figure",
                "Wang",
                "improvements",
                "Karimireddy",
                "Hsu",
                "aspects",
                "training",
                "aggregation",
                "works"
            ]
        }
    },
    {
        "id": "462311f1-b641-4b21-a5de-3ca74d5a0a06",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "A recent study on ViT-FL (Qu et al., 2022) presents a new perspective on utilizing ViT in federated vision models. Their empirical analysis demonstrates that “transform- ers are highly robust to distribution shifts in data”. However, the performance of ViT-FL heavily relies on the pre-training dataset, or ViT requires a huge amount of training data to out- perform CNNs when trained from scratch.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "presents",
                "models",
                "recent",
                "study",
                "perspective",
                "utilizing",
                "federated",
                "vision",
                "ViT-FL",
                "ViT"
            ]
        }
    },
    {
        "id": "947b5653-b219-4bfd-b919-00f69f6eb32f",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "Often, it is hard to readily obtain a well-pre-trained ViT model for various domain- specific tasks in a practical distributed system. Moreover, the unprecedented size of ViT (e.g., ≈6M for ViT-Tiny or ≈22M for ViT-S (Qu et al., 2022; Touvron et al., 2022)) may intro- duce hurdles to communication bottlenecks when exchanging the model updates for aggregation.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "domain",
                "specific",
                "system",
                "ViT",
                "model",
                "hard",
                "readily",
                "obtain",
                "tasks",
                "practical"
            ]
        }
    },
    {
        "id": "c47e277f-ffb3-40a8-839c-25b71dd0423f",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "Hence, in this study, we aim to tackle those challenges by deploying our OnDev-LCTs in FL scenarios under various data heterogeneity settings. 3. Preliminaries In this section, we describe the preliminaries and motivations behind the development of our OnDev-LCT models. Patch-based tokenization.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "study",
                "settings",
                "Preliminaries",
                "aim",
                "tackle",
                "challenges",
                "deploying",
                "scenarios",
                "data",
                "heterogeneity"
            ]
        }
    },
    {
        "id": "a484277b-abe6-4815-a4e9-d18cbae8922c",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "In the standard ViT model (Doso- vitskiy et al., 2021), a 2D input image x is split into multiple non-overlapping fixed-size patches: x ∈RH×W×C ⇒xp ∈RN×(P2C), where C is the number of channels, N = HW P2 is the number of patches with (H, W) and (P, P) for the dimensions of original image and patches, respectively. Image patches are flattened into a sequence of 1D vectors (aka tokens), which are then lin- early projected into D-dimensional space: [x1 pE; x2 pE; . . .",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "Doso",
                "number",
                "patches",
                "image",
                "model",
                "vitskiy",
                "input",
                "channels",
                "standard",
                "ViT"
            ]
        }
    },
    {
        "id": "d06ebe75-9d71-434b-a020-13e53d7f5ed3",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "; xN p E], where E ∈R(P2C)×D is a projection vector. For the purpose 4 ReLU V K Q BN ReLU Linear Classifier Class Cat Dog ...",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "vector",
                "Dog",
                "projection",
                "Linear",
                "Classifier",
                "Class",
                "Cat",
                "ReLU",
                "purpose"
            ]
        }
    },
    {
        "id": "460cc296-7b9c-4a4a-9d02-990de30bba9f",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "Linear Depthwise Separable Convolution LayerNorm Multi-Head Self-Attention LayerNorm Dense Dropout MLP block x 2 stride = 2 Linear Bottleneck Residual Linear Bottleneck LCT Encoder Input Image Reshaped Tokenized Patches stride = 1 Linear Bottleneck 3 x 3 Conv2D LCT Tokenizer Reshaping LCT Encoder Sequence Pooling BN BN Linear Depthwise Separable Convolution stride = s 3 x 3 Dwise Conv2D BN ReLU x L M x Expand 1 x 1 Pwise Conv2D Shrink 1 x 1 Pwise Conv2D x R Figure 3: An overview of On-Device Lightweight Convolutional Transformer (OnDev-LCT).",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "Linear",
                "Pwise",
                "Bottleneck",
                "LCT",
                "Depthwise",
                "Separable",
                "Convolution",
                "Encoder",
                "stride",
                "Dwise"
            ]
        }
    },
    {
        "id": "070998e4-39a4-49d3-9cf7-aa7a294b5768",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "The middle row depicts the architecture of an LCT tokenizer, visualizing its core components in the bottom row. The structure of an LCT encoder is illustrated on the right. of classification, an extra learnable xclass token is prepended to the patch embeddings. Since this ViT’s patch-based tokeniza- tion step ignores spatial inductive biases, information along the boundary regions of the input image is lost (Khan et al., 2021).",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "LCT",
                "row",
                "tokenizer",
                "visualizing",
                "middle",
                "depicts",
                "architecture",
                "core",
                "components",
                "bottom"
            ]
        }
    },
    {
        "id": "e5e501f9-8025-4886-9338-1861eb7e976e",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "Thus, a positional vector Epos ∈R(N+1)×D is encoded to the se- quence, resulting in an input embedding z0 to the transformer encoder as: z0 = [xclass; x1 pE; x2 pE; . . . ; xN p E] + Epos. Although this positional encoding makes it easier to learn com- plex features through visual representations, ViT still requires a large scale of training data to generalize well. Convolutional tokenization.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "Epos",
                "quence",
                "xclass",
                "resulting",
                "embedding",
                "vector",
                "encoded",
                "input",
                "transformer",
                "encoder"
            ]
        }
    },
    {
        "id": "3ab74668-5e76-4a25-9ccb-965330b802b6",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "A convolutional tokenization block introduced by CCT (Hassani et al., 2021) consists of a single standard convolution with a ReLU (Agarap, 2018) acti- vation followed by a MaxPool operation. An input image x is tokenized into convolutional patches and reshaped into an input embedding z0 to the transformer encoder as: z0 = MaxPool(ReLU(Conv2D(x))). This convolutional tokenization step makes CCT flexible enough to remove the positional encoding as well by injecting inductive biases into the model.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "Agarap",
                "Hassani",
                "acti",
                "ReLU",
                "CCT",
                "convolutional",
                "consists",
                "vation",
                "operation",
                "MaxPool"
            ]
        }
    },
    {
        "id": "88245cc9-61dd-4073-b5d6-1749288ee5b0",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "However, as the MaxPool op- eration reduces the spatial resolution of the input image, all the relevant information cannot be preserved well (Sabour et al., 2017; Xinyi and Chen, 2019). Moreover, having multiple con- volutional tokenization blocks may speed up the loss of all spa- tial information, which in turn hurts the model performance. Transformer encoder. In the original ViT (Dosovitskiy et al., 2021), each encoder, i.e., l = 1, 2, . . .",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "Sabour",
                "Xinyi",
                "Chen",
                "information",
                "eration",
                "image",
                "MaxPool",
                "reduces",
                "spatial",
                "resolution"
            ]
        }
    },
    {
        "id": "22a7f1ac-d2d1-47f2-add3-aad841220526",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": ", L, is stacked on top of one another, sending its output zl to the subsequent encoder. Each encoder contains two main components, such as a multi- head self-attention (MHSA) block and a 2-layer multi-layer perceptron (MLP) block, with a LayerNorm (LN) and residual connections in between. The last encoder outputs a sequence zL of image representations. Sequence pooling.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "sending",
                "encoder",
                "MHSA",
                "MLP",
                "block",
                "stacked",
                "top",
                "subsequent",
                "sequence",
                "components"
            ]
        }
    },
    {
        "id": "896892f3-168d-41fa-8db2-18e3ee9b0377",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "An attention-based sequence pooling (Se- qPool) in CVT (Hassani et al., 2021) maps the last encoder’s output sequence to the classifier by pooling the relevant infor- mation across different parts of the sequence and eliminates the need for an extra learnable class token. 4. OnDev-LCT Architecture In this section, we present the architecture of our OnDev- LCT by explaining key components in detail. It includes three main parts: LCT tokenizer, LCT encoder, and a classifier.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "sequence",
                "LCT",
                "CVT",
                "Hassani",
                "pooling",
                "qPool",
                "maps",
                "infor",
                "mation",
                "token"
            ]
        }
    },
    {
        "id": "7285d0ff-0688-42ac-bab2-21beeff9e24c",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "An overview of our OnDev-LCT is illustrated in Figure 3. LCT tokenizer. Inspired by the idea of leveraging a convolu- tional stem for image tokenization, we introduce spatial induc- tive priors into our models by replacing the entire patch-based tokenization of ViT (Dosovitskiy et al., 2021) with an LCT to- kenizer. Given an input image x, our LCT tokenizer encodes it into convolutional patches before forwarding it to the LCT encoder.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "Figure",
                "LCT",
                "overview",
                "OnDev-LCT",
                "illustrated",
                "tokenizer",
                "Dosovitskiy",
                "image",
                "tokenization",
                "kenizer"
            ]
        }
    },
    {
        "id": "621be9cb-ac95-4778-abc3-99e4f589034a",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "Our tokenizer starts with M convolution operations: xm = ReLU(BN(Conv2D(x))), m = 1, 2, . . . , M (1) where Conv2D represents a standard 3×3 convolution followed by a BatchNorm (BN) with ReLU (Agarap, 2018) activation.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "operations",
                "ReLU",
                "Agarap",
                "tokenizer",
                "starts",
                "convolution",
                "activation",
                "represents",
                "standard",
                "BatchNorm"
            ]
        }
    },
    {
        "id": "0d71426f-6aee-4f1a-abed-7878b570f640",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "After the final convolution operation, the output xM is sent to a block of Linear Depthwise Separable Convolution: x′ 0 = LinearDWS(xM), (2) which is composed of a layer of 3 × 3 depthwise convolution followed by a layer of 1 × 1 pointwise convolution: x′ 0,dw = ReLU(BN(DepthwiseConv2D(xM))), x′ 0 = Linear(BN(PointwiseConv2D(x′ 0,dw))).",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "Linear",
                "convolution",
                "layer",
                "Depthwise",
                "LinearDWS",
                "ReLU",
                "Separable",
                "operation",
                "pointwise",
                "final"
            ]
        }
    },
    {
        "id": "ad456e74-caf7-4a27-864d-c1054452f73c",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "(3) 5 DepthwiseConv2D extracts features through spatial filtering per input channel, while PointwiseConv2D is used to project chan- nels output from the depthwise layer by a linear combination of those channels. Each convolution operation is followed by BatchNorm with a ReLU for depthwise convolution and a Lin- ear activation for pointwise convolution. The output feature map is forwarded through R Residual Linear Bottleneck blocks: x′ r = LinearBottleneck(x′ r−1) + x′ r−1, r = 1, 2, . . .",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "channel",
                "channels",
                "linear",
                "extracts",
                "chan",
                "nels",
                "convolution",
                "depthwise",
                "spatial",
                "filtering"
            ]
        }
    },
    {
        "id": "b8816be7-8307-4808-baf5-7bf6604f8f59",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": ", R (4) Each Bottleneck block first shrinks the feature map via a 1 × 1 convolution. DWS convolution is then applied to remap the output to the same dimension as the input feature map: x′ r,shrink = ReLU(BN(PointwiseConv2D(x′ r−1))), x′ r = LinearDWS(x′ r,shrink). (5) Here, instead of using several MaxPool operations, we im- plement DWS convolutions at the heart of our OnDev-LCT to maintain the spatial information of the input image while build- ing up better representations.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "Bottleneck",
                "DWS",
                "feature",
                "map",
                "r,shrink",
                "convolution",
                "block",
                "shrinks",
                "input",
                "ReLU"
            ]
        }
    },
    {
        "id": "7cd7e1ef-b968-47d7-9c24-2ff8e0d63ff1",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "The amount of computations can also be drastically reduced as DWS breaks the standard convo- lution operation, i.e., channel-wise and spatial-wise computa- tions, into two separate layers. Moreover, the Bottleneck struc- ture reduces the number of parameters and computation costs while preventing information loss of using non-linear functions, such as ReLU (Agarap, 2018)).",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "tions",
                "DWS",
                "convo",
                "lution",
                "operation",
                "channel-wise",
                "computa",
                "layers",
                "Agarap",
                "amount"
            ]
        }
    },
    {
        "id": "874a9814-a499-4303-8480-8bf7fae7e95f",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "The idea of residual blocks is to make our model as thin as possible by increasing the depth with fewer parameters while improving the ability to effectively train a lightweight vision transformer. At the end of the LCT tokenization step, encoded patches are reshaped to be processed through L blocks of LCT encoder: x′ R ∈RHR×WR×CR ⇒z0 ∈RHW×D, (6) where CR = D is the dimension of the input embedding z0. ViT-based LCT encoder.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "LCT",
                "encoder",
                "blocks",
                "transformer",
                "idea",
                "residual",
                "make",
                "model",
                "thin",
                "increasing"
            ]
        }
    },
    {
        "id": "271d99ca-aaf5-466e-81b9-e895372cc6db",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "We construct our LCT encoder by following the encoder design of the original ViT (Dosovitskiy et al., 2021), which comprises MHSA and MLP blocks: z′ l = MHSA(LN(zl−1)) + zl−1, l = 1, 2, . . . , L (7) zl = MLP(LN(z′ l)) + z′ l, l = 1, 2, . . . , L (8) The MHSA block in the encoder receives the input embedding z0 in the form of three parameters Query (Q), Key (K), and Value (V).",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "Dosovitskiy",
                "LCT",
                "MHSA",
                "MLP",
                "encoder",
                "ViT",
                "construct",
                "design",
                "original",
                "comprises"
            ]
        }
    },
    {
        "id": "64ccac56-37df-4297-bd7f-940e9665368e",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "Self-attention (SA) begins by computing the dot products of the Query with all Keys and applying a softmax function to obtain the attention weights on the Values. Multi- head enables the multiple calculations of self-attention in par- allel with h independent attention heads: MHSA(Q, K, V) = Concat(SA1, SA2, . . . , SAh).",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "Query",
                "Keys",
                "begins",
                "Self-attention",
                "attention",
                "MHSA",
                "Concat",
                "computing",
                "dot",
                "products"
            ]
        }
    },
    {
        "id": "71a082bc-e38c-4891-ac5f-140fdd95601a",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "(9) With the multi-head self-attention (MHSA), our model can capture long-range positional information as each head can pay attention to different parts of the input embedding and capture different contextual information by calculating the associations between image patches in a unique way. The MLP block con- tains two layers with GELU (Hendrycks and Gimpel, 2016) ac- tivation and dropouts.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "MHSA",
                "capture",
                "information",
                "self-attention",
                "multi-head",
                "model",
                "long-range",
                "positional",
                "head",
                "pay"
            ]
        }
    },
    {
        "id": "f4771ad1-5a9f-4086-86eb-0a8dad913491",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "LayerNorm (LN) is applied before each component of the encoder to prevent its values from chang- ing abruptly, improving the training performance. A residual connection is also applied to route the information between the components. Then, the final output sequence zL is forwarded to the sequence pooling (SeqPool) layer. Classifier.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "LayerNorm",
                "chang",
                "ing",
                "abruptly",
                "improving",
                "performance",
                "applied",
                "encoder",
                "prevent",
                "training"
            ]
        }
    },
    {
        "id": "5b59f7fc-acfc-4ae6-9a90-57b908544fad",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "We apply SeqPool (Hassani et al., 2021) over the output sequence zL to eliminate the use of learnable class token: z = σ(FC(LN(zL))) × zL, (10) where LayerNorm (LN) is applied before passing the sequence to the fully connected (FC) layer, then attention weights are added by a softmax function σ.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "Hassani",
                "layer",
                "sequence",
                "SeqPool",
                "token",
                "LayerNorm",
                "connected",
                "apply",
                "output",
                "eliminate"
            ]
        }
    },
    {
        "id": "8b2dbbc0-1d57-40c5-a383-f6c0cc976a47",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "As the output sequence con- tains relevant information across different parts of the input embedding, preserving this information via attention weights can improve the model performance with no additional param- eters compared to the class token and thus significantly reduce the computation overhead. Eventually, the output z is sent to a single-layer classifier: y = FC(z), (11) where y is the final classification output of our OnDev-LCT. 5.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "information",
                "output",
                "con",
                "tains",
                "embedding",
                "preserving",
                "param",
                "eters",
                "overhead",
                "sequence"
            ]
        }
    },
    {
        "id": "40043216-5744-4fc8-af42-0ae4047e478d",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "Experiments In this section, we analyze the performance of our OnDev- LCTs over existing lightweight vision models by conducting extensive experiments on multiple image datasets in both cen- tralized and federated learning scenarios. Datasets. We conduct extensive experiments on six image classification datasets: CIFAR-10, CIFAR-100 (Krizhevsky et al., 2009), MNIST (Deng, 2012), Fashion-MNIST (Xiao et al., 2017), EMNIST-Balanced (Cohen et al., 2017), and FEMNIST (Caldas et al., 2018).",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "Experiments",
                "section",
                "OnDev",
                "LCTs",
                "cen",
                "tralized",
                "scenarios",
                "datasets",
                "extensive",
                "analyze"
            ]
        }
    },
    {
        "id": "abdb2701-bada-437c-b01b-4832f57b968f",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "Since our study focuses on edge devices with limited resources for tackling vision tasks in low-data regimes, we do not use additional pre-training data for all experiments. CIFAR-10 and CIFAR-100 (Krizhevsky et al., 2009) datasets consist of 50,000 training samples and 10,000 test samples of 32 × 32 RGB images in 10 and 100 classes, respectively.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "regimes",
                "experiments",
                "study",
                "focuses",
                "edge",
                "devices",
                "limited",
                "resources",
                "tackling",
                "vision"
            ]
        }
    },
    {
        "id": "f23b3b8c-53c8-4a62-bbc1-a8e12ca4afba",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "MNIST (Deng, 2012) and Fashion- MNIST (Xiao et al., 2017) datasets contain 60,000 training samples and 10,000 test samples of 28 × 28 single-channel im- ages in 10 classes. EMNIST-Balanced (Cohen et al., 2017) contains 112,800 training samples and 18,800 test samples of 28 × 28 single-channel handwritten character images in 47 bal- anced classes.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "MNIST",
                "Deng",
                "samples",
                "Fashion",
                "Xiao",
                "training",
                "test",
                "single-channel",
                "classes",
                "datasets"
            ]
        }
    },
    {
        "id": "a335c075-a3a6-4d3e-81db-86413f279cb1",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "We use the original training and test sets of each dataset for the centralized scenario while dividing the training sets into 10 and 50 client data partitions for the FL scenarios. To control the level of data heterogeneity for each partition, we adjust the concentration parameter β of the Dirichlet dis- tribution (Ferguson, 1973) from {0.1, 0.5, 5}, where smaller β corresponds to a higher degree of data heterogeneity.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "data",
                "training",
                "sets",
                "client",
                "scenario",
                "scenarios",
                "original",
                "test",
                "dataset",
                "centralized"
            ]
        }
    },
    {
        "id": "d0585d7b-7573-4e06-aa41-47aac1987981",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "Using 6 0 2 4 6 8 10 Client ID 0 2 4 6 8 10 Class ID 0 1000 2000 3000 4000 (a) For 10 clients (β = 0.1) 0 2 4 6 8 10 Client ID 0 2 4 6 8 10 Class ID 0 500 1000 1500 2000 2500 (b) For 10 clients (β = 0.5) 0 2 4 6 8 10 Client ID 0 2 4 6 8 10 Class ID 200 400 600 800 1000 1200 (c) For 10 clients (β = 5) 0 10 20 30 40 50 Client ID 0 2 4 6 8 10 Class ID 0 500 1000 1500 2000 (d) For 50 clients (β = 0.1) 0 10 20 30 40 50 Client ID 0 2 4 6 8 10 Class ID 0 100 200 300 400 500 600 700 (e) For 50 clients (β = 0.5) 0 10 20 30 40 50 Client ID 0 2 4 6 8 10 Class ID 50 100 150 200 250 (f) For 50 clients (β = 5) Figure 4: Detailed data partitions on CIFAR-10 dataset using Dirichlet distribution.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "Client",
                "clients",
                "Class",
                "Figure",
                "Detailed",
                "Dirichlet",
                "dataset",
                "distribution",
                "data",
                "partitions"
            ]
        }
    },
    {
        "id": "ed80d99c-5309-4372-ab59-9cec1f8d2865",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "Smaller β corresponds to higher data heterogeneity. Best viewed in color.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "Smaller",
                "heterogeneity",
                "corresponds",
                "higher",
                "data",
                "color",
                "viewed"
            ]
        }
    },
    {
        "id": "9d4ef9ea-3223-47f9-9e12-537d6464512d",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "Model #Trans #Convs Patch Pos Emb SeqPool #Params ViT-Lite-1/16 1 ✗ 16 × 16 ✓ ✗ 0.25M ViT-Lite-2/16 2 ✗ 16 × 16 ✓ ✗ 0.27M ViT-Lite-1/8 1 ✗ 8 × 8 ✓ ✗ 0.42M ViT-Lite-2/8 2 ✗ 8 × 8 ✓ ✗ 0.45M ViT-Lite-1/4 1 ✗ 4 × 4 ✓ ✗ 1.21M ViT-Lite-2/4 2 ✗ 4 × 4 ✓ ✗ 1.23M CCT-2/2 2 2 ✗ ✓ ✓ 0.28M CCT-4/2 4 2 ✗ ✓ ✓ 0.48M OnDev-LCT-1/1 1 1 ✗ ✗ ✓ 0.21M OnDev-LCT-2/1 2 1 ✗ ✗ ✓ 0.31M OnDev-LCT-4/1 4 1 ✗ ✗ ✓ 0.51M OnDev-LCT-8/1 8 1 ✗ ✗ ✓ 0.91M OnDev-LCT-1/3 1 3 ✗ ✗ ✓ 0.25M OnDev-LCT-2/3 2 3 ✗ ✗ ✓ 0.35M OnDev-LCT-4/3 4 3 ✗ ✗ ✓ 0.55M OnDev-LCT-8/3 8 3 ✗ ✗ ✓ 0.95M Table 1: Detailed architecture of transformer-based model variants.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "Model",
                "Trans",
                "Params",
                "Table",
                "Convs",
                "Detailed",
                "Patch",
                "Pos",
                "Emb",
                "SeqPool"
            ]
        }
    },
    {
        "id": "a64e4981-012c-4f11-b3bb-68793c2b28c9",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "We follow the implementation of Hassani et al. (2021) for ViT-Lite variants. this partitioning strategy, each FL client may have relatively few or no data samples in certain classes. Figure 4 visualizes the data distribution of 10 and 50 FL clients for the CIFAR- 10 dataset. Additionally, we evaluate the effectiveness of our OnDev-LCTs in realistic non-IID settings by using the Fed- erated Extended MNIST (FEMNIST) dataset provided by the LEAF benchmark (Caldas et al., 2018).",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "Hassani",
                "follow",
                "implementation",
                "dataset",
                "data",
                "FEMNIST",
                "CIFAR",
                "Fed",
                "MNIST",
                "Caldas"
            ]
        }
    },
    {
        "id": "874b8463-1cf2-48bb-acfe-dbd0c23d2a3c",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "The FEMNIST dataset is commonly used in the FL literature, as it is designed to repli- cate practical FL scenarios for multi-class (i.e., 62 classes) im- age classification by naturally partitioning the total of 805,263 data samples in Extended MNIST (EMNIST) (Cohen et al., 2017) dataset into 3,550 clients based on the writer of the digit or character.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "EMNIST",
                "MNIST",
                "Cohen",
                "classes",
                "dataset",
                "FEMNIST",
                "Extended",
                "literature",
                "repli",
                "cate"
            ]
        }
    },
    {
        "id": "6f91be73-4a38-4ef6-b326-ea1d737dead1",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "To align with prior FL works (Caldas et al., 2018; Wu et al., 2021b; Chen et al., 2022a; Shi et al., 2022), we sub- sample 5% of the total data (approximately 180 clients) and split it into 80% for training and 20% for testing per client. Implementation details. We implement our experiments using the TensorFlow (Abadi et al., 2015) framework. All experi- ments are conducted on a single NVIDIA RTX 3080 GPU with 10 GB memory.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "Caldas",
                "Chen",
                "Shi",
                "sample",
                "approximately",
                "clients",
                "client",
                "works",
                "data",
                "align"
            ]
        }
    },
    {
        "id": "5975aed8-6780-44c2-bf68-2768ecb49438",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "We consider two client sampling strategies for experiments in FL scenarios (except for the FEMNIST dataset): (1) we sample 10 clients where all the clients participate in ev- ery FL round, (2) we sample 50 clients and randomly choose 10 clients to participate in each FL round.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "sample",
                "round",
                "clients",
                "participate",
                "FEMNIST",
                "scenarios",
                "dataset",
                "ery",
                "choose",
                "sampling"
            ]
        }
    },
    {
        "id": "0993aa41-c829-4a95-982b-4166a097cb37",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "For CIFAR-10 and CIFAR-100 (Krizhevsky et al., 2009), we train each model for 200 epochs in centralized experiments and conduct 60 commu- nication rounds for the FL scenario with 10 clients and 100 rounds for the FL scenario with 50 clients, with each client training for 5 local epochs.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "scenario",
                "Krizhevsky",
                "rounds",
                "commu",
                "epochs",
                "clients",
                "conduct",
                "nication",
                "local",
                "train"
            ]
        }
    },
    {
        "id": "d9134a9e-d4aa-4a1d-873e-ba9012c263d8",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "For other datasets, except for FEM- NIST, we train each model for 50 epochs in centralized exper- iments and conduct 30 communication rounds for the FL sce- Model Family CIFAR 10 CIFAR 100 MNIST Fashion MNIST EMNIST Balanced FEMNIST Optimizer Type ResNet 5e−3 5e−3 5e−3 5e−3 5e−3 1e−3 Adam MobileNetv2 5e−3 5e−3 3e−3 5e−3 5e−3 3e−3 Adam ViT-Lite 1e−3 1e−3 1e−3 1e−3 1e−3 1e−3 AdamW CCT 1e−3 1e−3 1e−3 1e−3 1e−3 1e−3 AdamW OnDev-LCT 3e−3 5e−3 1e−3 3e−3 3e−3 1e−3 Adam Table 2: Learning rate values and optimizer types for each model family.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "Adam",
                "model",
                "CIFAR",
                "MNIST",
                "Family",
                "Optimizer",
                "AdamW",
                "NIST",
                "FEM",
                "CCT"
            ]
        }
    },
    {
        "id": "c15374ec-b9e2-408f-9b1e-4a0f17b3d2cd",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "Models with AdamW use the default values of β1 = 0.9, β2 = 0.999, and weight decay = 1e−4. nario with 10 clients and 60 rounds for the FL scenario with 50 clients, with each client training for 3 local epochs. For FL ex- periments with the FEMNIST dataset, we conduct 120 commu- nication rounds, with each client training for 3 local epochs per round.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "local",
                "Models",
                "decay",
                "training",
                "clients",
                "client",
                "AdamW",
                "default",
                "weight",
                "epochs"
            ]
        }
    },
    {
        "id": "88714ab7-c4aa-418a-bf02-1cf5e6066bfc",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "During the communication rounds, we randomly select C clients to participate in training, considering three different settings of C ∈{5, 10, 2 ∼10}, which simulate common sce- narios where clients may be offline or have limited connectivity to participate in every round. The setting 2 ∼10 represents the number of clients selected in each round, varying from 2 clients to 10 clients. We conduct 5 separate runs for each cen- tralized experiment and report the best accuracy values.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "participate",
                "clients",
                "round",
                "training",
                "sce",
                "narios",
                "communication",
                "randomly",
                "select",
                "simulate"
            ]
        }
    },
    {
        "id": "0ed82ba2-b8ec-4e54-b3bc-bf754b36ed87",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "For FL, reported values are the best median values from the last 5 FL rounds of 3 separate runs. Model variants and baselines. We explore several variants of OnDev-LCT by tuning the number of LCT encoders and the number of standard convolutions in the LCT tokenizer. For instance, “OnDev-LCT-2/3” specifies a variant with 2 LCT encoders and 3 standard convolutions. We compare the per- formance of our OnDev-LCTs with existing lightweight vi- sion models as baselines.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "LCT",
                "reported",
                "separate",
                "runs",
                "median",
                "rounds",
                "baselines",
                "encoders",
                "number",
                "standard"
            ]
        }
    },
    {
        "id": "5abb5ee1-3c78-462a-a4d6-9bf6bfa5896c",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "For pure CNN baselines, we imple- ment small-sized variants of ResNet (He et al., 2016) and Mo- bileNetv2 (Sandler et al., 2018), which are specially designed for CIFAR, denoted by “ResNet-r” where r is the number of layers and “MobileNetv2/α” where α is the width multiplier, respectively.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "ResNet-r",
                "Sandler",
                "CIFAR",
                "CNN",
                "baselines",
                "imple",
                "ment",
                "ResNet",
                "denoted",
                "multiplier"
            ]
        }
    },
    {
        "id": "0c5ea5b6-62c0-4718-8beb-01bfe4c5a93d",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "Following (Hassani et al., 2021), we design sev- eral “ViT-Lite” model variants with pure backbones of the vi- sion transformer (Dosovitskiy et al., 2021) by varying numbers of transformer layers and patch sizes. We also implement small “CCT” variants (Hassani et al., 2021) (only with 3 × 3 convolu- tion kernels), denoted with the number of transformer encoders followed by the number of convolutional blocks.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "Hassani",
                "Dosovitskiy",
                "eral",
                "ViT-Lite",
                "transformer",
                "number",
                "sev",
                "model",
                "sion",
                "sizes"
            ]
        }
    },
    {
        "id": "f261af95-0bf1-4507-9da6-f02c65a7b12a",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "For all the transformer-based models, we set the embedding dimension as 128 and use 4 attention heads. Table 1 shows the detailed ar- chitecture of transformer-based model variants. Hyperparameter selection. We perform manual hyperparame- ter tuning for each model family and apply the best settings.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "transformer-based",
                "attention",
                "heads",
                "model",
                "set",
                "embedding",
                "dimension",
                "Table",
                "shows",
                "chitecture"
            ]
        }
    },
    {
        "id": "e8511caf-b4cc-470b-9b27-3c489cfe4ac4",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "7 Model CIFAR-10 CIFAR-100 MNIST Fashion MNIST EMNIST Balanced #Params MACs Convolutional Neural Networks (Designed for CIFAR) ResNet-20 82.30 42.62 99.49 92.45 88.38 0.27M 0.04G ResNet-32 82.68 46.75 99.50 92.82 88.18 0.47M 0.07G ResNet-44 83.53 48.79 99.58 92.46 88.07 0.67M 0.10G ResNet-56 83.03 48.14 99.50 92.48 88.14 0.86M 0.13G MobileNetv2/0.2 71.79 37.99 99.06 90.16 85.42 0.21M <0.01G MobileNetv2/0.5 77.10 41.71 99.11 89.70 87.66 0.72M <0.01G MobileNetv2/0.75 80.10 44.37 99.16 90.13 86.75 1.39M <0.01G MobileNetv2/1.0 80.50 44.51 99.34 90.53 87.49 2.27M 0.01G Lite Vision Transformers ViT-Lite-1/16 53.71 25.24 97.66 87.37 82.02 0.25M <0.01G ViT-Lite-2/16 53.74 25.21 97.66 87.67 82.23 0.27M <0.01G ViT-Lite-1/8 61.67 32.51 98.04 87.01 83.35 0.42M 0.01G ViT-Lite-2/8 62.60 33.46 97.97 87.30 83.35 0.45M 0.02G ViT-Lite-1/4 68.66 38.43 98.63 90.25 86.22 1.21M 0.04G ViT-Lite-2/4 71.03 41.98 98.65 90.70 86.05 1.23M 0.07G Compact Convolutional Transformers CCT-2/2 79.71 50.75 99.17 91.37 88.93 0.28M 0.03G CCT-4/2 80.92 53.23 99.20 91.73 89.41 0.48M 0.05G On-Device Lightweight Convolutional Transformers (Ours) OnDev-LCT-1/1 84.55 57.69 99.46 92.73 89.52 0.21M 0.03G OnDev-LCT-2/1 86.27 59.17 99.39 92.50 89.18 0.31M 0.04G OnDev-LCT-4/1 86.61 61.36 99.53 92.70 89.39 0.51M 0.05G OnDev-LCT-8/1 86.64 62.62 99.49 92.59 89.55 0.91M 0.08G OnDev-LCT-1/3 85.73 57.66 99.41 92.72 88.96 0.25M 0.07G OnDev-LCT-2/3 86.04 60.26 99.46 93.31 89.60 0.35M 0.08G OnDev-LCT-4/3 87.03 61.95 99.38 92.60 89.49 0.55M 0.09G OnDev-LCT-8/3 87.65 62.91 99.28 92.82 89.45 0.95M 0.12G Table 3: Comparison on image classification performance in the centralized setting.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "Transformers",
                "Convolutional",
                "MNIST",
                "Model",
                "Table",
                "Balanced",
                "Params",
                "Networks",
                "Designed",
                "CIFAR"
            ]
        }
    },
    {
        "id": "5a51e550-aa37-494c-a8ce-30331274a83c",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "All reported accuracy (%) values are the best of 5 runs. All experiments are conducted without applying data augmentation or learning rate schedulers. The best accuracy is marked in bold. The best entry of each model family is underlined. In the centralized scenario, the training batch size is set to 256 for EMNIST-Balanced (Cohen et al., 2017) and 128 for others. In FL experiments, we reduce the batch size by half, i.e., 128 for EMNIST-Balanced (Cohen et al., 2017) and 64 for the rest.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "runs",
                "Cohen",
                "reported",
                "accuracy",
                "EMNIST-Balanced",
                "batch",
                "size",
                "experiments",
                "schedulers",
                "conducted"
            ]
        }
    },
    {
        "id": "081781e8-1da5-4efd-af73-fb3fd5d27718",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "It is important to note that, we set the training batch size to 16 for all experiments with the FEMNIST dataset. We use Adam (Kingma and Ba, 2014) optimizer for OnDev- LCTs and CNN baselines (He et al., 2016; Sandler et al., 2018) while AdamW (Loshchilov and Hutter, 2017a) with weight de- cay = 1e−4 is used for CCT (Hassani et al., 2021) and ViT- Lite (Dosovitskiy et al., 2021) variants. We manually tune the learning rate and use the best value for each model family.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "FEMNIST",
                "dataset",
                "Lite",
                "important",
                "note",
                "set",
                "training",
                "batch",
                "size",
                "experiments"
            ]
        }
    },
    {
        "id": "90051fa7-6301-4391-a785-7d9581e96596",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "The selected learning rate values and optimizers are shown in Ta- ble 2. Since we are focusing on a fair comparison of the model performance, we train all the models from scratch without pre- training or applying data augmentation techniques and learning rate schedulers for all experiments by default. These techniques could challenge the fairness of evaluation and provide sensitive results for tuning hyperparameters, especially in FL scenarios.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "ble",
                "learning",
                "rate",
                "selected",
                "optimizers",
                "shown",
                "techniques",
                "performance",
                "pre",
                "training"
            ]
        }
    },
    {
        "id": "48086f35-d1a9-4523-bbf0-e67918cfcc1a",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "However, label smoothing with a probability of 0.1 is used dur- ing the training of all model variants. Comparison in centralized scenario. Table 3 reports image classification performance of our OnDev-LCTs compared to existing lightweight vision models in a centralized scenario. The results show that our OnDev-LCT variants significantly outperform the other baselines while having lower computa- tional demands.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "scenario",
                "label",
                "dur",
                "ing",
                "smoothing",
                "probability",
                "training",
                "centralized",
                "variants",
                "Table"
            ]
        }
    },
    {
        "id": "7dd6c25f-9457-4a71-8053-30a6c7824cdd",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "Even our smallest OnDev-LCT-1/1 with 0.21M parameters can perform better in most cases compared to other model variants. Our larger variant OnDev-LCT-8/3 further im- proves the performance, especially on CIFAR-10 and CIFAR- 100 (Krizhevsky et al., 2009), achieving 4.62% and 14.77% better than ResNet-56 (He et al., 2016), which has a compa- rable model size and computational demands.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "smallest",
                "parameters",
                "model",
                "perform",
                "cases",
                "compared",
                "CIFAR",
                "Krizhevsky",
                "variants",
                "variant"
            ]
        }
    },
    {
        "id": "af565d2b-2d49-41c0-b7ee-291143eecf62",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "Figure 5 shows t-SNE (van der Maaten and Hinton, 2008) visualizations of feature embeddings learned by each model on the CIFAR- 10 (Krizhevsky et al., 2009) test data. Visualizations show that features learned by our proposed OnDev-LCT model are more divergent and separable than those learned by other baselines. Comparison in FL scenarios.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "Hinton",
                "CIFAR",
                "Krizhevsky",
                "Maaten",
                "Figure",
                "learned",
                "visualizations",
                "t-SNE",
                "van",
                "test"
            ]
        }
    },
    {
        "id": "977f118c-72d2-49b5-8ae4-bc2f1968e521",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "We conduct experiments in FL scenarios with small variants of each model family, constrain- ing the number of parameters to 1M (millions) and the com- 8 Class ID 0 1 2 3 4 5 6 7 8 9 (a) OnDev-LCT-1/1 (b) CCT-2/2 (c) ViT-1/8 (d) ResNet-20 (e) MobileNetv2/0.2 Figure 5: t-SNE visualizations of feature embeddings on CIFAR-10 test set learned by centralized models. Best viewed in color.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "Figure",
                "Class",
                "constrain",
                "millions",
                "family",
                "ing",
                "t-SNE",
                "test",
                "conduct",
                "experiments"
            ]
        }
    },
    {
        "id": "e29d716a-7bf0-4b9b-a3ad-c991d9fec481",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "Model FedAvg with 10 clients CIFAR-10 CIFAR-100 β = 0.1 β = 0.5 β = 5 β = 0.1 β = 0.5 β = 5 ResNet-20 67.55 75.43 77.72 27.89 37.08 37.53 ResNet-32 71.55 78.05 80.39 31.34 41.33 42.68 ResNet-44 72.56 77.73 80.55 32.85 43.60 44.81 MobileNetv2/0.2 56.55 67.07 69.44 21.52 30.57 31.67 MobileNetv2/0.5 60.50 72.35 74.27 23.40 33.28 32.58 ViT-Lite-1/8 48.53 58.05 59.70 17.85 27.60 31.37 ViT-Lite-2/8 49.03 58.57 60.02 17.99 28.20 32.32 CCT-2/2 66.45 74.86 78.42 35.21 47.99 50.50 CCT-4/2 66.38 75.36 78.55 39.44 50.43 52.76 OnDev-LCT-1/1 76.02 82.25 84.24 42.64 55.23 58.07 OnDev-LCT-2/1 76.85 82.81 84.95 44.83 56.74 59.53 OnDev-LCT-4/1 78.10 84.01 86.12 47.51 58.85 61.79 OnDev-LCT-8/1 79.70 84.74 86.77 49.75 59.71 62.42 Table 4: Comparison on image classification performance in FL with 10 clients under different data heterogeneity settings.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "clients",
                "Table",
                "Comparison",
                "Model",
                "settings",
                "FedAvg",
                "image",
                "classification",
                "performance",
                "data"
            ]
        }
    },
    {
        "id": "e206a61a-edb4-4ba8-a984-90f3790919f5",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "All reported accuracy (%) values are the best of 3 runs. Three best accuracy values are marked in bold. putation (i.e., MACs) to 0.1G (billions). For ViT-Lite (Has- sani et al., 2021) models, instead of choosing the smallest vari- ants, we choose larger variants with smaller patch sizes that can achieve better accuracy in centralized experiments.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "runs",
                "accuracy",
                "reported",
                "putation",
                "MACs",
                "billions",
                "bold",
                "models",
                "ants",
                "marked"
            ]
        }
    },
    {
        "id": "72710132-67c6-4224-866f-f9192a4194a5",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "Table 4 and Table 5 provide the image classification performance of our OnDev-LCTs on CIFAR-10 and CIFAR-100 (Krizhevsky et al., 2009) datasets compared to the other baselines under various data heterogeneity settings of FL. We can observe that our OnDev-LCTs significantly outperform other baselines in all data heterogeneity settings.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "Table",
                "Krizhevsky",
                "heterogeneity",
                "provide",
                "datasets",
                "OnDev-LCTs",
                "baselines",
                "data",
                "settings",
                "image"
            ]
        }
    },
    {
        "id": "8586a9be-bfcb-4fd7-82d7-3cb29e6de917",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "From Table 4 with a 10-client sce- nario, most of the transformer-based model variants can achieve comparable or only slightly lower results than the centralized performance under less data heterogeneity settings with β val- ues of 5 and 0.5. For CNNs, we can observe a significant gap between their centralized and FL performance, especially for the highest data heterogeneity setting with β = 0.1.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "Table",
                "sce",
                "nario",
                "heterogeneity",
                "val",
                "ues",
                "data",
                "centralized",
                "performance",
                "transformer-based"
            ]
        }
    },
    {
        "id": "05239589-9c2f-413a-940e-fd8a23e19ae2",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "Remark- ably, all the existing lightweight baseline models suffer from data heterogeneity in FL, especially the ViT-Lite model vari- ants. Meanwhile, our OnDev-LCT models achieve the best performance and are more robust to data heterogeneity across clients than the other baselines. Similarly, in Table 5 with a 50-client scenario, we can observe that our OnDev-LCTs out- perform the other baselines in all degrees of data heterogeneity.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "data",
                "heterogeneity",
                "Remark",
                "ably",
                "ants",
                "vari",
                "models",
                "baselines",
                "existing",
                "lightweight"
            ]
        }
    },
    {
        "id": "e9683d89-a3c6-4459-986f-df75f9fdfc2d",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "The reason behind this capability stems from the innova- tive combination of convolutions in our LCT tokenizer and the MHSA mechanism in our LCT encoder, which enables our OnDev-LCTs to strike a balance between local feature extrac- tion and global context representation.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "LCT",
                "MHSA",
                "innova",
                "tive",
                "encoder",
                "extrac",
                "tion",
                "representation",
                "reason",
                "capability"
            ]
        }
    },
    {
        "id": "35e60047-1cfe-4159-af79-1e6865ad8342",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "The strength of our LCT tokenizer lies in its efficient depthwise separable convolutions Model FedAvg with 50 clients CIFAR-10 CIFAR-100 β = 0.1 β = 0.5 β = 5 β = 0.1 β = 0.5 β = 5 ResNet-20 53.44 69.62 74.84 26.64 33.89 34.93 ResNet-32 55.79 70.50 76.17 26.27 36.89 38.02 ResNet-44 55.78 71.08 76.10 28.91 36.44 39.07 MobileNetv2/0.2 44.30 55.97 60.84 13.66 16.15 18.06 MobileNetv2/0.5 46.59 62.36 63.29 12.19 18.52 18.83 ViT-Lite-1/8 36.22 52.94 57.05 14.02 24.68 28.06 ViT-Lite-2/8 36.85 53.13 57.28 14.44 25.39 28.22 CCT-2/2 55.67 68.87 73.45 33.51 42.17 45.59 CCT-4/2 56.05 69.82 74.52 35.26 43.68 46.95 OnDev-LCT-1/1 66.18 76.93 79.71 32.69 49.34 53.83 OnDev-LCT-2/1 68.25 78.20 80.55 35.66 52.10 54.71 OnDev-LCT-4/1 67.24 77.68 80.87 38.82 52.64 56.15 OnDev-LCT-8/1 68.44 78.28 82.42 42.23 54.97 58.18 Table 5: Comparison on image classification performance in FL with 50 clients under different data heterogeneity settings.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "clients",
                "Table",
                "Comparison",
                "LCT",
                "Model",
                "settings",
                "strength",
                "tokenizer",
                "lies",
                "efficient"
            ]
        }
    },
    {
        "id": "3df79609-414b-4a26-bd62-42c1caf1ef76",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "All reported accuracy (%) values are the best of 3 runs. Three best accuracy values are marked in bold. and residual linear bottleneck blocks, as these convolutions cap- ture relevant spatial information efficiently while significantly reducing the computational burden and model size. By effec- tively extracting local features, our LCT tokenizer can capture essential details in image representations, thus enhancing the overall accuracy of the model.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "runs",
                "accuracy",
                "reported",
                "model",
                "LCT",
                "blocks",
                "cap",
                "ture",
                "size",
                "marked"
            ]
        }
    },
    {
        "id": "f481fb98-f08b-4ea2-9c94-b10d046321b5",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "Moreover, the MHSA mecha- nism in our LCT encoder facilitates learning global representa- tions of images, making OnDev-LCTs better at recognizing un- seen patterns and more adaptive to new and diverse data. This adaptability is crucial in practical applications where the model may encounter images from different sources or domains. In the context of FL, models are trained locally on individual devices with non-IID data distribution, and their updates are aggregated on the server.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "MHSA",
                "LCT",
                "mecha",
                "nism",
                "representa",
                "tions",
                "making",
                "images",
                "encoder",
                "facilitates"
            ]
        }
    },
    {
        "id": "7bb31c98-3cc0-4ba4-8779-4a91f0956c02",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "This process often introduces chal- lenges, such as data heterogeneity and communication bottle- necks. However, the lightweight nature of our OnDev-LCT ar- chitecture allows for more efficient communication of model updates between client devices and the central server, mitigat- ing the impact of communication constraints.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "lenges",
                "necks",
                "chal",
                "bottle",
                "communication",
                "process",
                "introduces",
                "data",
                "heterogeneity",
                "mitigat"
            ]
        }
    },
    {
        "id": "a98157ea-decd-42d6-8a14-cbf2987bf9e6",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "The advantage of OnDev-LCT can be attributed to its unique design and the in- tegration of image-specific inductive biases through the convo- lutional tokenization step before the transformer encoder. The incorporation of residual linear bottleneck blocks in the LCT tokenizer enhances the model’s ability to adapt to local varia- tions in data distribution, making it more resilient to non-IID data in the FL setting.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "tegration",
                "convo",
                "lutional",
                "encoder",
                "advantage",
                "OnDev-LCT",
                "attributed",
                "unique",
                "design",
                "image-specific"
            ]
        }
    },
    {
        "id": "42c5b2de-3223-469f-bbe9-2fcec4ee4c92",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "By leveraging these design choices, our OnDev-LCTs can learn more generalizable representations with fewer parameters, which is particularly beneficial in FL scenarios with limited data and communication constraints. As 9 Class ID 0 1 2 3 4 5 6 7 8 9 (a) OnDev-LCT-1/1 (b) CCT-2/2 (c) ViT-Lite-1/8 (d) ResNet-20 (e) MobileNetv2/0.2 Figure 6: t-SNE visualizations of feature embeddings on CIFAR-10 by FedAvg global models (10 clients with β = 0.5). Best viewed in color.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "choices",
                "parameters",
                "constraints",
                "Figure",
                "leveraging",
                "design",
                "OnDev-LCTs",
                "learn",
                "generalizable",
                "representations"
            ]
        }
    },
    {
        "id": "aaab77d4-f2e0-428b-995e-203678ccf035",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "Class ID 0 1 2 3 4 5 6 7 8 9 (a) OnDev-LCT-1/1 (b) CCT-2/2 (c) ViT-1/8 (d) ResNet-20 (e) MobileNetv2/0.2 Figure 7: t-SNE visualizations of feature embeddings on CIFAR-10 by FedAvg global models (50 clients with β = 0.5). Best viewed in color. Model No.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "Figure",
                "Class",
                "t-SNE",
                "clients",
                "visualizations",
                "feature",
                "embeddings",
                "FedAvg",
                "global",
                "color"
            ]
        }
    },
    {
        "id": "30d5ea8b-d5ae-4f04-828e-c0b693896b5d",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "of selected clients (C) 5 10 2∼10 ResNet-20 78.94 81.57 78.29 ResNet-32 79.82 82.23 79.27 ResNet-44 80.01 82.45 80.20 MobileNetv2/0.2 66.72 68.68 64.93 MobileNetv2/0.5 66.70 74.60 69.16 ViT-Lite-1/8 59.75 62.06 59.57 ViT-Lite-2/8 60.48 63.48 59.58 CCT-2/2 78.58 80.71 78.78 CCT-4/2 78.60 81.28 79.95 OnDev-LCT-1/1 77.86 80.68 76.67 OnDev-LCT-2/1 79.57 81.43 76.56 OnDev-LCT-4/1 79.84 81.84 78.76 OnDev-LCT-8/1 80.33 82.02 80.06 Table 6: Comparison on image classification performance in realistic non- IID FL settings using the FEMNIST dataset from LEAF benchmark.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "Table",
                "Comparison",
                "IID",
                "FEMNIST",
                "LEAF",
                "clients",
                "benchmark",
                "selected",
                "image",
                "classification"
            ]
        }
    },
    {
        "id": "82a559bb-4d34-4a42-b9e5-b50d09fdf29b",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "All re- ported accuracy (%) values are the best of 3 runs. Three best accuracy values are marked in bold. a result, the proposed OnDev-LCT models demonstrate higher performance and robustness compared to other baselines under various data heterogeneity settings of FL. However, the scope and applicability of OnDev-LCT are not confined to FL scenar- ios and, the flexibility and efficient nature of OnDev-LCT holds great potential in emerging fields like edge AI and on-device AI.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "runs",
                "ported",
                "accuracy",
                "OnDev-LCT",
                "bold",
                "marked",
                "result",
                "scenar",
                "ios",
                "proposed"
            ]
        }
    },
    {
        "id": "59d35f66-e5ef-4cd0-bfe7-d97442bbe00c",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "It is important to emphasize that the superior performance of the proposed OnDev-LCT highlights its potential value and broader applicability in FL and other resource-constrained on- device vision tasks across various domains. Figure 6 and Fig- ure 7 show the t-SNE (van der Maaten and Hinton, 2008) visu- alizations of feature embeddings learned by each global model under FedAvg setting (β = 0.5) with 10 clients and 50 clients on the CIFAR-10 (Krizhevsky et al., 2009) test data.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "device",
                "domains",
                "clients",
                "important",
                "emphasize",
                "superior",
                "performance",
                "proposed",
                "OnDev-LCT",
                "highlights"
            ]
        }
    },
    {
        "id": "d6ec73bb-b513-40e4-b546-6f68a10657e1",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "Comparison in realistic FL scenarios. In this section, we con- ducted a performance comparison of our proposed OnDev- LCTs with other baseline models using the FEMNIST (Cal- das et al., 2018) dataset. The results, as presented in Table 6, demonstrate that convolutional transformers, such as OnDev- LCTs and CCTs (Hassani et al., 2021), achieve competitive per- formance on par with ResNet variants (He et al., 2016).",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "scenarios",
                "Cal",
                "OnDev",
                "LCTs",
                "Comparison",
                "FEMNIST",
                "realistic",
                "dataset",
                "Table",
                "Hassani"
            ]
        }
    },
    {
        "id": "712461ec-0029-4109-8bfa-67e516fa0fe3",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "ViT- Lite (Hassani et al., 2021) models, while being lightweight, show comparatively weaker performance, due to their limited ability to efficiently capture both local and global features, which are crucial for accurate image classification in realis- tic FL scenarios like FEMNIST. The accuracy of MobileNetv2 variants (Sandler et al., 2018) may not have reached its peak during the conducted training epochs, implying the potential for increased accuracy with further training.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "Lite",
                "Hassani",
                "FEMNIST",
                "ViT",
                "models",
                "lightweight",
                "show",
                "performance",
                "due",
                "features"
            ]
        }
    },
    {
        "id": "79b8ffe9-86f5-4044-810e-ae8f0a6e1d39",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "Nonetheless, our proposed OnDev-LCTs consistently show their impressive per- formance across various scenarios with different numbers of se- lected clients participating in each training round, thus demon- strating the effectiveness of our model design for on-device vi- sion tasks in FL scenarios.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "Nonetheless",
                "formance",
                "lected",
                "round",
                "demon",
                "strating",
                "sion",
                "scenarios",
                "proposed",
                "OnDev-LCTs"
            ]
        }
    },
    {
        "id": "37b8fa5e-a3dd-4d08-9ab1-5ea6f05cb56d",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "The integration of efficient convo- lutions, self-attention mechanisms, and image-specific induc- tive biases empowers our models to strike a balance between local feature extraction and global context representation, con- tributing to their superior performance and efficiency. These compelling findings highlight the potential of our OnDev-LCT architecture as a practical solution for on-device vision tasks in FL with limited data and computational resources. 6.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "lutions",
                "con",
                "convo",
                "self-attention",
                "mechanisms",
                "induc",
                "tive",
                "representation",
                "tributing",
                "efficiency"
            ]
        }
    },
    {
        "id": "69c654b0-d74b-438b-9eb6-e13da8cad51c",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "Additional Results We present additional results for the performance analysis of our proposed architecture in this section. Comparative analysis with low-bit vision transformers. As an orthogonal approach to our work, we explore the existing lit- erature on popular quantized, aka low-bit vision transformers, emphasizing their contributions to the domain of on-device im- age classification tasks. Liu et al.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "Results",
                "Additional",
                "transformers",
                "section",
                "vision",
                "analysis",
                "present",
                "performance",
                "proposed",
                "architecture"
            ]
        }
    },
    {
        "id": "53861930-98c9-4b06-b028-feedee29fcef",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "(2021) present VT-PTQ, a mixed-precision post-training quantization scheme, which in- troduces a ranking loss to the quantization objective to keep 10 Model Quantization method #bit(W-A) CIFAR-10 FEMNIST (C = 10) Inference time (s) #Params Centralized FedAvg with 50 clients β = 0.1 β = 0.5 β = 5 Q-ViT QAT 4-4 55.91 37.89 43.02 45.73 59.48 3.50 22M MinMax PTQ 8-8 55.05 31.04 45.27 49.56 37.99 2.42 22M EMA PTQ 8-8 52.86 29.75 45.08 51.19 50.73 2.43 22M Percentile PTQ 8-8 53.70 29.82 45.79 50.42 47.75 2.46 22M OMSE PTQ 8-8 52.69 30.83 45.66 50.24 49.50 2.44 22M FQ-ViT PTQ 8-8 51.72 28.86 44.52 49.19 49.37 2.62 22M OnDev-LCT-1/1 None 32-32 84.55 66.18 76.93 79.71 80.68 2.66 0.21M OnDev-LCT-2/1 None 32-32 86.27 68.25 78.20 80.55 81.43 2.78 0.31M OnDev-LCT-4/1 None 32-32 86.61 67.24 77.68 80.87 81.84 3.13 0.51M OnDev-LCT-8/1 None 32-32 86.64 68.44 78.28 82.42 82.02 3.77 0.91M Table 7: Comparative analysis with low-bit vision transformers on CIFAR-10 and FEMNIST datasets.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "PTQ",
                "32-32",
                "FEMNIST",
                "quantization",
                "Table",
                "Model",
                "Inference",
                "Params",
                "EMA",
                "Percentile"
            ]
        }
    },
    {
        "id": "350b97fa-1380-4c11-b805-e1105a23da63",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "Three best accuracy (%) values are marked in bold. Inference time represents the wall-clock time, measured on a single NVIDIA RTX 3080 GPU for the CIFAR-10 test set with a batch size of 1000.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "accuracy",
                "bold",
                "RTX",
                "GPU",
                "marked",
                "NVIDIA",
                "time",
                "Inference",
                "measured",
                "test"
            ]
        }
    },
    {
        "id": "8fc01808-cd85-4d21-8b50-b462c9887c27",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "Model Pruning Rate CIFAR-10 #Params Params Ratio Centralized FedAvg with 50 clients β = 0.1 β = 0.5 β = 5 ViT-Tiny (Baseline) 0.0 55.86 36.25 47.85 50.76 5.49M 1.000× L1-norm Importance 0.3 52.91 33.23 44.94 46.88 3.01M 0.548× L2-norm Importance 0.3 53.21 33.59 44.86 46.71 3.01M 0.548× Taylor Importance 0.3 53.90 32.51 44.97 47.58 3.01M 0.548× Random Importance 0.3 39.88 27.66 37.59 39.08 3.01M 0.548× OnDev-LCT-1/1 0.0 84.55 66.18 76.93 79.71 0.21M 0.038× OnDev-LCT-2/1 0.0 86.27 68.25 78.20 80.55 0.31M 0.056× OnDev-LCT-4/1 0.0 86.61 67.24 77.68 80.87 0.51M 0.093× OnDev-LCT-8/1 0.0 86.64 68.44 78.28 82.42 0.91M 0.166× Table 8: Comparative analysis with pruned vision transformers on CIFAR-10 dataset.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "Importance",
                "Params",
                "Baseline",
                "Table",
                "Rate",
                "Taylor",
                "Random",
                "Comparative",
                "Pruning",
                "Ratio"
            ]
        }
    },
    {
        "id": "007e516a-e8d5-4bfc-ac7f-cb32ea2a143d",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "Three best accuracy (%) values are marked in bold. the relative order of the self-attention results after quantiza- tion. Yuan et al. (2022) develop the efficient PTQ4ViT frame- work, employing twin uniform quantization to handle the spe- cial distributions of post-softmax and post-GELU activations.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "accuracy",
                "bold",
                "marked",
                "tion",
                "quantiza",
                "frame",
                "work",
                "relative",
                "order",
                "self-attention"
            ]
        }
    },
    {
        "id": "de003feb-191e-4dbe-b5c6-f49c343de923",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "FQ-ViT (Lin et al., 2022) is then implemented under the PTQ, a training-free paradigm, by introducing the power-of-two fac- tor (PTF) and log-int-softmax (LIS) quantization methods for LayerNorm and Softmax modules, which are not quantized in PTQ4ViT.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "PTF",
                "LIS",
                "FQ-ViT",
                "fac",
                "tor",
                "Lin",
                "PTQ",
                "Softmax",
                "paradigm",
                "quantization"
            ]
        }
    },
    {
        "id": "6de75f8c-b770-4bcd-b5a6-3745360351f4",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "In comparison to the adaptation of vision transform- ers to prior PTQ-based quantization schemes, including Min- Max, EMA (Jacob et al., 2018), Percentile (Li et al., 2019), OMSE (Choukroun et al., 2019), and PTQ4ViT (Yuan et al., 2022), FQ-ViT achieves a nearly lossless quantization perfor- mance with full-precision models. On the other hand, Li et al.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "Max",
                "EMA",
                "Percentile",
                "OMSE",
                "Min",
                "Jacob",
                "Choukroun",
                "Yuan",
                "quantization",
                "transform"
            ]
        }
    },
    {
        "id": "8e28bf5f-239b-4fc1-a68e-2110f9b23e0f",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "(2022) analyze the quantization robustness of each component in the vision transformer to implement a fully quantized ViT baseline under the straightforward QAT pipeline. The Q-ViT is then proposed by introducing an information rectification mod- ule (IRM) and a distribution-guided distillation (DGD) scheme towards accurate and fully quantized vision transformers.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "QAT",
                "analyze",
                "pipeline",
                "vision",
                "fully",
                "quantized",
                "IRM",
                "DGD",
                "quantization",
                "robustness"
            ]
        }
    },
    {
        "id": "5521ab87-7de2-4481-b105-e5e768b9064c",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "Ex- perimental results on the ImageNet (Deng et al., 2009) dataset show that Q-ViTs outperform the baselines and achieve com- petitive performance with the full-precision counterparts while significantly reducing memory and computational resources.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "Deng",
                "perimental",
                "ImageNet",
                "dataset",
                "petitive",
                "resources",
                "results",
                "show",
                "Q-ViTs",
                "outperform"
            ]
        }
    },
    {
        "id": "3243f6e7-de4d-4b88-90f8-4aa9f0230547",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "In Table 7, we present a comprehensive comparison of our proposed OnDev-LCTs with various low-bit transformers (i.e., ViT-S backbone) in terms of accuracy, inference time, and num- ber of parameters for image classification tasks on CIFAR- 10 (Krizhevsky et al., 2009) and FEMNIST (Caldas et al., 2018) datasets. To ensure a fair comparison, we implement Q-ViT without the DGD scheme, which relies on the performance of a powerful teacher model.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "Table",
                "CIFAR",
                "Krizhevsky",
                "FEMNIST",
                "Caldas",
                "datasets",
                "transformers",
                "ViT-S",
                "backbone",
                "accuracy"
            ]
        }
    },
    {
        "id": "c3827296-4dd1-4151-9a96-c3f21f3c119f",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "All models are trained from scratch without pre-training or applying data augmentation techniques and learning rate schedulers. The results clearly show that our proposed OnDev-LCTs outperform the low-bit vision trans- formers in all cases.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "schedulers",
                "models",
                "trained",
                "scratch",
                "pre-training",
                "applying",
                "data",
                "augmentation",
                "techniques",
                "learning"
            ]
        }
    },
    {
        "id": "ceb0ac62-5db4-40cc-8942-7133339336f9",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "Nonetheless, we maintain the perspective that the performance of low-bit vision transformers can also be significantly improved by leveraging a powerful backbone model along with a proper configuration of data augmentation and learning rate scheduler while being smaller and faster than traditional vision transformers. However, it is worth noting that low-bit vision transformers can be more complex to train and may not be as effective for certain tasks.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "transformers",
                "Nonetheless",
                "vision",
                "low-bit",
                "maintain",
                "perspective",
                "performance",
                "significantly",
                "improved",
                "leveraging"
            ]
        }
    },
    {
        "id": "32c700a8-3f69-46ce-b7a6-25cb21c3059c",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "As a result, the choice of whether to use our OnDev-LCT or a low-bit vision trans- former may depend on the specific task and application. Comparative analysis with vision transformer pruning. In this section, we delve into prior research on vision transformer pruning, which is an emerging study within the broader domain of model compression and optimization.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "pruning",
                "vision",
                "result",
                "trans",
                "application",
                "transformer",
                "choice",
                "OnDev-LCT",
                "low-bit",
                "depend"
            ]
        }
    },
    {
        "id": "81079638-5531-421e-bcd2-ed34134a3361",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "This process system- atically identifies and removes unnecessary parameters, such as unimportant weights or neurons, from a pre-trained vision transformer, guided by various criteria, including weight mag- nitude (LeCun et al., 1989; Han et al., 2016; Liu et al., 2017), gradient magnitude (Molchanov et al., 2019, 2016), activation values (Chen et al., 2021), or importance scores (Zhu et al., 2021).",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "Han",
                "Liu",
                "Molchanov",
                "Chen",
                "Zhu",
                "nitude",
                "system",
                "atically",
                "parameters",
                "neurons"
            ]
        }
    },
    {
        "id": "a17918c5-5ac7-4c18-9867-b22b75ca9140",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "The primary goal is to create a more compact vision transformer that maintains reasonable performance, making it suitable for deployment on resource-constrained devices where computational efficiency is crucial.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "performance",
                "making",
                "crucial",
                "primary",
                "goal",
                "create",
                "compact",
                "vision",
                "transformer",
                "maintains"
            ]
        }
    },
    {
        "id": "fc3dfd31-362a-47a9-8c50-967265713350",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "One notable approach, 11 Model CIFAR-10 FEMNIST (C = 10) #Params Centralized FedAvg with 50 clients β = 0.1 β = 0.5 β = 5 EdgeNeXt-XXS 58.17 36.22 45.24 53.16 63.65 1.17M MobileViT-XXS 52.91 28.30 45.91 49.37 75.37 0.94M MobileViTv2-0.5 52.84 28.28 48.95 51.80 75.64 1.13M CCT-2/2 79.71 55.67 68.87 73.45 80.71 0.28M CCT-4/2 80.92 56.05 69.82 74.52 81.28 0.48M Hybrid Vision Transformer (ViT) 74.64 45.22 65.70 70.93 80.25 0.75M Hybrid Vision Performer (ViP) 77.52 43.87 65.53 70.89 80.37 0.76M Hybrid Vision Linformer (ViL) 50.68 31.83 49.01 57.91 76.65 0.95M Hybrid Vision Nystr¨oformer (ViN) 77.35 51.25 63.81 68.56 81.31 0.76M OnDev-LCT-1/1 84.55 66.18 76.93 79.71 80.68 0.21M OnDev-LCT-2/1 86.27 68.25 78.20 80.55 81.43 0.31M OnDev-LCT-4/1 86.61 67.24 77.68 80.87 81.84 0.51M OnDev-LCT-8/1 86.64 68.44 78.28 82.42 82.02 0.91M Table 9: Comparative analysis with convolutional transformers on CIFAR-10 and FEMNIST datasets.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "Hybrid",
                "Vision",
                "Model",
                "Table",
                "FEMNIST",
                "Params",
                "Performer",
                "Linformer",
                "Nystr",
                "Comparative"
            ]
        }
    },
    {
        "id": "21034a1e-6254-4d71-bfee-e40114f0b7dd",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "Each model variant is trained from scratch without applying data augmentation or learning rate schedulers. Three best accuracy (%) values are marked in bold.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "schedulers",
                "model",
                "variant",
                "trained",
                "scratch",
                "applying",
                "data",
                "augmentation",
                "learning",
                "rate"
            ]
        }
    },
    {
        "id": "2e498479-0658-450e-bdc4-99dcd4d6201e",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "Model Pre-trained CIFAR-10 FEMNIST (C = 10) #Params Centralized FedAvg with 50 clients β = 0.1 β = 0.5 β = 5 MobileNetv2/1.0 ✓ 82.22 49.85 80.74 84.00 79.53 2.30M MobileNetv2/1.0 ✗ 36.76 31.34 47.92 50.72 71.81 2.30M ViT-Tiny ✓ 67.51 48.82 64.09 68.20 61.77 5.49M ViT-Tiny ✗ 55.86 36.25 47.85 50.76 57.10 5.49M MobileViT-XXS ✓ 81.27 56.75 79.12 82.77 81.04 0.94M MobileViT-XXS ✗ 52.91 28.30 45.91 49.37 75.37 0.94M MobileViTv2-0.5 ✓ 82.69 44.06 80.52 83.13 80.65 1.13M MobileViTv2-0.5 ✗ 52.84 28.28 48.95 51.80 75.64 1.13M OnDev-LCT-8/1 ✗ 86.64 68.44 78.28 82.42 82.02 0.91M OnDev-LCT-8/3 ✗ 87.65 68.56 78.56 82.70 82.75 0.95M Table 10: Comparative analysis with ImageNet pre-trained models on CIFAR-10 and FEMNIST datasets.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "ViT-Tiny",
                "MobileViT-XXS",
                "FEMNIST",
                "Table",
                "Pre-trained",
                "Params",
                "Comparative",
                "Centralized",
                "clients",
                "datasets"
            ]
        }
    },
    {
        "id": "ce545700-7bbd-482b-a1f1-b2861a0539f2",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "Three best accuracy (%) values are marked in bold. VTP (Zhu et al., 2021), extends the principles of network slim- ming (Liu et al., 2017) to reduce the number of embedding di- mensions through the introduction of learnable coefficients that evaluate importance scores. Neurons with small coefficients are removed based on a predefined threshold. However, it’s worth noting that VTP requires manual tuning of thresholds for all layers and subsequent model fine-tuning.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "accuracy",
                "bold",
                "VTP",
                "marked",
                "Zhu",
                "Liu",
                "coefficients",
                "ming",
                "extends",
                "slim"
            ]
        }
    },
    {
        "id": "a748b0d8-153d-41e0-ba50-f64d9fa68cff",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "Another method, WDPruning (Yu et al., 2022), is a structured pruning technique for vision transformers, which employs a 0/1 mask to differ- entiate unimportant and important parameters based on their magnitudes. While it uses a differentiable threshold, the non- differentiable mask may introduce gradient bias, potentially re- sulting in suboptimal weight retention.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "WDPruning",
                "method",
                "transformers",
                "differ",
                "entiate",
                "magnitudes",
                "mask",
                "structured",
                "pruning",
                "technique"
            ]
        }
    },
    {
        "id": "e9f00a73-7c20-4315-89d6-5cdaeb6abe9f",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "X-Pruner (Yu and Xi- ang, 2023) focuses on removing less contributing units, where each prunable unit is assigned an explainability-aware mask to quantify its contribution to predicting each class in terms of explainability. Finally, DepGraph (Fang et al., 2023) serves as a grouping algorithm used to analyze dependencies in net- works for enabling any structural pruning over various network architectures.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "X-Pruner",
                "ang",
                "focuses",
                "explainability",
                "removing",
                "contributing",
                "prunable",
                "assigned",
                "explainability-aware",
                "mask"
            ]
        }
    },
    {
        "id": "bb16d47c-4215-4bdb-859f-08149f2f1735",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "Nonetheless, while vision transformer pruning can yield smaller and faster models, the challenge remains in finding the right balance between reducing model size and pre- serving task-specific features. Table 8 provides a performance comparison between our proposed OnDev-LCTs and pruned vision transformers on the CIFAR-10 (Krizhevsky et al., 2009) dataset. We focus on key metrics such as accuracy and the reduction in parameters com- pared to the ViT-Tiny (Dosovitskiy et al., 2021) backbone.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "Nonetheless",
                "vision",
                "pre",
                "serving",
                "features",
                "models",
                "model",
                "pruning",
                "yield",
                "smaller"
            ]
        }
    },
    {
        "id": "0c33f2e2-28c9-467e-8e4a-248d83a2ff9e",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "We followed the Torch-Pruning (TP) implementation for structural pruning, which employs the DepGraph algorithm to physically remove parameters (Fang et al., 2023). Several important cri- teria are considered, including L-p Norm, Taylor, and Random. The results demonstrate that our proposed OnDev-LCTs con- sistently outperform the pruned ViT models in all scenarios.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "Fang",
                "Torch-Pruning",
                "implementation",
                "pruning",
                "parameters",
                "Taylor",
                "structural",
                "employs",
                "DepGraph",
                "algorithm"
            ]
        }
    },
    {
        "id": "a20a4a79-5e83-407a-9919-438df5200fd7",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "From one point of view, developing effective pruning strategies can be a complex task, often involving iterative trial and error to identify the optimal model components for pruning. Aggressive pruning carries the risk of significant model generalization loss, as removing too many parameters may lead to underfitting and reduced performance. In summary, vision transformer pruning is a technique applied to existing large models to reduce their size.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "pruning",
                "view",
                "developing",
                "task",
                "point",
                "effective",
                "strategies",
                "complex",
                "involving",
                "iterative"
            ]
        }
    },
    {
        "id": "d1e48170-7626-4462-adda-41bd6c1010fa",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "On the other hand, OnDev-LCT provides an efficient and lightweight neural network architecture, offering an alternative solution for resource-constrained on-device vision applications. Comparative analysis with convolutional transformers. In this analysis, we conduct a comparative study between our OnDev- LCTs and other convolutional transformers that do not rely on heavy data augmentation techniques.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "hand",
                "OnDev-LCT",
                "architecture",
                "offering",
                "applications",
                "efficient",
                "lightweight",
                "neural",
                "network",
                "alternative"
            ]
        }
    },
    {
        "id": "8afa0e9f-282a-498f-acc3-d26cb310488d",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "All models are trained from scratch without utilizing data augmentation or learning rate schedulers.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "schedulers",
                "models",
                "trained",
                "scratch",
                "utilizing",
                "data",
                "augmentation",
                "learning",
                "rate"
            ]
        }
    },
    {
        "id": "fe050fe7-aed9-4b55-86d8-368c62504b0e",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "It is important to note that while CCTs (Hassani et al., 2021) require high-level data augmentation techniques to 12 Model Centralized Top-1 Acc Centralized Top-5 Acc FedAvg with 10 clients FedAvg with 50 clients #Params MACs β = 0.1 β = 0.5 β = 5 β = 0.1 β = 0.5 β = 5 ResNet-20 26.99 50.38 13.91 22.18 26.13 9.08 16.64 22.09 0.34M 0.04G ResNet-32 27.92 51.98 14.78 22.82 27.03 10.23 17.89 21.84 0.53M 0.07G ResNet-44 28.12 52.35 15.16 23.45 27.67 11.32 17.25 23.04 0.73M 0.10G ResNet-56 28.18 52.57 15.03 23.56 27.93 12.45 18.42 23.06 0.93M 0.13G MobileNetv2/0.5 18.44 39.22 11.16 18.87 20.28 8.90 14.09 16.19 1.99M <0.01G MobileNetv2/0.75 20.66 42.72 16.27 23.60 25.58 10.63 15.85 18.04 2.66M <0.01G MobileNetv2/1.0 29.84 54.60 16.58 24.78 27.76 11.00 16.92 20.14 3.54M 0.01G ViT-Lite-1/4 11.76 27.39 4.85 8.53 10.61 3.42 7.26 10.18 1.46M 0.04G ViT-Lite-2/4 12.94 30.31 5.55 9.35 11.73 3.81 8.00 11.08 1.48M 0.07G CCT-2/2 26.97 50.04 14.07 22.71 27.43 10.26 19.16 24.14 0.40M 0.03G CCT-4/2 30.27 53.89 15.82 25.21 29.99 11.20 20.92 26.46 0.60M 0.05G OnDev-LCT-1/1 38.46 62.95 20.91 32.01 36.93 13.87 24.68 31.07 0.34M 0.03G OnDev-LCT-2/1 40.59 65.53 21.78 32.93 38.59 14.84 26.23 32.39 0.44M 0.04G OnDev-LCT-4/1 42.34 67.33 23.27 34.66 39.98 15.54 27.28 33.77 0.64M 0.05G OnDev-LCT-8/1 43.37 68.30 24.87 35.23 41.03 16.32 28.10 35.84 1.04M 0.08G Table 11: Performance analysis on ImageNet-32 dataset.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "Centralized",
                "Acc",
                "clients",
                "Table",
                "FedAvg",
                "Hassani",
                "Model",
                "Params",
                "Performance",
                "dataset"
            ]
        }
    },
    {
        "id": "4d948c93-d0ee-470e-af28-5e898f4103f5",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "Three best accuracy (%) values are marked in bold. achieve their best performance, we include them in our com- parison to provide a complete evaluation of existing lightweight convolutional transformers. Based on the reported values in Ta- ble 9, our models outperform the other benchmarks, demon- strating a significant performance gap compared to variants with similar sizes.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "accuracy",
                "bold",
                "marked",
                "performance",
                "ble",
                "demon",
                "achieve",
                "parison",
                "transformers",
                "Based"
            ]
        }
    },
    {
        "id": "7d7923d4-5d12-47a1-8f4f-9a3b6b040706",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "This highlights the superior performance and efficiency of our proposed architecture in terms of parameters and computation cost. In contrast to other methods that com- bine convolutions and transformers, the distinguishing factor of our OnDev-LCT lies in its unique and efficient LCT tokenizer design, effectively leveraging depthwise separable convolutions and residual linear bottleneck blocks.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "cost",
                "highlights",
                "superior",
                "performance",
                "efficiency",
                "proposed",
                "architecture",
                "terms",
                "parameters",
                "computation"
            ]
        }
    },
    {
        "id": "566d6829-f1bd-4b7e-a6c3-2c5bb67f297e",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "By integrating the LCT tokenizer before the transformer encoder, we introduce image- specific inductive biases to our OnDev-LCT, enabling it to cap- ture essential local details from image representations. Further- more, the LCT encoder’s MHSA mechanism aids in learning global representations of images.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "LCT",
                "specific",
                "OnDev-LCT",
                "enabling",
                "cap",
                "ture",
                "encoder",
                "representations",
                "integrating",
                "tokenizer"
            ]
        }
    },
    {
        "id": "85ade1d8-0b6c-4b0c-9e03-5d9e130d6e54",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "This combination of comple- mentary components allows our model to achieve high accuracy while maintaining its lightweight nature, making it well-suited for on-device vision tasks with limited resources. Comparative analysis with pre-trained models. In this analy- sis, we evaluate the performance of our OnDev-LCTs in com- parison to variants of existing state-of-the-art models that are pre-trained on the ImageNet (Deng et al., 2009) dataset.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "comple",
                "mentary",
                "nature",
                "making",
                "resources",
                "combination",
                "components",
                "achieve",
                "high",
                "accuracy"
            ]
        }
    },
    {
        "id": "bbb4ad0c-85b9-4232-adfc-c748bc61c281",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "We implement all the pre-trained models using the PyTorch (Paszke et al., 2019) framework, following the official implementation provided by the PyTorch Image Models (timm) (Wightman, 2019) collection. According to the results shown in Tabel 10, our OnDev-LCTs achieve competitive performance even with limited training data, illustrating their robustness and effective- ness for on-device vision tasks.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "Wightman",
                "models",
                "Paszke",
                "PyTorch",
                "Image",
                "framework",
                "timm",
                "collection",
                "implement",
                "pre-trained"
            ]
        }
    },
    {
        "id": "b544be06-5bc7-43b2-a4f2-8a9ed306d2b6",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "While pre-trained models have shown promising performance across various vision tasks, their practicality in resource-constrained on-device scenarios is of- ten hindered by challenges related to accessing and utilizing large pre-training datasets. Such datasets may not be readily available in many domains (Varoquaux and Cheplygina, 2022; Parekh et al., 2022; Mutis and Ambekar, 2020; Ghassemi et al., 2020) due to privacy concerns or data distribution heterogene- ity across devices.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "tasks",
                "ten",
                "datasets",
                "pre-trained",
                "models",
                "shown",
                "promising",
                "performance",
                "vision",
                "practicality"
            ]
        }
    },
    {
        "id": "8f768a6c-0f41-4be3-ba47-e939acd2b5af",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "Since pre-trained models are designed and Model Augment Augment + LR Scheduler Baseline #Params OnDev-LCT-1/1 90.04 91.05 84.55 0.21M OnDev-LCT-8/1 90.36 91.14 86.64 0.91M OnDev-LCT-1/3 90.25 91.41 85.73 0.25M OnDev-LCT-8/3 90.50 91.99 87.65 0.95M Table 12: Performance analysis of our OnDev-LCT architecture on CIFAR-10 dataset for the impact of simple data augmentation and learning rate scheduler.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "Params",
                "Table",
                "Baseline",
                "Performance",
                "Augment",
                "Scheduler",
                "dataset",
                "models",
                "Model",
                "pre-trained"
            ]
        }
    },
    {
        "id": "9ddfe627-5438-4e7d-b43f-a1cac9a4d828",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "optimized for specific datasets and objectives, which may not closely align with the target domain, fine-tuning a pre-trained model on downstream tasks may not always be straightfor- ward for on-device scenarios with limited training data and re- sources.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "sources",
                "optimized",
                "objectives",
                "domain",
                "fine-tuning",
                "straightfor",
                "ward",
                "specific",
                "datasets",
                "closely"
            ]
        }
    },
    {
        "id": "8481836d-d9c6-47fe-9043-9c0829494904",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "Despite not relying on extensive pre-training datasets, our OnDev-LCTs leverage depthwise separable convolutions and MHSA mechanism to efficiently capture both local and global features from images, making them well-suited for on- device vision tasks with limited data.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "MHSA",
                "datasets",
                "images",
                "making",
                "device",
                "data",
                "relying",
                "extensive",
                "pre-training",
                "OnDev-LCTs"
            ]
        }
    },
    {
        "id": "0b03b1d0-6c43-47c8-a87f-0682f200ad61",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "Thus, the unique design of our OnDev-LCT offers a practical and effective alternative to pre-train-and-fine-tune schemes, providing more robust and efficient solutions for on-device vision tasks in FL scenarios, where pre-training datasets may not be readily available. Analysis on ImageNet-32 dataset.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "schemes",
                "providing",
                "scenarios",
                "unique",
                "design",
                "OnDev-LCT",
                "offers",
                "practical",
                "effective",
                "alternative"
            ]
        }
    },
    {
        "id": "ca06abb2-9222-4bd2-b185-9ef30e1550b7",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "Table 11 compares the cen- tralized and FL performance of our proposed models for im- age classification on ImageNet-32, the down-sampled version of the original ImageNet-1K (Deng et al., 2009) dataset, which contains 1,281,167 training samples and 50,000 test samples of 32×32 RGB images in 1,000 classes. We apply the same hyper- parameter settings and training strategy used with the CIFAR- 100 dataset and train all models from scratch without data aug- mentation and learning rate schedulers.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "Deng",
                "RGB",
                "samples",
                "Table",
                "classes",
                "dataset",
                "compares",
                "cen",
                "tralized",
                "age"
            ]
        }
    },
    {
        "id": "3bb5d8df-8073-486a-94ab-76b5dece6f1c",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "According to the re- ported values, our models outperform the other baselines, and there is a significant gap between their performance and that of variants with comparable sizes. Thus, we can summarize that our proposed architecture is better in performance and also ef- ficient in terms of parameters and computation cost. Analysis on the impact of data augmentation and learning rate scheduler.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "performance",
                "ported",
                "baselines",
                "sizes",
                "models",
                "outperform",
                "significant",
                "gap",
                "variants",
                "comparable"
            ]
        }
    },
    {
        "id": "b111b1b7-77e8-4a82-b930-566cd9ac495e",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "In this experiment, we analyze the improved 13 Model FedAvg with 10 clients FedAvg with 50 clients MNIST Fashion MNIST EMNIST Balanced MNIST Fashion MNIST EMNIST Balanced β = 0.1 β = 0.5 β = 5 β = 0.1 β = 0.5 β = 5 β = 0.1 β = 0.5 β = 5 β = 0.1 β = 0.5 β = 5 β = 0.1 β = 0.5 β = 5 β = 0.1 β = 0.5 β = 5 ResNet-20 98.99 99.50 99.58 81.33 91.83 92.75 81.47 88.46 89.68 97.65 99.46 99.54 70.63 90.71 92.06 82.98 88.64 89.50 ResNet-32 99.13 99.49 99.59 82.23 92.04 92.77 83.18 89.23 89.64 97.38 99.42 99.51 72.24 90.75 92.12 83.70 88.78 89.51 ResNet-44 98.99 99.51 99.57 82.27 92.15 92.73 83.86 88.99 89.88 97.85 99.44 99.52 71.51 91.01 92.04 83.90 88.74 89.61 MobileNetv2/0.2 98.31 99.02 99.26 72.01 89.49 89.87 67.78 84.97 83.94 94.16 98.64 98.77 46.82 86.62 87.59 69.66 80.64 80.69 MobileNetv2/0.5 98.71 99.24 99.32 75.03 90.15 90.62 73.37 85.70 86.90 96.85 98.97 98.93 50.20 87.41 89.58 75.46 83.87 80.07 ViT-Lite-1/8 93.58 97.95 98.10 73.90 87.82 88.79 57.96 81.02 84.00 83.33 96.89 97.23 57.54 82.33 85.17 58.55 77.72 80.24 ViT-Lite-2/8 93.71 97.99 98.11 75.74 87.89 88.82 58.92 81.10 84.10 84.19 97.01 97.31 58.97 82.17 85.42 58.86 77.85 80.26 CCT-2/2 98.45 99.29 99.41 81.90 90.42 91.33 82.74 87.94 89.39 95.56 99.03 99.18 72.44 88.07 89.75 81.77 86.87 88.45 CCT-4/2 98.47 99.34 99.45 82.80 90.74 91.53 83.64 88.32 89.53 96.49 99.07 99.34 73.82 87.30 89.98 82.77 87.47 88.69 OnDev-LCT-1/1 98.97 99.43 99.53 82.75 92.03 92.85 82.32 88.96 89.95 97.92 99.29 99.38 75.30 90.92 91.88 82.13 87.92 89.34 OnDev-LCT-2/1 99.18 99.46 99.52 83.12 92.07 92.97 84.43 89.06 90.06 97.85 99.37 99.37 78.56 90.78 92.21 82.98 88.04 89.58 OnDev-LCT-4/1 99.07 99.50 99.54 83.71 92.26 93.13 85.96 89.25 90.32 98.21 99.38 99.36 78.24 90.92 92.03 84.15 88.32 89.64 OnDev-LCT-8/1 99.10 99.43 99.59 83.42 92.37 93.15 86.99 89.48 90.35 97.87 99.28 99.43 78.82 90.84 91.94 84.56 88.84 89.88 Table 13: Comparison on image classification performance in FL under different degrees of data heterogeneity.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "MNIST",
                "Fashion",
                "EMNIST",
                "Balanced",
                "clients",
                "FedAvg",
                "Table",
                "Model",
                "Comparison",
                "experiment"
            ]
        }
    },
    {
        "id": "d246fd09-93ea-4af3-b9dd-d538f00dc6ff",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "All reported accuracy (%) values are the best of 3 runs. All experiments were conducted without applying any data augmentation or learning rate schedulers. Three best accuracy values are marked in bold. 16 28 32 48 56 64 Image size 76.0 77.5 79.0 80.5 82.0 83.5 85.0 86.5 88.0 Accuracy (%) OnDev-LCT-1/1 OnDev-LCT-1/3 OnDev-LCT-8/1 OnDev-LCT-8/3 Figure 8: Accuracy (%) versus image size on CIFAR-10.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "runs",
                "accuracy",
                "reported",
                "Image",
                "Figure",
                "size",
                "schedulers",
                "experiments",
                "conducted",
                "applying"
            ]
        }
    },
    {
        "id": "4697a467-e8e8-4664-b81d-1a43ce362046",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "performance of our proposed OnDev-LCTs on the CIFAR- 10 (Krizhevsky et al., 2009) dataset by applying simple data augmentation and learning rate schedulers. We conduct exper- iments in the centralized scenario by selecting the smallest and largest model variants of our OnDev-LCT with different num- bers of standard convolutions in the LCT tokenizer. First, we apply simple data augmentation techniques, including horizon- tal flip, random rotation, and zoom.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "CIFAR",
                "Krizhevsky",
                "performance",
                "dataset",
                "schedulers",
                "data",
                "simple",
                "augmentation",
                "proposed",
                "applying"
            ]
        }
    },
    {
        "id": "218affb9-bd8f-4969-b839-eef06993acc1",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "As shown in Table 12, our models significantly improve accuracy even with simple aug- mentations applied to the training data. Then, we set a linear learning rate warm-up for the initial 10 epochs, followed by a gradual reduction of the learning rate per epoch using cosine annealing (Loshchilov and Hutter, 2017b).",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "Table",
                "aug",
                "mentations",
                "data",
                "shown",
                "models",
                "significantly",
                "improve",
                "accuracy",
                "simple"
            ]
        }
    },
    {
        "id": "09f80c5b-5650-44ba-bade-672394fc4e86",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "The performance is further enhanced, demonstrating that a proper configuration of data augmentation and learning rate scheduler can effectively improve the efficiency of our OnDev-LCT when implementing it in real-world applications. Analysis on the impact of input image resolution. Here, we analyze the performance of our OnDev-LCTs on various im- age sizes using the CIFAR-10 (Krizhevsky et al., 2009) dataset.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "enhanced",
                "demonstrating",
                "applications",
                "performance",
                "proper",
                "configuration",
                "data",
                "augmentation",
                "learning",
                "rate"
            ]
        }
    },
    {
        "id": "c35d4650-e455-4aed-b08e-ad7918ebfae7",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "We conduct this analysis by downsampling the images to two smaller sizes, i.e., 16 × 16, 28 × 28 or upsampling the images to three larger sizes, i.e., 48 × 48, 56 × 56, and 64 × 64. From Fig- ure 8, we can observe that our models still perform well with smaller images, even without data augmentation.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "sizes",
                "images",
                "smaller",
                "conduct",
                "analysis",
                "downsampling",
                "upsampling",
                "larger",
                "Fig",
                "ure"
            ]
        }
    },
    {
        "id": "a4739840-5f57-442b-9c7f-00b5164c1963",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "Thus, we can infer that by incorporating inductive biases via the LCT tok- 1 2 4 8 12 16 Attention heads 84.0 84.5 85.0 85.5 86.0 86.5 87.0 87.5 88.0 Accuracy (%) OnDev-LCT-1/1 OnDev-LCT-1/3 OnDev-LCT-8/1 OnDev-LCT-8/3 Figure 9: Accuracy (%) versus number of attention heads on CIFAR-10. enizer, our OnDev-LCTs can maintain the spatial information of images, which helps in learning better image representations. Analysis on the impact of multi-head self-attention.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "Accuracy",
                "Figure",
                "Attention",
                "LCT",
                "heads",
                "tok",
                "versus",
                "infer",
                "incorporating",
                "inductive"
            ]
        }
    },
    {
        "id": "dc3fbf9e-5ab4-4a33-be39-f2786ad8963a",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "In this ex- periment, we investigate the impact of varying the number of attention heads in the LCT encoder on the performance of our OnDev-LCTs. Multiple attention heads in the MHSA block of an LCT encoder help in parallel processing, giving our OnDev- LCT greater power to encode multiple relationships between image patches. Figure 9 demonstrates the performance of our OnDev-LCTs with different numbers of attention heads.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "LCT",
                "heads",
                "attention",
                "periment",
                "encoder",
                "performance",
                "OnDev-LCTs",
                "investigate",
                "impact",
                "varying"
            ]
        }
    },
    {
        "id": "15649974-1903-4547-be9f-ee099bc4c5bb",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "Obvi- ously, small variants of our models perform better with more attention heads, but larger variants gradually drop in accuracy when using 12 and 16 attention heads. Still, all our models pre- serve their performance even with a single attention head. Analysis on various FL settings.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "heads",
                "Obvi",
                "ously",
                "attention",
                "variants",
                "head",
                "small",
                "models",
                "perform",
                "larger"
            ]
        }
    },
    {
        "id": "d6293ea7-c69f-472f-abee-a2d1144cf9e5",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "Image classification perfor- mance of our proposed OnDev-LCTs on the MNIST, Fashion- MNIST, and EMNIST-Balanced datasets, compared to the other baselines under various data heterogeneity settings, is shown in Table 13. The reported values are the best median values from the last 5 FL rounds of 3 separate runs. Our findings indicate that OnDev-LCTs outperform the competition in most situa- tions.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "MNIST",
                "Fashion",
                "Table",
                "Image",
                "perfor",
                "mance",
                "datasets",
                "compared",
                "settings",
                "classification"
            ]
        }
    },
    {
        "id": "36ddcede-2ca5-4630-b4e0-b0d8098fe406",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "Figure 10 shows the accuracy versus the number of FL rounds on the CIFAR-10 dataset with varying degrees of data heterogeneity for both 10-client and 50-client scenarios; thus, our proposed models outperform the other baselines in all cases.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "Figure",
                "scenarios",
                "shows",
                "dataset",
                "cases",
                "accuracy",
                "versus",
                "number",
                "rounds",
                "varying"
            ]
        }
    },
    {
        "id": "9f078b27-4897-420b-bf9b-931fb7e72206",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "Especially for higher β values, OnDev-LCT variants converge 14 0 10 20 30 40 50 60 FL rounds 10 20 30 40 50 60 70 80 90 Accuracy (%) OnDev-LCT-1/1 OnDev-LCT-2/1 OnDev-LCT-4/1 OnDev-LCT-8/1 MobileNetv2/0.5 MobileNetv2/0.2 Resnet-20 Resnet-32 Resnet-44 ViT-Lite-1/8 ViT-Lite-2/8 CCT-2/2 CCT-4/2 (a) For 10 clients (β = 0.1) 0 10 20 30 40 50 60 FL rounds 10 20 30 40 50 60 70 80 90 Accuracy (%) (b) For 10 clients (β = 0.5) 0 10 20 30 40 50 60 FL rounds 10 20 30 40 50 60 70 80 90 Accuracy (%) (c) For 10 clients (β = 5) 0 20 40 60 80 100 FL rounds 10 20 30 40 50 60 70 80 90 Accuracy (%) (d) For 50 clients (β = 0.1) 0 20 40 60 80 100 FL rounds 10 20 30 40 50 60 70 80 90 Accuracy (%) (e) For 50 clients (β = 0.5) 0 20 40 60 80 100 FL rounds 10 20 30 40 50 60 70 80 90 Accuracy (%) (f) For 50 clients (β = 5) Figure 10: Accuracy (%) versus FL rounds on CIFAR-10 test set under different degrees of data heterogeneity.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "Accuracy",
                "clients",
                "rounds",
                "Figure",
                "OnDev-LCT",
                "converge",
                "versus",
                "test",
                "heterogeneity",
                "higher"
            ]
        }
    },
    {
        "id": "f37957e9-3afe-4cad-96c9-1467bf5b534f",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "Best viewed in color.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "color",
                "viewed"
            ]
        }
    },
    {
        "id": "c5f9a7e6-e6c6-4d8b-9caa-0ff61ef0f092",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "1 5 10 Local epochs 50 55 60 65 70 75 80 Accuracy (%) OnDev-LCT-1/1 (10 clients) OnDev-LCT-2/1 (10 clients) OnDev-LCT-4/1 (10 clients) OnDev-LCT-8/1 (10 clients) OnDev-LCT-1/1 (50 clients) OnDev-LCT-2/1 (50 clients) OnDev-LCT-4/1 (50 clients) OnDev-LCT-8/1 (50 clients) (a) β = 0.1 1 5 10 Local epochs 70 72 74 76 78 80 82 84 Accuracy (%) (b) β = 0.5 1 5 10 Local epochs 78 80 82 84 86 Accuracy (%) (c) β = 5 Figure 11: Accuracy (%) versus the number of local epochs on CIFAR-10 test set under different degrees of data heterogeneity.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "clients",
                "Accuracy",
                "Local",
                "epochs",
                "Figure",
                "versus",
                "test",
                "heterogeneity",
                "number",
                "set"
            ]
        }
    },
    {
        "id": "00224b23-4aec-45cd-9880-7d71c4ad3fde",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "Best viewed in color. faster than the other baselines with a significant performance gap in between. Figure 11 depicts the impact of local epochs in each FL round for the CIFAR-10 dataset. When we increase the local epoch from 1 to 5, the performance of our models dra- matically improves, but just a minor gain when increasing to 10. As a result, we set the default number of local epochs to 5. 7.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "local",
                "color",
                "viewed",
                "epochs",
                "performance",
                "faster",
                "Figure",
                "dataset",
                "epoch",
                "baselines"
            ]
        }
    },
    {
        "id": "951597b7-f2ff-4190-b19e-51e6ccd9a5fe",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "Conclusion In this study, we propose a novel design, namely OnDev- LCT, which introduces inductive biases of CNNs to the vi- sion transformer by incorporating an early convolutional stem before the transformer encoder. We leverage efficient depth- wise separable convolutions to build residual linear bottleneck blocks of the LCT tokenizer for extracting local features from images. The LCT encoder enables our models to learn global representations of images via multi-head self-attention.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "LCT",
                "transformer",
                "Conclusion",
                "study",
                "design",
                "OnDev",
                "sion",
                "propose",
                "introduces",
                "inductive"
            ]
        }
    },
    {
        "id": "c48b6ec7-82d5-4fa6-bf4e-4958f26df077",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "Com- bining the strengths of convolutions and attention mechanisms, we design our OnDev-LCTs for on-device vision tasks with limited training data and resources. We conduct extensive ex- periments to analyze the efficiency of our proposed OnDev- LCTs on various image classification tasks compared to the popular lightweight vision models. Centralized experiments on five benchmark image datasets show the superiority of our OnDev-LCTs over the other baselines.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "bining",
                "mechanisms",
                "resources",
                "vision",
                "tasks",
                "OnDev-LCTs",
                "strengths",
                "convolutions",
                "attention",
                "design"
            ]
        }
    },
    {
        "id": "b89e3250-51ac-47c2-9e9a-42cd716fdb65",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "Furthermore, extensive FL experiments under various settings indicate the efficacy of our OnDev-LCTs in dealing with data heterogeneity and com- munication bottlenecks since our models significantly outper- form the other baselines while having fewer parameters and lower computational demands. We advocate that the proper configuration of data augmentations and learning rate sched- ulers could further improve the performance of our OnDev- LCT when implementing it in real-world low-data scenarios.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "extensive",
                "munication",
                "outper",
                "form",
                "demands",
                "data",
                "experiments",
                "settings",
                "efficacy",
                "OnDev-LCTs"
            ]
        }
    },
    {
        "id": "154e0400-da1f-4f58-a312-ba1443b49188",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "References Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., Corrado, G.S., Davis, A., Dean, J., Devin, M., et al., 2015. Tensorflow: Large-scale machine learning on heterogeneous systems. Agarap, A.F., 2018. Deep learning using rectified linear units (relu). arXiv preprint arXiv:1803.08375 . 15 Bello, I., Zoph, B., Vaswani, A., Shlens, J., Le, Q.V., 2019.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "Agarwal",
                "Barham",
                "Brevdo",
                "Chen",
                "Citro",
                "Corrado",
                "Davis",
                "Dean",
                "Devin",
                "Abadi"
            ]
        }
    },
    {
        "id": "ed704ea7-d36c-433b-bc08-f3750e8d613a",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "Attention aug- mented convolutional networks, in: Proceedings of the IEEE/CVF interna- tional conference on computer vision, pp. 3286–3295. Blalock, D., Gonzalez Ortiz, J.J., Frankle, J., Guttag, J., 2020. What is the state of neural network pruning? Proceedings of machine learning and systems 2, 129–146. Caldas, S., Duddu, S.M.K., Wu, P., Li, T., Koneˇcn`y, J., McMahan, H.B., Smith, V., Talwalkar, A., 2018. Leaf: A benchmark for federated settings. arXiv preprint arXiv:1812.01097 .",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "IEEE",
                "CVF",
                "Proceedings",
                "Attention",
                "aug",
                "mented",
                "interna",
                "tional",
                "vision",
                "Frankle"
            ]
        }
    },
    {
        "id": "da91f26a-d01e-4287-abaa-90079d7c9eab",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "Chen, D., Gao, D., Kuang, W., Li, Y., Ding, B., 2022a. pfl-bench: A compre- hensive benchmark for personalized federated learning. Advances in Neural Information Processing Systems 35, 9344–9360. Chen, T., Cheng, Y., Gan, Z., Yuan, L., Zhang, L., Wang, Z., 2021. Chasing sparsity in vision transformers: An end-to-end exploration. Advances in Neural Information Processing Systems 34, 19974–19988. Chen, Y., Dai, X., Chen, D., Liu, M., Dong, X., Yuan, L., Liu, Z., 2022b.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "Chen",
                "Gao",
                "Kuang",
                "Ding",
                "Yuan",
                "Liu",
                "Systems",
                "Neural",
                "Information",
                "Processing"
            ]
        }
    },
    {
        "id": "a2711d4b-5edc-45b1-8feb-51c713657616",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "Mobile-former: Bridging mobilenet and transformer, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 5270–5279. Choromanski, K.M., Likhosherstov, V., Dohan, D., Song, X., Gane, A., Sarlos, T., Hawkins, P., Davis, J.Q., Mohiuddin, A., Kaiser, L., et al., 2020. Re- thinking attention with performers, in: International Conference on Learn- ing Representations. Choukroun, Y., Kravchik, E., Yang, F., Kisilev, P., 2019.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "Bridging",
                "Proceedings",
                "IEEE",
                "CVF",
                "Recognition",
                "Mobile-former",
                "Computer",
                "Vision",
                "Pattern",
                "Conference"
            ]
        }
    },
    {
        "id": "c36f4c92-5019-4876-ad07-7bcf175cb42c",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "Low-bit quantiza- tion of neural networks for efficient inference, in: 2019 IEEE/CVF Inter- national Conference on Computer Vision Workshop (ICCVW), IEEE. pp. 3009–3018. Cohen, G., Afshar, S., Tapson, J., Van Schaik, A., 2017. Emnist: Extending mnist to handwritten letters, in: 2017 international joint conference on neu- ral networks (IJCNN), IEEE. pp. 2921–2926. Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L., 2009.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "IEEE",
                "ICCVW",
                "CVF",
                "Inter",
                "Workshop",
                "Computer",
                "Vision",
                "IEEE.",
                "Low-bit",
                "quantiza"
            ]
        }
    },
    {
        "id": "18d4cec0-3319-499f-9110-64d64e3c7c5c",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "Imagenet: A large-scale hierarchical image database, in: 2009 IEEE conference on computer vision and pattern recognition, Ieee. pp. 248–255. Deng, L., 2012. The mnist database of handwritten digit images for machine learning research [best of the web]. IEEE signal processing magazine 29, 141–142. Ding, Y., Qin, H., Yan, Q., Chai, Z., Liu, J., Wei, X., Liu, X., 2022.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "Ieee.",
                "Imagenet",
                "Liu",
                "IEEE",
                "recognition",
                "Qin",
                "Yan",
                "Chai",
                "Wei",
                "large-scale"
            ]
        }
    },
    {
        "id": "af237f5b-9862-4920-aae1-0f210a9087ca",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "Towards accurate post-training quantization for vision transformer, in: Proceedings of the 30th ACM International Conference on Multimedia, pp. 5380–5388. Dong, X., Chen, S., Pan, S., 2017. Learning to prune deep neural networks via layer-wise optimal brain surgeon. Advances in neural information process- ing systems 30. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Un- terthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., Houlsby, N., 2021.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "Proceedings",
                "ACM",
                "Multimedia",
                "International",
                "Conference",
                "transformer",
                "Chen",
                "Pan",
                "accurate",
                "post-training"
            ]
        }
    },
    {
        "id": "2a2e28d4-6823-4ae5-8768-946876f4457a",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "An image is worth 16x16 words: Transformers for image recognition at scale, in: International Conference on Learning Repre- sentations. URL: https://openreview.net/forum?id=YicbFdNTTy. Fang, G., Ma, X., Song, M., Mi, M.B., Wang, X., 2023. Depgraph: Towards any structural pruning, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 16091–16101. Ferguson, T.S., 1973. A bayesian analysis of some nonparametric problems. The annals of statistics , 209–230.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "Transformers",
                "International",
                "Repre",
                "Learning",
                "words",
                "sentations",
                "image",
                "Conference",
                "URL",
                "worth"
            ]
        }
    },
    {
        "id": "e4065f1f-204b-4bb2-bcfd-0b6af72ff074",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "Frankle, J., Dziugaite, G.K., Roy, D.M., Carbin, M., 2021. Pruning neural net- works at initialization: Why are we missing the mark?, in: 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Aus- tria, May 3-7, 2021, OpenReview.net. URL: https://openreview.net/ forum?id=Ig-VyQc-MLK. Ghassemi, M., Naumann, T., Schulam, P., Beam, A.L., Chen, I.Y., Ranganath, R., 2020. A review of challenges and opportunities in machine learning for health.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "Dziugaite",
                "Roy",
                "Carbin",
                "Frankle",
                "ICLR",
                "Aus",
                "URL",
                "International",
                "Representations",
                "Virtual"
            ]
        }
    },
    {
        "id": "e9a09c11-fabc-4148-b3eb-05c6e773797a",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "AMIA Summits on Translational Science Proceedings 2020, 191. Graham, B., El-Nouby, A., Touvron, H., Stock, P., Joulin, A., J´egou, H., Douze, M., 2021. Levit: a vision transformer in convnet’s clothing for faster infer- ence, in: Proceedings of the IEEE/CVF international conference on com- puter vision, pp. 12259–12269. Han, K., Wang, Y., Tian, Q., Guo, J., Xu, C., Xu, C., 2020.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "AMIA",
                "Summits",
                "Translational",
                "Science",
                "Touvron",
                "Stock",
                "Joulin",
                "J´egou",
                "Douze",
                "Proceedings"
            ]
        }
    },
    {
        "id": "55257c39-ef98-41a0-bdb2-51467abbc911",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "Ghostnet: More fea- tures from cheap operations, in: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1580–1589. Han, S., Mao, H., Dally, W.J., 2016. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. In- ternational Conference on Learning Representations (ICLR) . Hassani, A., Walton, S., Shah, N., Abuduweili, A., Li, J., Shi, H., 2021. Es- caping the big data paradigm with compact transformers.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "Proceedings",
                "IEEE",
                "CVF",
                "Ghostnet",
                "fea",
                "tures",
                "operations",
                "recognition",
                "Mao",
                "Dally"
            ]
        }
    },
    {
        "id": "e357ce81-e8fa-4569-bf32-cb7aa6e0fab7",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "arXiv preprint arXiv:2104.05704 . Hassibi, B., Stork, D.G., Wolff, G.J., 1993. Optimal brain surgeon and gen- eral network pruning, in: IEEE international conference on neural networks, IEEE. pp. 293–299. He, K., Zhang, X., Ren, S., Sun, J., 2016. Deep residual learning for image recognition, in: Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778. Hendrycks, D., Gimpel, K., 2016. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415 .",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "Hassibi",
                "Stork",
                "Wolff",
                "arXiv",
                "IEEE",
                "Zhang",
                "Ren",
                "Sun",
                "preprint",
                "recognition"
            ]
        }
    },
    {
        "id": "200f4bb2-1996-4d61-bb65-5c38fa488054",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "Howard, A., Sandler, M., Chu, G., Chen, L.C., Chen, B., Tan, M., Wang, W., Zhu, Y., Pang, R., Vasudevan, V., et al., 2019. Searching for mobilenetv3, in: Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 1314–1324. Howard, A.G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T., Andreetto, M., Adam, H., 2017. Mobilenets: Efficient convolutional neural networks for mobile vision applications. arXiv preprint arXiv:1704.04861 .",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "Chen",
                "Sandler",
                "Chu",
                "Tan",
                "Pang",
                "Vasudevan",
                "Wang",
                "Zhu",
                "Howard",
                "Proceedings"
            ]
        }
    },
    {
        "id": "1e6b09dc-f379-44fe-a414-752cc90d6375",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "Hsu, T.M.H., Qi, H., Brown, M., 2019. Measuring the effects of non- identical data distribution for federated visual classification. arXiv preprint arXiv:1909.06335 . Hu, J., Shen, L., Sun, G., 2018. Squeeze-and-excitation networks, in: Proceed- ings of the IEEE conference on computer vision and pattern recognition, pp. 7132–7141. Iandola, F.N., Han, S., Moskewicz, M.W., Ashraf, K., Dally, W.J., Keutzer, K., 2016.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "Brown",
                "Hsu",
                "Shen",
                "Sun",
                "Proceed",
                "Han",
                "Moskewicz",
                "Ashraf",
                "Dally",
                "Keutzer"
            ]
        }
    },
    {
        "id": "cae4efac-c884-456f-aab5-64956a7d0e67",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "Squeezenet: Alexnet-level accuracy with 50x fewer parameters and¡ 0.5 mb model size. arXiv preprint arXiv:1602.07360 . Jacob, B., Kligys, S., Chen, B., Zhu, M., Tang, M., Howard, A., Adam, H., Kalenichenko, D., 2018. Quantization and training of neural networks for efficient integer-arithmetic-only inference, in: Proceedings of the IEEE con- ference on computer vision and pattern recognition, pp. 2704–2713. Jeevan, P., Sethi, A., 2022.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "Alexnet-level",
                "Squeezenet",
                "Jacob",
                "Kligys",
                "Chen",
                "Zhu",
                "Tang",
                "Howard",
                "Adam",
                "Kalenichenko"
            ]
        }
    },
    {
        "id": "4ab770fd-1b55-4918-8565-e12bfd835014",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "Resource-efficient hybrid x-formers for vision, in: Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 2982–2990. Karimireddy, S.P., Kale, S., Mohri, M., Reddi, S., Stich, S., Suresh, A.T., 2020. Scaffold: Stochastic controlled averaging for federated learning, in: Interna- tional Conference on Machine Learning, PMLR. pp. 5132–5143. Khan, A., Sohail, A., Zahoora, U., Qureshi, A.S., 2020.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "vision",
                "Proceedings",
                "IEEE",
                "CVF",
                "Winter",
                "Applications",
                "Computer",
                "Conference",
                "Resource-efficient",
                "learning"
            ]
        }
    },
    {
        "id": "a6c8fd83-93e2-4ccc-b818-5d49429ed9cb",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "A survey of the recent architectures of deep convolutional neural networks. Artificial intelligence review 53, 5455–5516. Khan, S., Naseer, M., Hayat, M., Zamir, S.W., Khan, F.S., Shah, M., 2021. Transformers in vision: A survey. ACM Computing Surveys (CSUR) . Kingma, D.P., Ba, J., 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980 . Krizhevsky, A., Hinton, G., et al., 2009. Learning multiple lay- ers of features from tiny images.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "Khan",
                "networks",
                "Naseer",
                "Hayat",
                "Zamir",
                "Shah",
                "recent",
                "architectures",
                "deep",
                "convolutional"
            ]
        }
    },
    {
        "id": "c67d0461-4c68-4476-875d-53a92fc979bb",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "Technical report, University of Toronto, 2009 URL: https://www.cs.toronto.edu/~kriz/ learning-features-2009-TR.pdf. Krizhevsky, A., Sutskever, I., Hinton, G.E., 2012. Imagenet classification with deep convolutional neural networks. Advances in neural information pro- cessing systems 25. LeCun, Y., Bengio, Y., Hinton, G., 2015. Deep learning. nature 521, 436–444. LeCun, Y., Denker, J., Solla, S., 1989. Optimal brain damage. Advances in neural information processing systems 2.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "URL",
                "University",
                "Toronto",
                "Hinton",
                "Sutskever",
                "LeCun",
                "Technical",
                "report",
                "neural",
                "Advances"
            ]
        }
    },
    {
        "id": "bcbc3efd-85f1-4f82-acd0-4114b16ddfe8",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "Lenc, K., Vedaldi, A., 2015. Understanding image representations by measur- ing their equivariance and equivalence, in: Proceedings of the IEEE confer- ence on computer vision and pattern recognition, pp. 991–999. Li, Q., He, B., Song, D., 2021a. Model-contrastive federated learning, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10713–10722. Li, R., Wang, Y., Liang, F., Qin, H., Yan, J., Fan, R., 2019.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "Vedaldi",
                "Lenc",
                "Proceedings",
                "recognition",
                "IEEE",
                "computer",
                "vision",
                "pattern",
                "Song",
                "Wang"
            ]
        }
    },
    {
        "id": "a9acb0b7-c759-4568-9190-27d0dd615e42",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "Fully quantized network for object detection, in: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 2810–2819. Li, T., Sahu, A.K., Talwalkar, A., Smith, V., 2020a. Federated learning: Chal- lenges, methods, and future directions. IEEE Signal Processing Magazine 37, 50–60. Li, T., Sahu, A.K., Zaheer, M., Sanjabi, M., Talwalkar, A., Smith, V., 2020b. Federated optimization in heterogeneous networks. Proceedings of Machine Learning and Systems 2, 429–450.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "CVF",
                "Sahu",
                "Talwalkar",
                "Smith",
                "Proceedings",
                "Fully",
                "detection",
                "recognition",
                "IEEE",
                "Chal"
            ]
        }
    },
    {
        "id": "a52f3e40-57f9-4e13-944b-4b1f8cdfbfe5",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "Li, Y., Chen, Y., Dai, X., Chen, D., Liu, M., Yuan, L., Liu, Z., Zhang, L., Vasconcelos, N., 2021b. Micronet: Improving image recognition with ex- tremely low flops, in: Proceedings of the IEEE/CVF International confer- 16 ence on computer vision, pp. 468–477. Li, Y., Xu, S., Zhang, B., Cao, X., Gao, P., Guo, G., 2022. Q-vit: Accurate and fully quantized low-bit vision transformer, in: Oh, A.H., Agarwal, A., Belgrave, D., Cho, K. (Eds.), Advances in Neural Information Processing Systems.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "Chen",
                "Liu",
                "Dai",
                "Yuan",
                "Vasconcelos",
                "Zhang",
                "Improving",
                "Proceedings",
                "IEEE",
                "CVF"
            ]
        }
    },
    {
        "id": "52519a55-baf1-4372-b34a-66314865c16b",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "URL: https://openreview.net/forum?id=fU-m9kQe0ke. Li, Z., Liu, F., Yang, W., Peng, S., Zhou, J., 2021c. A survey of convolutional neural networks: analysis, applications, and prospects. IEEE transactions on neural networks and learning systems . Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Doll´ar, P., Zitnick, C.L., 2014. Microsoft coco: Common objects in context, in: European conference on computer vision, Springer. pp. 740–755.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "URL",
                "Liu",
                "Yang",
                "Peng",
                "Zhou",
                "Lin",
                "Maire",
                "Belongie",
                "Hays",
                "Perona"
            ]
        }
    },
    {
        "id": "482168e7-a426-47c7-b950-cd64a90b1d61",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "Lin, Y., Zhang, T., Sun, P., Li, Z., Zhou, S., 2022. Fq-vit: Post-training quanti- zation for fully quantized vision transformer, in: Proceedings of the Thirty- First International Joint Conference on Artificial Intelligence, IJCAI-22, pp. 1173–1179. Liu, S.Y., Liu, Z., Cheng, K.T., 2023. Oscillation-free quantization for low-bit vision transformers. arXiv preprint arXiv:2302.02210 . Liu, Z., Li, J., Shen, Z., Huang, G., Yan, S., Zhang, C., 2017.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "Sun",
                "Zhou",
                "Liu",
                "Zhang",
                "Lin",
                "Post-training",
                "Proceedings",
                "Thirty",
                "Intelligence",
                "International"
            ]
        }
    },
    {
        "id": "2c71e1ee-cfc2-4b2d-bb81-deff68c398fe",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "Learning efficient convolutional networks through network slimming, in: Proceedings of the IEEE international conference on computer vision, pp. 2736–2744. Liu, Z., Wang, Y., Han, K., Zhang, W., Ma, S., Gao, W., 2021. Post-training quantization for vision transformer. Advances in Neural Information Pro- cessing Systems 34, 28092–28103. Loshchilov, I., Hutter, F., 2017a. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101 . Loshchilov, I., Hutter, F., 2017b.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "Proceedings",
                "Hutter",
                "IEEE",
                "Loshchilov",
                "Learning",
                "slimming",
                "Wang",
                "Han",
                "Zhang",
                "Gao"
            ]
        }
    },
    {
        "id": "195eb347-0bb0-4d36-bc2d-4be732d0bfa3",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "SGDR: Stochastic gradient descent with warm restarts, in: International Conference on Learning Representations. URL: https://openreview.net/forum?id=Skq89Scxx. van der Maaten, L., Hinton, G., 2008. Visualizing data using t-sne. Journal of Machine Learning Research 9, 2579–2605. URL: http://jmlr.org/ papers/v9/vandermaaten08a.html. Maaz, M., Shaker, A., Cholakkal, H., Khan, S., Zamir, S.W., Anwer, R.M., Shahbaz Khan, F., 2022.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "SGDR",
                "Stochastic",
                "International",
                "Representations",
                "URL",
                "Conference",
                "restarts",
                "Hinton",
                "Khan",
                "Learning"
            ]
        }
    },
    {
        "id": "da4cfff7-0d90-4600-bd69-497ee53e6228",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "Edgenext: efficiently amalgamated cnn- transformer architecture for mobile vision applications, in: European Con- ference on Computer Vision, Springer. pp. 3–20. McMahan, B., Moore, E., Ramage, D., Hampson, S., y Arcas, B.A., 2017. Communication-efficient learning of deep networks from decentralized data, in: Artificial intelligence and statistics, PMLR. pp. 1273–1282. Mehta, S., Rastegari, M., 2022.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "European",
                "Con",
                "Springer.",
                "Edgenext",
                "vision",
                "Computer",
                "efficiently",
                "cnn",
                "transformer",
                "applications"
            ]
        }
    },
    {
        "id": "aa084678-b0c1-449e-875a-5a28ce4bd9cc",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "Mobilevit: Light-weight, general-purpose, and mobile-friendly vision transformer, in: International Conference on Learning Representations. URL: https://openreview.net/forum?id= vh-0sUt8HlG. Mehta, S., Rastegari, M., 2023. Separable self-attention for mobile vision transformers. Transactions on Machine Learning Research URL: https: //openreview.net/forum?id=tBl4yBEjKi. Menghani, G., 2023. Efficient deep learning: A survey on making deep learning models smaller, faster, and better.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "Light-weight",
                "International",
                "Representations",
                "Conference",
                "Mobilevit",
                "general-purpose",
                "Learning",
                "URL",
                "Rastegari",
                "mobile-friendly"
            ]
        }
    },
    {
        "id": "7ad17a39-0c09-4964-aa9f-99a43b8dcefa",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "ACM Computing Surveys 55, 1–37. Mitchell, B.R., et al., 2017. The spatial inductive bias of deep learning. Ph.D. thesis. Johns Hopkins University. Molchanov, P., Mallya, A., Tyree, S., Frosio, I., Kautz, J., 2019. Importance estimation for neural network pruning, in: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 11264–11272. Molchanov, P., Tyree, S., Karras, T., Aila, T., Kautz, J., 2016.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "ACM",
                "Surveys",
                "Tyree",
                "Kautz",
                "Computing",
                "Molchanov",
                "Mitchell",
                "Mallya",
                "Frosio",
                "Karras"
            ]
        }
    },
    {
        "id": "79bd73df-a491-496a-894a-4560214d1d79",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "Pruning con- volutional neural networks for resource efficient inference. arXiv preprint arXiv:1611.06440 . Mutis, I., Ambekar, A., 2020. Challenges and enablers of augmented reality technology for in situ walkthrough applications. J. Inf. Technol. Constr. 25, 55–71. Neuman, S.M., Plancher, B., Duisterhof, B.P., Krishnan, S., Banbury, C., Mazumder, M., Prakash, S., Jabbour, J., Faust, A., de Croon, G.C., et al., 2022.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "Mutis",
                "Ambekar",
                "Pruning",
                "con",
                "volutional",
                "inference",
                "neural",
                "networks",
                "resource",
                "efficient"
            ]
        }
    },
    {
        "id": "7244de24-a4b8-434a-be08-9c6517e36ac8",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "Tiny robot learning: challenges and directions for machine learning in resource-constrained robots, in: 2022 IEEE 4th International Conference on Artificial Intelligence Circuits and Systems (AICAS), IEEE. pp. 296–299. Parekh, D., Poddar, N., Rajpurkar, A., Chahal, M., Kumar, N., Joshi, G.P., Cho, W., 2022. A review on autonomous vehicles: Progress, methods and challenges. Electronics 11, 2162. Park, J., Woo, S., Lee, J.Y., Kweon, I.S., 2020.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "IEEE",
                "AICAS",
                "International",
                "Systems",
                "Conference",
                "Artificial",
                "Intelligence",
                "Circuits",
                "learning",
                "Tiny"
            ]
        }
    },
    {
        "id": "8880b046-2da1-4e3a-93ca-3000ff5ff1e6",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "A simple and light-weight attention module for convolutional neural networks. International journal of computer vision 128, 783–798. Parmar, N., Vaswani, A., Uszkoreit, J., Kaiser, L., Shazeer, N., Ku, A., Tran, D., 2018. Image transformer, in: International Conference on Machine Learn- ing, PMLR. pp. 4055–4064. Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., et al., 2019.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "networks",
                "International",
                "simple",
                "light-weight",
                "attention",
                "module",
                "convolutional",
                "neural",
                "Vaswani",
                "Uszkoreit"
            ]
        }
    },
    {
        "id": "1fe5be63-8326-4ada-a5e9-de2b956ed51c",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "Pytorch: An imperative style, high-performance deep learning library. Advances in neural informa- tion processing systems 32. Qu, L., Zhou, Y., Liang, P.P., Xia, Y., Wang, F., Adeli, E., Fei-Fei, L., Rubin, D., 2022. Rethinking architecture design for tackling data heterogeneity in fed- erated learning, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10061–10071.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "Pytorch",
                "style",
                "high-performance",
                "library",
                "Zhou",
                "Liang",
                "Xia",
                "Wang",
                "Adeli",
                "Rubin"
            ]
        }
    },
    {
        "id": "e5e3093a-41a9-47de-b8f9-c6865e9a7591",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "Roy, N., Posner, I., Barfoot, T., Beaudoin, P., Bengio, Y., Bohg, J., Brock, O., Depatie, I., Fox, D., Koditschek, D., et al., 2021. From machine learning to robotics: challenges and opportunities for embodied intelligence. arXiv preprint arXiv:2110.15245 . Sabour, S., Frosst, N., Hinton, G.E., 2017. Dynamic routing between capsules. Advances in neural information processing systems 30. Sandler, M., Howard, A., Zhu, M., Zhmoginov, A., Chen, L.C., 2018.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "Posner",
                "Barfoot",
                "Beaudoin",
                "Bengio",
                "Bohg",
                "Brock",
                "Depatie",
                "Fox",
                "Koditschek",
                "Roy"
            ]
        }
    },
    {
        "id": "7bd3f4f5-07a2-4be6-b784-5ba0fdbc1a26",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "Mo- bilenetv2: Inverted residuals and linear bottlenecks, in: Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 4510– 4520. Sanh, V., Wolf, T., Rush, A., 2020. Movement pruning: Adaptive sparsity by fine-tuning. Advances in Neural Information Processing Systems 33, 20378–20389. Shi, M., Zhou, Y., Ye, Q., Lv, J., 2022. Personalized federated learning with hidden information on personalized prior. arXiv preprint arXiv:2211.10684 . Simonyan, K., Zisserman, A., 2014.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "Inverted",
                "Proceedings",
                "IEEE",
                "bottlenecks",
                "recognition",
                "Wolf",
                "Rush",
                "Zhou",
                "residuals",
                "linear"
            ]
        }
    },
    {
        "id": "a8fd1b50-2b17-4be0-b83c-6a29870a16d7",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "Very deep convolutional networks for large-scale image recognition. CoRR abs/1409.1556. URL: https://api. semanticscholar.org/CorpusID:14124313. Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Vanhoucke, V., Rabinovich, A., 2015. Going deeper with convolutions, in: Proceedings of the IEEE conference on computer vision and pattern recog- nition, pp. 1–9. Tan, M., Le, Q., 2019.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "recognition",
                "URL",
                "Liu",
                "Jia",
                "Sermanet",
                "Reed",
                "Anguelov",
                "Erhan",
                "Vanhoucke",
                "Rabinovich"
            ]
        }
    },
    {
        "id": "fdd60adf-aede-448c-b182-61d65cd3a44a",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "Efficientnet: Rethinking model scaling for convolutional neural networks, in: International conference on machine learning, PMLR. pp. 6105–6114. Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., J´egou, H., 2021. Training data-efficient image transformers & distillation through attention, in: International Conference on Machine Learning, PMLR. pp. 10347–10357. Touvron, H., Cord, M., El-Nouby, A., Verbeek, J., J´egou, H., 2022.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "Rethinking",
                "International",
                "Cord",
                "J´egou",
                "Efficientnet",
                "learning",
                "conference",
                "machine",
                "Touvron",
                "PMLR."
            ]
        }
    },
    {
        "id": "bea8c014-fef8-450f-82dc-5910ead2dd08",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "Three things everyone should know about vision transformers, in: Computer Vision–ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part XXIV, Springer. pp. 497–515. Varoquaux, G., Cheplygina, V., 2022. Machine learning for medical imaging: methodological failures and recommendations for the future. NPJ digital medicine 5, 48. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, Ł., Polosukhin, I., 2017.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "ECCV",
                "Israel",
                "October",
                "Proceedings",
                "Computer",
                "European",
                "Conference",
                "Tel",
                "Aviv",
                "Part"
            ]
        }
    },
    {
        "id": "4d74fda0-2daa-4b2b-bf13-56a0f4fd9019",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "Attention is all you need. Advances in neural information processing systems 30. Wang, F., Jiang, M., Qian, C., Yang, S., Li, C., Zhang, H., Wang, X., Tang, X., 2017. Residual attention network for image classification, in: Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 3156– 3164. Wang, H., Yurochkin, M., Sun, Y., Papailiopoulos, D., Khazaeni, Y., 2020a. Federated learning with matched averaging, in: International Conference on Learning Representations.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "Wang",
                "Jiang",
                "Qian",
                "Yang",
                "Zhang",
                "Tang",
                "conference",
                "Yurochkin",
                "Sun",
                "Papailiopoulos"
            ]
        }
    },
    {
        "id": "d71c87ce-16a8-498b-9c9f-57edf1c857d3",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "URL: https://openreview.net/forum?id= BkluqlSFDS. Wang, J., Liu, Q., Liang, H., Joshi, G., Poor, H.V., 2020b. Tackling the objective inconsistency problem in heterogeneous federated optimization. Advances in neural information processing systems 33, 7611–7623. Wang, S., Li, B.Z., Khabsa, M., Fang, H., Ma, H., 2020c. Linformer: Self- attention with linear complexity. arXiv preprint arXiv:2006.04768 . Wightman, R., 2019. Pytorch image models. https://github.com/ rwightman/pytorch-image-models.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "URL",
                "BkluqlSFDS",
                "Wang",
                "Liu",
                "Liang",
                "Joshi",
                "Poor",
                "Khabsa",
                "Fang",
                "Wightman"
            ]
        }
    },
    {
        "id": "4de10f4b-e9ff-4f89-bdc2-c68e746a42ca",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "doi:10.5281/zenodo.4414861. Wu, H., Xiao, B., Codella, N., Liu, M., Dai, X., Yuan, L., Zhang, L., 2021a. Cvt: Introducing convolutions to vision transformers, in: Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 22–31. Wu, Y., Kang, Y., Luo, J., He, Y., Yang, Q., 2021b. Fedcg: Leverage conditional gan for protecting privacy and maintaining competitive performance in fed- erated learning, in: International Joint Conference on Artificial Intelligence.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "doi",
                "Xiao",
                "Codella",
                "Liu",
                "Dai",
                "Yuan",
                "Zhang",
                "Conference",
                "International",
                "vision"
            ]
        }
    },
    {
        "id": "9f1cfdcc-631a-41ed-b65e-4f409a39390f",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "URL: https://api.semanticscholar.org/CorpusID:244130332. 17 Xiao, H., Rasul, K., Vollgraf, R., 2017. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms. arXiv preprint arXiv:1708.07747 . Xinyi, Z., Chen, L., 2019. Capsule graph neural network, in: International conference on learning representations. Xiong, Y., Zeng, Z., Chakraborty, R., Tan, M., Fung, G., Li, Y., Singh, V., 2021.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "URL",
                "Xiao",
                "Rasul",
                "Vollgraf",
                "Xinyi",
                "Chen",
                "Zeng",
                "Chakraborty",
                "Tan",
                "Fung"
            ]
        }
    },
    {
        "id": "00757ca3-e409-44fd-8d56-aa9f8c53acca",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "Nystr¨omformer: A nystr¨om-based algorithm for approximating self- attention, in: Proceedings of the AAAI Conference on Artificial Intelli- gence, pp. 14138–14148. Yang, H., Yin, H., Shen, M., Molchanov, P., Li, H., Kautz, J., 2023. Global vision transformer pruning with hessian-aware saliency, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 18547–18557. Yu, F., Huang, K., Wang, M., Cheng, Y., Chu, W., Cui, L., 2022.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "Intelli",
                "Nystr",
                "Proceedings",
                "AAAI",
                "Artificial",
                "omformer",
                "attention",
                "gence",
                "Conference",
                "om-based"
            ]
        }
    },
    {
        "id": "ad80f319-b8b0-4829-9212-09d1307f682d",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "Width & depth pruning for vision transformers, in: Proceedings of the AAAI Conference on Artificial Intelligence, pp. 3143–3151. Yu, L., Xiang, W., 2023. X-pruner: explainable pruning for vision transform- ers, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 24355–24363. Yuan, Z., Xue, C., Chen, Y., Wu, Q., Sun, G., 2022.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "Intelligence",
                "Proceedings",
                "AAAI",
                "Artificial",
                "Width",
                "Conference",
                "vision",
                "depth",
                "transformers",
                "Xiang"
            ]
        }
    },
    {
        "id": "6c21181b-d8d3-45e2-82f0-5e46bbdaf13f",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "Ptq4vit: Post-training quantization for vision transformers with twin uniform quantization, in: Eu- ropean Conference on Computer Vision, Springer. pp. 191–207. Zhu, M., Tang, Y., Han, K., 2021. Vision transformer pruning. arXiv preprint arXiv:2104.08500 . Zuo, Z., Shuai, B., Wang, G., Liu, X., Wang, X., Wang, B., Chen, Y., 2015.",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "Wang",
                "Post-training",
                "Springer.",
                "Conference",
                "Computer",
                "quantization",
                "vision",
                "ropean",
                "Tang",
                "Han"
            ]
        }
    },
    {
        "id": "68cadd21-365d-454a-867c-675510d7aa7c",
        "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning",
        "chunk_text": "Convolutional recurrent neural networks: Learning spatial dependencies for image representation, in: Proceedings of the IEEE conference on computer vision and pattern recognition workshops, pp. 18–26. 18",
        "metadata": {
            "author": "Chu Myaet Thwal; Minh N. H. Nguyen; Ye Lin Tun; Seong Tae Kim; My T. Thai; Choong Seon Hong; ",
            "keywords": [
                "Learning",
                "Proceedings",
                "IEEE",
                "Convolutional",
                "networks",
                "representation",
                "workshops",
                "recurrent",
                "neural",
                "spatial"
            ]
        }
    }
]